{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AAF_lo1sRaEA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgZqs5fpRaEB"
      },
      "source": [
        "## Loading the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "XUX_vZX0RaEB",
        "outputId": "5d92e577-b66f-4b9b-a1ca-6161b60e3252"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             0         1         2         3         4         5         6  \\\n",
              "0     7.161286  7.835325  2.911583  0.984049 -1.499546 -2.094097  0.576000   \n",
              "1     0.225763 -0.094169 -0.603646  0.497745  0.874036  0.290280 -0.077659   \n",
              "2    -0.692525 -0.517801 -0.788035  1.214351 -0.907214  0.880213  0.406899   \n",
              "3    -0.735562 -0.684055  2.058215  0.716328 -0.011393  0.805396  1.497982   \n",
              "4     0.570272  0.273157 -0.279214  0.083456  1.049331 -0.869295 -0.265858   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "1054  0.399577  0.310805 -0.039326 -0.111546  0.304586 -0.943453  0.114960   \n",
              "1055  1.640386  1.306224  0.192745 -1.816855 -1.311906 -2.128963 -1.875967   \n",
              "1056 -0.772360 -0.670596 -0.840420 -0.832105  0.277346  1.152162  0.241470   \n",
              "1057 -0.996965 -1.099395  3.515274 -0.508185 -1.102654  0.192081  0.069821   \n",
              "1058 -0.150911 -0.094333 -0.568885 -0.614652  0.332477 -0.954948 -1.527722   \n",
              "\n",
              "             7         8         9  ...        60        61        62  \\\n",
              "0    -1.205671  1.849122 -0.425598  ... -1.504263  0.351267 -1.018726   \n",
              "1    -0.887385  0.432062 -0.093963  ... -0.495712 -0.465077 -0.157861   \n",
              "2    -0.694895 -0.901869 -1.701574  ... -0.637167  0.147260  0.217914   \n",
              "3     0.114752  0.692847  0.052377  ... -0.178325 -0.065059 -0.724247   \n",
              "4    -0.401676 -0.872639  1.147483  ... -0.919463 -0.667912 -0.820172   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "1054 -0.335898  0.826753 -0.393786  ... -0.558717  0.998897 -0.106835   \n",
              "1055  0.094232 -1.429742  0.873777  ...  0.223143 -0.032425  0.226782   \n",
              "1056  0.229092  0.019036 -0.068804  ...  0.449239 -0.965270 -0.590039   \n",
              "1057  0.264674 -0.411533  0.501164  ...  1.941398  1.769292  0.738616   \n",
              "1058 -1.591471 -3.678713 -5.930209  ...  5.121875  4.103031  3.673086   \n",
              "\n",
              "            63        64        65        66        67     lg      la  \n",
              "0    -0.174878 -1.089543 -0.668840 -0.914772 -0.836250 -15.75  -47.95  \n",
              "1    -0.157189  0.380951  1.088478 -0.123595  1.391141  14.91  -23.51  \n",
              "2     2.718442  0.972919  2.081069  1.375763  1.063847  12.65   -8.00  \n",
              "3    -1.020687 -0.751380 -0.385005 -0.012326 -0.392197   9.03   38.74  \n",
              "4    -0.190488  0.306974  0.119658  0.271838  1.289783  34.03   -6.85  \n",
              "...        ...       ...       ...       ...       ...    ...     ...  \n",
              "1054  1.526307  0.646088  2.467278  1.867699  1.719302  -6.17   35.74  \n",
              "1055  0.182107  0.517466  1.126762  2.220671  4.422651  11.55  104.91  \n",
              "1056 -0.804297  0.044170 -0.718175 -0.983640 -0.573822  41.33   19.80  \n",
              "1057  1.240377 -0.546002 -0.137473 -0.781036 -0.832167  54.68   25.31  \n",
              "1058  0.960420  1.067164  5.244305  2.506568  1.462580  54.68   25.31  \n",
              "\n",
              "[1059 rows x 70 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-30d2a49f-9148-4533-b251-1fe7122ce667\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>lg</th>\n",
              "      <th>la</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.161286</td>\n",
              "      <td>7.835325</td>\n",
              "      <td>2.911583</td>\n",
              "      <td>0.984049</td>\n",
              "      <td>-1.499546</td>\n",
              "      <td>-2.094097</td>\n",
              "      <td>0.576000</td>\n",
              "      <td>-1.205671</td>\n",
              "      <td>1.849122</td>\n",
              "      <td>-0.425598</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.504263</td>\n",
              "      <td>0.351267</td>\n",
              "      <td>-1.018726</td>\n",
              "      <td>-0.174878</td>\n",
              "      <td>-1.089543</td>\n",
              "      <td>-0.668840</td>\n",
              "      <td>-0.914772</td>\n",
              "      <td>-0.836250</td>\n",
              "      <td>-15.75</td>\n",
              "      <td>-47.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.225763</td>\n",
              "      <td>-0.094169</td>\n",
              "      <td>-0.603646</td>\n",
              "      <td>0.497745</td>\n",
              "      <td>0.874036</td>\n",
              "      <td>0.290280</td>\n",
              "      <td>-0.077659</td>\n",
              "      <td>-0.887385</td>\n",
              "      <td>0.432062</td>\n",
              "      <td>-0.093963</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.495712</td>\n",
              "      <td>-0.465077</td>\n",
              "      <td>-0.157861</td>\n",
              "      <td>-0.157189</td>\n",
              "      <td>0.380951</td>\n",
              "      <td>1.088478</td>\n",
              "      <td>-0.123595</td>\n",
              "      <td>1.391141</td>\n",
              "      <td>14.91</td>\n",
              "      <td>-23.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.692525</td>\n",
              "      <td>-0.517801</td>\n",
              "      <td>-0.788035</td>\n",
              "      <td>1.214351</td>\n",
              "      <td>-0.907214</td>\n",
              "      <td>0.880213</td>\n",
              "      <td>0.406899</td>\n",
              "      <td>-0.694895</td>\n",
              "      <td>-0.901869</td>\n",
              "      <td>-1.701574</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.637167</td>\n",
              "      <td>0.147260</td>\n",
              "      <td>0.217914</td>\n",
              "      <td>2.718442</td>\n",
              "      <td>0.972919</td>\n",
              "      <td>2.081069</td>\n",
              "      <td>1.375763</td>\n",
              "      <td>1.063847</td>\n",
              "      <td>12.65</td>\n",
              "      <td>-8.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.735562</td>\n",
              "      <td>-0.684055</td>\n",
              "      <td>2.058215</td>\n",
              "      <td>0.716328</td>\n",
              "      <td>-0.011393</td>\n",
              "      <td>0.805396</td>\n",
              "      <td>1.497982</td>\n",
              "      <td>0.114752</td>\n",
              "      <td>0.692847</td>\n",
              "      <td>0.052377</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.178325</td>\n",
              "      <td>-0.065059</td>\n",
              "      <td>-0.724247</td>\n",
              "      <td>-1.020687</td>\n",
              "      <td>-0.751380</td>\n",
              "      <td>-0.385005</td>\n",
              "      <td>-0.012326</td>\n",
              "      <td>-0.392197</td>\n",
              "      <td>9.03</td>\n",
              "      <td>38.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.570272</td>\n",
              "      <td>0.273157</td>\n",
              "      <td>-0.279214</td>\n",
              "      <td>0.083456</td>\n",
              "      <td>1.049331</td>\n",
              "      <td>-0.869295</td>\n",
              "      <td>-0.265858</td>\n",
              "      <td>-0.401676</td>\n",
              "      <td>-0.872639</td>\n",
              "      <td>1.147483</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.919463</td>\n",
              "      <td>-0.667912</td>\n",
              "      <td>-0.820172</td>\n",
              "      <td>-0.190488</td>\n",
              "      <td>0.306974</td>\n",
              "      <td>0.119658</td>\n",
              "      <td>0.271838</td>\n",
              "      <td>1.289783</td>\n",
              "      <td>34.03</td>\n",
              "      <td>-6.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1054</th>\n",
              "      <td>0.399577</td>\n",
              "      <td>0.310805</td>\n",
              "      <td>-0.039326</td>\n",
              "      <td>-0.111546</td>\n",
              "      <td>0.304586</td>\n",
              "      <td>-0.943453</td>\n",
              "      <td>0.114960</td>\n",
              "      <td>-0.335898</td>\n",
              "      <td>0.826753</td>\n",
              "      <td>-0.393786</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.558717</td>\n",
              "      <td>0.998897</td>\n",
              "      <td>-0.106835</td>\n",
              "      <td>1.526307</td>\n",
              "      <td>0.646088</td>\n",
              "      <td>2.467278</td>\n",
              "      <td>1.867699</td>\n",
              "      <td>1.719302</td>\n",
              "      <td>-6.17</td>\n",
              "      <td>35.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1055</th>\n",
              "      <td>1.640386</td>\n",
              "      <td>1.306224</td>\n",
              "      <td>0.192745</td>\n",
              "      <td>-1.816855</td>\n",
              "      <td>-1.311906</td>\n",
              "      <td>-2.128963</td>\n",
              "      <td>-1.875967</td>\n",
              "      <td>0.094232</td>\n",
              "      <td>-1.429742</td>\n",
              "      <td>0.873777</td>\n",
              "      <td>...</td>\n",
              "      <td>0.223143</td>\n",
              "      <td>-0.032425</td>\n",
              "      <td>0.226782</td>\n",
              "      <td>0.182107</td>\n",
              "      <td>0.517466</td>\n",
              "      <td>1.126762</td>\n",
              "      <td>2.220671</td>\n",
              "      <td>4.422651</td>\n",
              "      <td>11.55</td>\n",
              "      <td>104.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1056</th>\n",
              "      <td>-0.772360</td>\n",
              "      <td>-0.670596</td>\n",
              "      <td>-0.840420</td>\n",
              "      <td>-0.832105</td>\n",
              "      <td>0.277346</td>\n",
              "      <td>1.152162</td>\n",
              "      <td>0.241470</td>\n",
              "      <td>0.229092</td>\n",
              "      <td>0.019036</td>\n",
              "      <td>-0.068804</td>\n",
              "      <td>...</td>\n",
              "      <td>0.449239</td>\n",
              "      <td>-0.965270</td>\n",
              "      <td>-0.590039</td>\n",
              "      <td>-0.804297</td>\n",
              "      <td>0.044170</td>\n",
              "      <td>-0.718175</td>\n",
              "      <td>-0.983640</td>\n",
              "      <td>-0.573822</td>\n",
              "      <td>41.33</td>\n",
              "      <td>19.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1057</th>\n",
              "      <td>-0.996965</td>\n",
              "      <td>-1.099395</td>\n",
              "      <td>3.515274</td>\n",
              "      <td>-0.508185</td>\n",
              "      <td>-1.102654</td>\n",
              "      <td>0.192081</td>\n",
              "      <td>0.069821</td>\n",
              "      <td>0.264674</td>\n",
              "      <td>-0.411533</td>\n",
              "      <td>0.501164</td>\n",
              "      <td>...</td>\n",
              "      <td>1.941398</td>\n",
              "      <td>1.769292</td>\n",
              "      <td>0.738616</td>\n",
              "      <td>1.240377</td>\n",
              "      <td>-0.546002</td>\n",
              "      <td>-0.137473</td>\n",
              "      <td>-0.781036</td>\n",
              "      <td>-0.832167</td>\n",
              "      <td>54.68</td>\n",
              "      <td>25.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1058</th>\n",
              "      <td>-0.150911</td>\n",
              "      <td>-0.094333</td>\n",
              "      <td>-0.568885</td>\n",
              "      <td>-0.614652</td>\n",
              "      <td>0.332477</td>\n",
              "      <td>-0.954948</td>\n",
              "      <td>-1.527722</td>\n",
              "      <td>-1.591471</td>\n",
              "      <td>-3.678713</td>\n",
              "      <td>-5.930209</td>\n",
              "      <td>...</td>\n",
              "      <td>5.121875</td>\n",
              "      <td>4.103031</td>\n",
              "      <td>3.673086</td>\n",
              "      <td>0.960420</td>\n",
              "      <td>1.067164</td>\n",
              "      <td>5.244305</td>\n",
              "      <td>2.506568</td>\n",
              "      <td>1.462580</td>\n",
              "      <td>54.68</td>\n",
              "      <td>25.31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1059 rows Ã— 70 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-30d2a49f-9148-4533-b251-1fe7122ce667')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-30d2a49f-9148-4533-b251-1fe7122ce667 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-30d2a49f-9148-4533-b251-1fe7122ce667');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "Data_set = pd.read_csv('/content/Reg-Data.txt',header=None)\n",
        "Data_set['lg'] = Data_set[68]\n",
        "Data_set['la'] = Data_set[69]\n",
        "Data_set = Data_set.drop([68,69],axis = 1)\n",
        "input = Data_set.drop(['lg','la'],axis = 1).values\n",
        "output = Data_set[['lg','la']].values\n",
        "Data_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc-lhoHuRaEC"
      },
      "source": [
        "## Train/Test Split\n",
        "15% of the dataset is assigned to test set. 10% of the remaining data is assigned to validation set. And the rest is assigned to training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oVff8scNRaED"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(input, output, test_size=0.2, random_state=50)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZF1U31JRaED"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4o0Tv3GRaEE"
      },
      "source": [
        "### Dealing with null values\n",
        "There are no null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n-Y2O8ORaEE",
        "outputId": "c31c8f52-e831-4b7a-bde1-558520b0f07b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     0\n",
              "1     0\n",
              "2     0\n",
              "3     0\n",
              "4     0\n",
              "     ..\n",
              "65    0\n",
              "66    0\n",
              "67    0\n",
              "lg    0\n",
              "la    0\n",
              "Length: 70, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "Data_set.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMsEhcRSRaEE"
      },
      "source": [
        "### Scaling features to a range\n",
        "68 continuous features are not between 0 and 1. So let's use MinMaxScaler to scale this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OUNrNS7DRaEF"
      },
      "outputs": [],
      "source": [
        "scaler = preprocessing.MinMaxScaler().fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntbW8oT_RaEF"
      },
      "source": [
        "### Converting the numpy arrays to tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "l0R7wZ4lRaEF"
      },
      "outputs": [],
      "source": [
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float)\n",
        "\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float).reshape(-1,2)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float).reshape(-1,2)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float).reshape(-1,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPQlnVOiRaEG"
      },
      "source": [
        "## Part 1 : Linear Regression using Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omojz_fERaEG",
        "outputId": "218eba26-d9ba-4dad-cdaf-f5f4c324cf89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss : 0.2245902532528966\n"
          ]
        }
      ],
      "source": [
        "reg = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "print(f'Loss : {reg.score(X_test, y_test)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IRXP3OfRaEG",
        "outputId": "9f257ca6-7b88-48fe-fcd8-1d9f19820f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE : 1222.2411347316672  MAE : 24.77076982524664\n"
          ]
        }
      ],
      "source": [
        "y_pred = reg.predict(X_test)\n",
        "mean_squared_error = mean_squared_error(y_test, y_pred)\n",
        "mean_absolute_error = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(f'MSE : {mean_squared_error}  MAE : {mean_absolute_error}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efQQPWYlRaEH"
      },
      "source": [
        "# Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Cz6DJsISRaEH"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, n_cont, output_size, layers, p=0.5, activation_function = nn.ReLU(inplace=True),\n",
        "                enable_dropout = False, enable_batch_normalization = False) :\n",
        "        super().__init__()\n",
        "        self.enable_batch_normalization = enable_batch_normalization\n",
        "        if enable_batch_normalization :\n",
        "            self.bn_cont = nn.BatchNorm1d(n_cont)\n",
        "        \n",
        "        layerlist = []\n",
        "        input_n = n_cont\n",
        "        if len(layers) == 0 :\n",
        "            layerlist.append(nn.Linear(input_n,output_size))\n",
        "            if activation_function != None :\n",
        "                layerlist.append(activation_function)\n",
        "            if enable_dropout == True :\n",
        "                layerlist.append(nn.Dropout(p))\n",
        "        else :\n",
        "            for i in layers:\n",
        "                layerlist.append(nn.Linear(input_n,i)) \n",
        "                if activation_function != None :\n",
        "                    layerlist.append(activation_function)\n",
        "                if enable_batch_normalization :\n",
        "                    layerlist.append(nn.BatchNorm1d(i))\n",
        "                if enable_dropout == True :\n",
        "                    layerlist.append(nn.Dropout(p))\n",
        "                input_n = i\n",
        "            layerlist.append(nn.Linear(layers[-1],output_size))\n",
        "            \n",
        "        self.layers = nn.Sequential(*layerlist)\n",
        "    \n",
        "    def forward(self, x_1):\n",
        "        if self.enable_batch_normalization :\n",
        "            x_1 = self.bn_cont(x_1)\n",
        "        x_1 = self.layers(x_1)\n",
        "        return x_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_0ldDXPGRaEI"
      },
      "outputs": [],
      "source": [
        "def train(model,criterion,optimizer,epochs) :\n",
        "    train_loss_1 = []\n",
        "    valid_loss_1 = []\n",
        "    test_loss_1 = []\n",
        "\n",
        "    for i in range(epochs):\n",
        "        i += 1\n",
        "        with torch.no_grad():\n",
        "            valid_y = model(X_val_tensor)\n",
        "            valid_loss = criterion(valid_y, y_val_tensor)\n",
        "            valid_loss_1.append(valid_loss.item())\n",
        "        with torch.no_grad():\n",
        "            valid_y = model(X_test_tensor)\n",
        "            test_loss = criterion(valid_y, y_test_tensor)\n",
        "            test_loss_1.append(test_loss.item())\n",
        "\n",
        "        prediction_y = model(X_train_tensor)\n",
        "        loss = criterion(prediction_y, y_train_tensor)\n",
        "        train_loss_1.append(loss.item())\n",
        "\n",
        "        if i%5 == 1:\n",
        "            print(f'Epoch: {i:3}  Training Loss: {loss.item():10.8f} Validation Loss: {valid_loss} Test Loss: {test_loss}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch: {i:3}  Training Loss: {loss.item():10.8f} Validation Loss: {valid_loss} Test Loss: {test_loss}')\n",
        "\n",
        "    return train_loss_1,valid_loss_1,test_loss_1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "A99_r2CxRaEI"
      },
      "outputs": [],
      "source": [
        "def show_diagrams(train_loss_1,valid_loss_1,test_loss_1) :\n",
        "    \n",
        "    plt.plot(train_loss_1)\n",
        "    plt.plot(valid_loss_1)\n",
        "    plt.plot(test_loss_1)\n",
        "    plt.title('Loss per epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train','validation', 'test'], loc='upper right')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXY_ZyHnRaEJ"
      },
      "source": [
        "# Part 2 : Neural network without any hidden layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0aqaV8FRaEJ"
      },
      "source": [
        "### Loss function : MSE & optimizer : SGD\n",
        "I got the best result with MSE loss function and SGD optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq2_8TPVRaEJ",
        "outputId": "2cf730fc-bd7e-4e02-bf25-621632dcad00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=68, out_features=2, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "torch.manual_seed(50)\n",
        "input_features = 68\n",
        "output_features = 2\n",
        "hidden_layers = []\n",
        "tr = Model(input_features, output_features, hidden_layers, p=0.5)\n",
        "tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PNa9ABjVRaEJ"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(tr.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11TDlA1ORaEJ"
      },
      "source": [
        "#### Training for 50 epochs\n",
        "The results from Linear Regression is bettet than this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jnuI2ZoRaEJ",
        "outputId": "e7663231-4634-4148-a3b3-67e125c003f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:   1  Training Loss: 1739.56823730 Validation Loss: 1546.58984375 Test Loss: 1870.268310546875\n",
            "Epoch:   6  Training Loss: 1737.81237793 Validation Loss: 1546.619140625 Test Loss: 1868.7039794921875\n",
            "Epoch:  11  Training Loss: 1736.08264160 Validation Loss: 1546.6343994140625 Test Loss: 1867.1541748046875\n",
            "Epoch:  16  Training Loss: 1734.37829590 Validation Loss: 1546.6435546875 Test Loss: 1865.623046875\n",
            "Epoch:  21  Training Loss: 1732.69848633 Validation Loss: 1546.65234375 Test Loss: 1864.1134033203125\n",
            "Epoch:  26  Training Loss: 1731.04345703 Validation Loss: 1546.6639404296875 Test Loss: 1862.6273193359375\n",
            "Epoch:  31  Training Loss: 1729.41223145 Validation Loss: 1546.6802978515625 Test Loss: 1861.1650390625\n",
            "Epoch:  36  Training Loss: 1727.80432129 Validation Loss: 1546.70166015625 Test Loss: 1859.7265625\n",
            "Epoch:  41  Training Loss: 1726.21911621 Validation Loss: 1546.726806640625 Test Loss: 1858.31103515625\n",
            "Epoch:  46  Training Loss: 1724.65649414 Validation Loss: 1546.7547607421875 Test Loss: 1856.9171142578125\n",
            "Epoch:  51  Training Loss: 1723.11608887 Validation Loss: 1546.7845458984375 Test Loss: 1855.544189453125\n",
            "Epoch:  56  Training Loss: 1721.59716797 Validation Loss: 1546.814697265625 Test Loss: 1854.1907958984375\n",
            "Epoch:  61  Training Loss: 1720.09887695 Validation Loss: 1546.8447265625 Test Loss: 1852.8564453125\n",
            "Epoch:  66  Training Loss: 1718.62182617 Validation Loss: 1546.87353515625 Test Loss: 1851.5400390625\n",
            "Epoch:  71  Training Loss: 1717.16491699 Validation Loss: 1546.9014892578125 Test Loss: 1850.241455078125\n",
            "Epoch:  76  Training Loss: 1715.72790527 Validation Loss: 1546.9283447265625 Test Loss: 1848.9600830078125\n",
            "Epoch:  81  Training Loss: 1714.31018066 Validation Loss: 1546.9534912109375 Test Loss: 1847.69580078125\n",
            "Epoch:  86  Training Loss: 1712.91174316 Validation Loss: 1546.977783203125 Test Loss: 1846.4481201171875\n",
            "Epoch:  91  Training Loss: 1711.53186035 Validation Loss: 1547.000732421875 Test Loss: 1845.217041015625\n",
            "Epoch:  96  Training Loss: 1710.17041016 Validation Loss: 1547.0225830078125 Test Loss: 1844.0020751953125\n",
            "Epoch: 100  Training Loss: 1709.09411621 Validation Loss: 1547.03955078125 Test Loss: 1843.041259765625\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "train_loss_1,valid_loss_1,test_loss_1 = train(tr,criterion,optimizer,epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "r20EpPj5RaEK",
        "outputId": "bfe21b24-1715-46c6-f57b-d69aa3f762aa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "show_diagrams(train_loss_1,valid_loss_1,test_loss_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coP9NBTtRaEK"
      },
      "source": [
        "#### Training for 10 epochs\n",
        "The results from Linear Regression is bettet than this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nT2pIRqRaEK",
        "outputId": "bc325dfd-95f8-471c-c752-e572fe00993e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:   1  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:   6  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  11  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  16  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  21  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  26  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  31  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  36  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  41  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  46  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  51  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  56  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  61  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  66  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  71  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  76  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  81  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  86  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  91  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch:  96  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 101  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 106  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 111  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 116  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 121  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 126  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 131  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 136  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 141  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 146  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 151  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 156  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 161  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 166  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 171  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 176  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 181  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 186  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 191  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 196  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 201  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 206  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 211  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 216  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 221  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 226  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 231  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 236  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 241  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 246  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 251  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 256  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 261  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 266  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 271  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 276  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 281  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 286  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 291  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 296  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 301  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 306  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 311  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 316  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 321  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 326  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 331  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 336  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 341  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 346  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 351  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 356  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 361  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 366  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 371  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 376  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 381  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 386  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 391  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 396  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 401  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 406  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 411  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 416  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 421  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 426  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 431  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 436  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 441  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 446  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 451  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 456  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 461  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 466  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 471  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 476  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 481  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 486  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 491  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 496  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 501  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 506  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 511  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 516  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 521  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 526  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 531  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 536  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 541  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 546  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 551  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 556  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 561  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 566  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 571  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 576  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 581  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 586  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 591  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 596  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 601  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 606  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 611  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 616  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 621  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 626  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 631  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 636  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 641  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 646  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 651  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 656  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 661  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 666  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 671  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 676  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 681  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 686  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 691  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 696  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 701  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 706  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 711  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 716  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 721  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 726  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 731  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 736  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 741  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 746  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 751  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 756  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 761  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 766  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 771  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 776  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 781  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 786  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 791  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 796  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 801  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 806  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 811  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 816  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 821  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 826  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 831  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 836  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 841  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 846  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 851  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 856  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 861  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 866  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 871  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 876  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 881  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 886  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 891  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 896  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 901  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 906  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 911  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 916  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 921  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 926  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 931  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 936  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 941  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 946  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 951  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 956  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 961  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 966  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 971  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 976  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 981  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 986  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 991  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 996  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n",
            "Epoch: 1000  Training Loss: 2479.64184570 Validation Loss: 2448.155029296875 Test Loss: 2744.58984375\n"
          ]
        }
      ],
      "source": [
        "epochs = 1000\n",
        "model = Model(input_features, output_features, hidden_layers, p=0.5)\n",
        "train_loss_1,valid_loss_1,test_loss_1 = train(model,criterion,optimizer,epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "eXwQ0IpiRaEK",
        "outputId": "bad0c9dc-57ab-4d52-c982-ba4e9e70f32e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "show_diagrams(train_loss_1,valid_loss_1,test_loss_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCHkw4MlRaEK"
      },
      "source": [
        "### Loss function : MSE & optimizer : Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8w3d4LJRaEL",
        "outputId": "8416d0e3-499c-4853-82c8-02f91d0fe700"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=68, out_features=2, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "torch.manual_seed(50)\n",
        "input_features = 68\n",
        "output_features = 2\n",
        "hidden_layers = []\n",
        "tr = Model(input_features, output_features, hidden_layers, p=0.5)\n",
        "tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "GL8czOpoRaEL"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(tr.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoYU7eSKRaEL"
      },
      "source": [
        "#### Training for 50 epochs\n",
        "The results from Linear Regression is bettet than this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blrAHNq3RaEL",
        "outputId": "e5eb2efe-2687-453c-87da-814b87c4da8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:   1  Training Loss: 2468.62695312 Validation Loss: 2435.549560546875 Test Loss: 2732.228515625\n",
            "Epoch:   6  Training Loss: 2464.60009766 Validation Loss: 2431.02197265625 Test Loss: 2727.78076171875\n",
            "Epoch:  11  Training Loss: 2460.58593750 Validation Loss: 2426.507080078125 Test Loss: 2723.34521484375\n",
            "Epoch:  16  Training Loss: 2456.58593750 Validation Loss: 2422.00634765625 Test Loss: 2718.9248046875\n",
            "Epoch:  21  Training Loss: 2452.60107422 Validation Loss: 2417.519775390625 Test Loss: 2714.51904296875\n",
            "Epoch:  26  Training Loss: 2448.63159180 Validation Loss: 2413.04931640625 Test Loss: 2710.130126953125\n",
            "Epoch:  31  Training Loss: 2444.67846680 Validation Loss: 2408.5947265625 Test Loss: 2705.7568359375\n",
            "Epoch:  36  Training Loss: 2440.74169922 Validation Loss: 2404.156982421875 Test Loss: 2701.401611328125\n",
            "Epoch:  41  Training Loss: 2436.82202148 Validation Loss: 2399.736083984375 Test Loss: 2697.0634765625\n",
            "Epoch:  46  Training Loss: 2432.91918945 Validation Loss: 2395.332763671875 Test Loss: 2692.742919921875\n",
            "Epoch:  51  Training Loss: 2429.03393555 Validation Loss: 2390.94677734375 Test Loss: 2688.4404296875\n",
            "Epoch:  56  Training Loss: 2425.16577148 Validation Loss: 2386.57861328125 Test Loss: 2684.156005859375\n",
            "Epoch:  61  Training Loss: 2421.31494141 Validation Loss: 2382.2275390625 Test Loss: 2679.8896484375\n",
            "Epoch:  66  Training Loss: 2417.48168945 Validation Loss: 2377.894287109375 Test Loss: 2675.6416015625\n",
            "Epoch:  71  Training Loss: 2413.66552734 Validation Loss: 2373.5791015625 Test Loss: 2671.410888671875\n",
            "Epoch:  76  Training Loss: 2409.86669922 Validation Loss: 2369.281005859375 Test Loss: 2667.19873046875\n",
            "Epoch:  81  Training Loss: 2406.08520508 Validation Loss: 2365.000732421875 Test Loss: 2663.00439453125\n",
            "Epoch:  86  Training Loss: 2402.32104492 Validation Loss: 2360.73779296875 Test Loss: 2658.827880859375\n",
            "Epoch:  91  Training Loss: 2398.57421875 Validation Loss: 2356.4921875 Test Loss: 2654.669189453125\n",
            "Epoch:  96  Training Loss: 2394.84423828 Validation Loss: 2352.264404296875 Test Loss: 2650.528564453125\n",
            "Epoch: 100  Training Loss: 2391.87255859 Validation Loss: 2348.894775390625 Test Loss: 2647.228759765625\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "train_loss_1,valid_loss_1,test_loss_1 = train(tr,criterion,optimizer,epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niCaPwMMRaEL",
        "outputId": "9e511586-f5f4-43b3-d04c-a1f69d655c63"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "show_diagrams(training_losses,valid_losses,test_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow2cUa_ERaEM"
      },
      "source": [
        "#### Training for 10 epochs\n",
        "The results from Linear Regression is bettet than this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSuJax4WRaEM",
        "outputId": "36a4cc81-c623-4b72-ba57-bd31f1c0586d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:   1  Training Loss: 2391.13134766 Validation Loss: 2348.053955078125 Test Loss: 2646.40576171875\n",
            "Epoch:   6  Training Loss: 2387.43530273 Validation Loss: 2343.86083984375 Test Loss: 2642.30078125\n",
            "Epoch:  11  Training Loss: 2383.75659180 Validation Loss: 2339.684814453125 Test Loss: 2638.212890625\n",
            "Epoch:  16  Training Loss: 2380.09472656 Validation Loss: 2335.5263671875 Test Loss: 2634.143310546875\n",
            "Epoch:  21  Training Loss: 2376.44970703 Validation Loss: 2331.385009765625 Test Loss: 2630.0908203125\n",
            "Epoch:  26  Training Loss: 2372.82153320 Validation Loss: 2327.2607421875 Test Loss: 2626.055908203125\n",
            "Epoch:  31  Training Loss: 2369.21020508 Validation Loss: 2323.153564453125 Test Loss: 2622.038818359375\n",
            "Epoch:  36  Training Loss: 2365.61523438 Validation Loss: 2319.063232421875 Test Loss: 2618.038818359375\n",
            "Epoch:  41  Training Loss: 2362.03759766 Validation Loss: 2314.990478515625 Test Loss: 2614.056640625\n",
            "Epoch:  46  Training Loss: 2358.47631836 Validation Loss: 2310.93408203125 Test Loss: 2610.091064453125\n",
            "Epoch:  51  Training Loss: 2354.93115234 Validation Loss: 2306.894775390625 Test Loss: 2606.143310546875\n",
            "Epoch:  56  Training Loss: 2351.40283203 Validation Loss: 2302.872314453125 Test Loss: 2602.212158203125\n",
            "Epoch:  61  Training Loss: 2347.89086914 Validation Loss: 2298.866455078125 Test Loss: 2598.298828125\n",
            "Epoch:  66  Training Loss: 2344.39550781 Validation Loss: 2294.877685546875 Test Loss: 2594.40234375\n",
            "Epoch:  71  Training Loss: 2340.91674805 Validation Loss: 2290.90576171875 Test Loss: 2590.52294921875\n",
            "Epoch:  76  Training Loss: 2337.45385742 Validation Loss: 2286.949951171875 Test Loss: 2586.66064453125\n",
            "Epoch:  81  Training Loss: 2334.00732422 Validation Loss: 2283.010986328125 Test Loss: 2582.815185546875\n",
            "Epoch:  86  Training Loss: 2330.57666016 Validation Loss: 2279.088623046875 Test Loss: 2578.98681640625\n",
            "Epoch:  91  Training Loss: 2327.16284180 Validation Loss: 2275.1826171875 Test Loss: 2575.175048828125\n",
            "Epoch:  96  Training Loss: 2323.76464844 Validation Loss: 2271.29345703125 Test Loss: 2571.380126953125\n",
            "Epoch: 101  Training Loss: 2320.38281250 Validation Loss: 2267.420166015625 Test Loss: 2567.601806640625\n",
            "Epoch: 106  Training Loss: 2317.01635742 Validation Loss: 2263.563720703125 Test Loss: 2563.84033203125\n",
            "Epoch: 111  Training Loss: 2313.66650391 Validation Loss: 2259.7236328125 Test Loss: 2560.09521484375\n",
            "Epoch: 116  Training Loss: 2310.33227539 Validation Loss: 2255.899169921875 Test Loss: 2556.367431640625\n",
            "Epoch: 121  Training Loss: 2307.01416016 Validation Loss: 2252.09130859375 Test Loss: 2552.65576171875\n",
            "Epoch: 126  Training Loss: 2303.71166992 Validation Loss: 2248.299560546875 Test Loss: 2548.960205078125\n",
            "Epoch: 131  Training Loss: 2300.42456055 Validation Loss: 2244.524169921875 Test Loss: 2545.281494140625\n",
            "Epoch: 136  Training Loss: 2297.15380859 Validation Loss: 2240.7646484375 Test Loss: 2541.619140625\n",
            "Epoch: 141  Training Loss: 2293.89819336 Validation Loss: 2237.021240234375 Test Loss: 2537.97314453125\n",
            "Epoch: 146  Training Loss: 2290.65844727 Validation Loss: 2233.29345703125 Test Loss: 2534.34326171875\n",
            "Epoch: 151  Training Loss: 2287.43432617 Validation Loss: 2229.58203125 Test Loss: 2530.729736328125\n",
            "Epoch: 156  Training Loss: 2284.22534180 Validation Loss: 2225.886474609375 Test Loss: 2527.13232421875\n",
            "Epoch: 161  Training Loss: 2281.03173828 Validation Loss: 2222.20654296875 Test Loss: 2523.55126953125\n",
            "Epoch: 166  Training Loss: 2277.85400391 Validation Loss: 2218.54248046875 Test Loss: 2519.986083984375\n",
            "Epoch: 171  Training Loss: 2274.69116211 Validation Loss: 2214.89404296875 Test Loss: 2516.43701171875\n",
            "Epoch: 176  Training Loss: 2271.54394531 Validation Loss: 2211.261474609375 Test Loss: 2512.903564453125\n",
            "Epoch: 181  Training Loss: 2268.41210938 Validation Loss: 2207.64453125 Test Loss: 2509.386474609375\n",
            "Epoch: 186  Training Loss: 2265.29516602 Validation Loss: 2204.04296875 Test Loss: 2505.88525390625\n",
            "Epoch: 191  Training Loss: 2262.19335938 Validation Loss: 2200.457275390625 Test Loss: 2502.399658203125\n",
            "Epoch: 196  Training Loss: 2259.10668945 Validation Loss: 2196.88671875 Test Loss: 2498.93017578125\n",
            "Epoch: 201  Training Loss: 2256.03515625 Validation Loss: 2193.33203125 Test Loss: 2495.476318359375\n",
            "Epoch: 206  Training Loss: 2252.97875977 Validation Loss: 2189.79248046875 Test Loss: 2492.0380859375\n",
            "Epoch: 211  Training Loss: 2249.93701172 Validation Loss: 2186.268310546875 Test Loss: 2488.615478515625\n",
            "Epoch: 216  Training Loss: 2246.91040039 Validation Loss: 2182.759765625 Test Loss: 2485.20849609375\n",
            "Epoch: 221  Training Loss: 2243.89868164 Validation Loss: 2179.26611328125 Test Loss: 2481.817138671875\n",
            "Epoch: 226  Training Loss: 2240.90136719 Validation Loss: 2175.78759765625 Test Loss: 2478.44091796875\n",
            "Epoch: 231  Training Loss: 2237.91918945 Validation Loss: 2172.32470703125 Test Loss: 2475.08056640625\n",
            "Epoch: 236  Training Loss: 2234.95166016 Validation Loss: 2168.876953125 Test Loss: 2471.735595703125\n",
            "Epoch: 241  Training Loss: 2231.99877930 Validation Loss: 2165.44384765625 Test Loss: 2468.40576171875\n",
            "Epoch: 246  Training Loss: 2229.06030273 Validation Loss: 2162.02587890625 Test Loss: 2465.09130859375\n",
            "Epoch: 251  Training Loss: 2226.13671875 Validation Loss: 2158.623046875 Test Loss: 2461.792236328125\n",
            "Epoch: 256  Training Loss: 2223.22729492 Validation Loss: 2155.2353515625 Test Loss: 2458.50830078125\n",
            "Epoch: 261  Training Loss: 2220.33276367 Validation Loss: 2151.862060546875 Test Loss: 2455.2392578125\n",
            "Epoch: 266  Training Loss: 2217.45239258 Validation Loss: 2148.50439453125 Test Loss: 2451.985595703125\n",
            "Epoch: 271  Training Loss: 2214.58618164 Validation Loss: 2145.160888671875 Test Loss: 2448.746826171875\n",
            "Epoch: 276  Training Loss: 2211.73486328 Validation Loss: 2141.832275390625 Test Loss: 2445.523193359375\n",
            "Epoch: 281  Training Loss: 2208.89721680 Validation Loss: 2138.518310546875 Test Loss: 2442.314453125\n",
            "Epoch: 286  Training Loss: 2206.07421875 Validation Loss: 2135.219482421875 Test Loss: 2439.12060546875\n",
            "Epoch: 291  Training Loss: 2203.26538086 Validation Loss: 2131.934814453125 Test Loss: 2435.94189453125\n",
            "Epoch: 296  Training Loss: 2200.47045898 Validation Loss: 2128.664794921875 Test Loss: 2432.77783203125\n",
            "Epoch: 301  Training Loss: 2197.68994141 Validation Loss: 2125.40966796875 Test Loss: 2429.62841796875\n",
            "Epoch: 306  Training Loss: 2194.92333984 Validation Loss: 2122.168701171875 Test Loss: 2426.493896484375\n",
            "Epoch: 311  Training Loss: 2192.17065430 Validation Loss: 2118.9423828125 Test Loss: 2423.373779296875\n",
            "Epoch: 316  Training Loss: 2189.43164062 Validation Loss: 2115.73046875 Test Loss: 2420.268798828125\n",
            "Epoch: 321  Training Loss: 2186.70727539 Validation Loss: 2112.53271484375 Test Loss: 2417.177978515625\n",
            "Epoch: 326  Training Loss: 2183.99609375 Validation Loss: 2109.349365234375 Test Loss: 2414.101806640625\n",
            "Epoch: 331  Training Loss: 2181.29882812 Validation Loss: 2106.180419921875 Test Loss: 2411.0400390625\n",
            "Epoch: 336  Training Loss: 2178.61547852 Validation Loss: 2103.025634765625 Test Loss: 2407.992919921875\n",
            "Epoch: 341  Training Loss: 2175.94580078 Validation Loss: 2099.88525390625 Test Loss: 2404.9599609375\n",
            "Epoch: 346  Training Loss: 2173.28979492 Validation Loss: 2096.7587890625 Test Loss: 2401.94189453125\n",
            "Epoch: 351  Training Loss: 2170.64721680 Validation Loss: 2093.646240234375 Test Loss: 2398.9375\n",
            "Epoch: 356  Training Loss: 2168.01831055 Validation Loss: 2090.548095703125 Test Loss: 2395.94775390625\n",
            "Epoch: 361  Training Loss: 2165.40332031 Validation Loss: 2087.4638671875 Test Loss: 2392.9716796875\n",
            "Epoch: 366  Training Loss: 2162.80126953 Validation Loss: 2084.3935546875 Test Loss: 2390.01025390625\n",
            "Epoch: 371  Training Loss: 2160.21313477 Validation Loss: 2081.33740234375 Test Loss: 2387.06298828125\n",
            "Epoch: 376  Training Loss: 2157.63769531 Validation Loss: 2078.294921875 Test Loss: 2384.12939453125\n",
            "Epoch: 381  Training Loss: 2155.07617188 Validation Loss: 2075.26611328125 Test Loss: 2381.210205078125\n",
            "Epoch: 386  Training Loss: 2152.52783203 Validation Loss: 2072.25146484375 Test Loss: 2378.304931640625\n",
            "Epoch: 391  Training Loss: 2149.99267578 Validation Loss: 2069.25048828125 Test Loss: 2375.41357421875\n",
            "Epoch: 396  Training Loss: 2147.47119141 Validation Loss: 2066.262939453125 Test Loss: 2372.5361328125\n",
            "Epoch: 401  Training Loss: 2144.96215820 Validation Loss: 2063.2890625 Test Loss: 2369.67236328125\n",
            "Epoch: 406  Training Loss: 2142.46655273 Validation Loss: 2060.329345703125 Test Loss: 2366.822509765625\n",
            "Epoch: 411  Training Loss: 2139.98388672 Validation Loss: 2057.3828125 Test Loss: 2363.986328125\n",
            "Epoch: 416  Training Loss: 2137.51440430 Validation Loss: 2054.44970703125 Test Loss: 2361.163818359375\n",
            "Epoch: 421  Training Loss: 2135.05810547 Validation Loss: 2051.530517578125 Test Loss: 2358.35546875\n",
            "Epoch: 426  Training Loss: 2132.61425781 Validation Loss: 2048.62451171875 Test Loss: 2355.560302734375\n",
            "Epoch: 431  Training Loss: 2130.18383789 Validation Loss: 2045.73193359375 Test Loss: 2352.778564453125\n",
            "Epoch: 436  Training Loss: 2127.76586914 Validation Loss: 2042.8529052734375 Test Loss: 2350.010498046875\n",
            "Epoch: 441  Training Loss: 2125.36083984 Validation Loss: 2039.9871826171875 Test Loss: 2347.256103515625\n",
            "Epoch: 446  Training Loss: 2122.96826172 Validation Loss: 2037.1343994140625 Test Loss: 2344.51513671875\n",
            "Epoch: 451  Training Loss: 2120.58886719 Validation Loss: 2034.295166015625 Test Loss: 2341.787353515625\n",
            "Epoch: 456  Training Loss: 2118.22167969 Validation Loss: 2031.4691162109375 Test Loss: 2339.072998046875\n",
            "Epoch: 461  Training Loss: 2115.86743164 Validation Loss: 2028.656005859375 Test Loss: 2336.3720703125\n",
            "Epoch: 466  Training Loss: 2113.52563477 Validation Loss: 2025.8564453125 Test Loss: 2333.68408203125\n",
            "Epoch: 471  Training Loss: 2111.19628906 Validation Loss: 2023.0694580078125 Test Loss: 2331.009521484375\n",
            "Epoch: 476  Training Loss: 2108.87988281 Validation Loss: 2020.2958984375 Test Loss: 2328.34814453125\n",
            "Epoch: 481  Training Loss: 2106.57543945 Validation Loss: 2017.534912109375 Test Loss: 2325.699951171875\n",
            "Epoch: 486  Training Loss: 2104.28344727 Validation Loss: 2014.787353515625 Test Loss: 2323.06494140625\n",
            "Epoch: 491  Training Loss: 2102.00390625 Validation Loss: 2012.0523681640625 Test Loss: 2320.442626953125\n",
            "Epoch: 496  Training Loss: 2099.73681641 Validation Loss: 2009.33056640625 Test Loss: 2317.833251953125\n",
            "Epoch: 501  Training Loss: 2097.48168945 Validation Loss: 2006.62109375 Test Loss: 2315.237060546875\n",
            "Epoch: 506  Training Loss: 2095.23901367 Validation Loss: 2003.925048828125 Test Loss: 2312.65380859375\n",
            "Epoch: 511  Training Loss: 2093.00854492 Validation Loss: 2001.2413330078125 Test Loss: 2310.08349609375\n",
            "Epoch: 516  Training Loss: 2090.78979492 Validation Loss: 1998.570556640625 Test Loss: 2307.525390625\n",
            "Epoch: 521  Training Loss: 2088.58374023 Validation Loss: 1995.912109375 Test Loss: 2304.98095703125\n",
            "Epoch: 526  Training Loss: 2086.38916016 Validation Loss: 1993.266357421875 Test Loss: 2302.44873046875\n",
            "Epoch: 531  Training Loss: 2084.20703125 Validation Loss: 1990.63330078125 Test Loss: 2299.92919921875\n",
            "Epoch: 536  Training Loss: 2082.03662109 Validation Loss: 1988.0128173828125 Test Loss: 2297.42236328125\n",
            "Epoch: 541  Training Loss: 2079.87817383 Validation Loss: 1985.4049072265625 Test Loss: 2294.92822265625\n",
            "Epoch: 546  Training Loss: 2077.73168945 Validation Loss: 1982.809326171875 Test Loss: 2292.446533203125\n",
            "Epoch: 551  Training Loss: 2075.59692383 Validation Loss: 1980.226318359375 Test Loss: 2289.977294921875\n",
            "Epoch: 556  Training Loss: 2073.47412109 Validation Loss: 1977.655517578125 Test Loss: 2287.520751953125\n",
            "Epoch: 561  Training Loss: 2071.36279297 Validation Loss: 1975.096923828125 Test Loss: 2285.076416015625\n",
            "Epoch: 566  Training Loss: 2069.26318359 Validation Loss: 1972.55078125 Test Loss: 2282.64453125\n",
            "Epoch: 571  Training Loss: 2067.17529297 Validation Loss: 1970.0169677734375 Test Loss: 2280.22509765625\n",
            "Epoch: 576  Training Loss: 2065.09912109 Validation Loss: 1967.495361328125 Test Loss: 2277.81787109375\n",
            "Epoch: 581  Training Loss: 2063.03442383 Validation Loss: 1964.9857177734375 Test Loss: 2275.4228515625\n",
            "Epoch: 586  Training Loss: 2060.98120117 Validation Loss: 1962.4884033203125 Test Loss: 2273.0400390625\n",
            "Epoch: 591  Training Loss: 2058.93969727 Validation Loss: 1960.0032958984375 Test Loss: 2270.66943359375\n",
            "Epoch: 596  Training Loss: 2056.90942383 Validation Loss: 1957.5301513671875 Test Loss: 2268.310791015625\n",
            "Epoch: 601  Training Loss: 2054.89062500 Validation Loss: 1955.0689697265625 Test Loss: 2265.96435546875\n",
            "Epoch: 606  Training Loss: 2052.88330078 Validation Loss: 1952.6195068359375 Test Loss: 2263.630126953125\n",
            "Epoch: 611  Training Loss: 2050.88671875 Validation Loss: 1950.182373046875 Test Loss: 2261.307861328125\n",
            "Epoch: 616  Training Loss: 2048.90209961 Validation Loss: 1947.7568359375 Test Loss: 2258.997314453125\n",
            "Epoch: 621  Training Loss: 2046.92846680 Validation Loss: 1945.3433837890625 Test Loss: 2256.69873046875\n",
            "Epoch: 626  Training Loss: 2044.96582031 Validation Loss: 1942.9415283203125 Test Loss: 2254.412109375\n",
            "Epoch: 631  Training Loss: 2043.01464844 Validation Loss: 1940.5516357421875 Test Loss: 2252.13720703125\n",
            "Epoch: 636  Training Loss: 2041.07446289 Validation Loss: 1938.1737060546875 Test Loss: 2249.874267578125\n",
            "Epoch: 641  Training Loss: 2039.14514160 Validation Loss: 1935.80712890625 Test Loss: 2247.623046875\n",
            "Epoch: 646  Training Loss: 2037.22705078 Validation Loss: 1933.4521484375 Test Loss: 2245.383544921875\n",
            "Epoch: 651  Training Loss: 2035.32006836 Validation Loss: 1931.109130859375 Test Loss: 2243.155517578125\n",
            "Epoch: 656  Training Loss: 2033.42382812 Validation Loss: 1928.777587890625 Test Loss: 2240.939453125\n",
            "Epoch: 661  Training Loss: 2031.53833008 Validation Loss: 1926.4573974609375 Test Loss: 2238.734619140625\n",
            "Epoch: 666  Training Loss: 2029.66418457 Validation Loss: 1924.148681640625 Test Loss: 2236.54150390625\n",
            "Epoch: 671  Training Loss: 2027.80017090 Validation Loss: 1921.8514404296875 Test Loss: 2234.359619140625\n",
            "Epoch: 676  Training Loss: 2025.94750977 Validation Loss: 1919.5657958984375 Test Loss: 2232.189453125\n",
            "Epoch: 681  Training Loss: 2024.10534668 Validation Loss: 1917.291748046875 Test Loss: 2230.03076171875\n",
            "Epoch: 686  Training Loss: 2022.27380371 Validation Loss: 1915.0286865234375 Test Loss: 2227.88330078125\n",
            "Epoch: 691  Training Loss: 2020.45275879 Validation Loss: 1912.7769775390625 Test Loss: 2225.747314453125\n",
            "Epoch: 696  Training Loss: 2018.64270020 Validation Loss: 1910.53662109375 Test Loss: 2223.622314453125\n",
            "Epoch: 701  Training Loss: 2016.84289551 Validation Loss: 1908.307373046875 Test Loss: 2221.5087890625\n",
            "Epoch: 706  Training Loss: 2015.05383301 Validation Loss: 1906.08935546875 Test Loss: 2219.40625\n",
            "Epoch: 711  Training Loss: 2013.27526855 Validation Loss: 1903.8826904296875 Test Loss: 2217.315185546875\n",
            "Epoch: 716  Training Loss: 2011.50683594 Validation Loss: 1901.6868896484375 Test Loss: 2215.23486328125\n",
            "Epoch: 721  Training Loss: 2009.74902344 Validation Loss: 1899.50244140625 Test Loss: 2213.166015625\n",
            "Epoch: 726  Training Loss: 2008.00158691 Validation Loss: 1897.328857421875 Test Loss: 2211.108154296875\n",
            "Epoch: 731  Training Loss: 2006.26440430 Validation Loss: 1895.1661376953125 Test Loss: 2209.06103515625\n",
            "Epoch: 736  Training Loss: 2004.53735352 Validation Loss: 1893.0145263671875 Test Loss: 2207.02490234375\n",
            "Epoch: 741  Training Loss: 2002.82092285 Validation Loss: 1890.8739013671875 Test Loss: 2205.0\n",
            "Epoch: 746  Training Loss: 2001.11413574 Validation Loss: 1888.7442626953125 Test Loss: 2202.985595703125\n",
            "Epoch: 751  Training Loss: 1999.41796875 Validation Loss: 1886.6253662109375 Test Loss: 2200.982421875\n",
            "Epoch: 756  Training Loss: 1997.73156738 Validation Loss: 1884.51708984375 Test Loss: 2198.98974609375\n",
            "Epoch: 761  Training Loss: 1996.05566406 Validation Loss: 1882.4197998046875 Test Loss: 2197.008056640625\n",
            "Epoch: 766  Training Loss: 1994.38940430 Validation Loss: 1880.3331298828125 Test Loss: 2195.036865234375\n",
            "Epoch: 771  Training Loss: 1992.73327637 Validation Loss: 1878.25732421875 Test Loss: 2193.076171875\n",
            "Epoch: 776  Training Loss: 1991.08728027 Validation Loss: 1876.19189453125 Test Loss: 2191.12646484375\n",
            "Epoch: 781  Training Loss: 1989.45080566 Validation Loss: 1874.1373291015625 Test Loss: 2189.187255859375\n",
            "Epoch: 786  Training Loss: 1987.82446289 Validation Loss: 1872.0931396484375 Test Loss: 2187.2587890625\n",
            "Epoch: 791  Training Loss: 1986.20800781 Validation Loss: 1870.0596923828125 Test Loss: 2185.340576171875\n",
            "Epoch: 796  Training Loss: 1984.60119629 Validation Loss: 1868.0369873046875 Test Loss: 2183.432861328125\n",
            "Epoch: 801  Training Loss: 1983.00415039 Validation Loss: 1866.0242919921875 Test Loss: 2181.53564453125\n",
            "Epoch: 806  Training Loss: 1981.41687012 Validation Loss: 1864.022216796875 Test Loss: 2179.648681640625\n",
            "Epoch: 811  Training Loss: 1979.83923340 Validation Loss: 1862.030517578125 Test Loss: 2177.7724609375\n",
            "Epoch: 816  Training Loss: 1978.27136230 Validation Loss: 1860.04931640625 Test Loss: 2175.90625\n",
            "Epoch: 821  Training Loss: 1976.71289062 Validation Loss: 1858.078369140625 Test Loss: 2174.05029296875\n",
            "Epoch: 826  Training Loss: 1975.16381836 Validation Loss: 1856.11767578125 Test Loss: 2172.20458984375\n",
            "Epoch: 831  Training Loss: 1973.62463379 Validation Loss: 1854.1671142578125 Test Loss: 2170.369384765625\n",
            "Epoch: 836  Training Loss: 1972.09460449 Validation Loss: 1852.22705078125 Test Loss: 2168.544189453125\n",
            "Epoch: 841  Training Loss: 1970.57434082 Validation Loss: 1850.296875 Test Loss: 2166.72900390625\n",
            "Epoch: 846  Training Loss: 1969.06311035 Validation Loss: 1848.377197265625 Test Loss: 2164.923828125\n",
            "Epoch: 851  Training Loss: 1967.56140137 Validation Loss: 1846.4674072265625 Test Loss: 2163.12890625\n",
            "Epoch: 856  Training Loss: 1966.06909180 Validation Loss: 1844.56787109375 Test Loss: 2161.343994140625\n",
            "Epoch: 861  Training Loss: 1964.58593750 Validation Loss: 1842.6781005859375 Test Loss: 2159.568603515625\n",
            "Epoch: 866  Training Loss: 1963.11181641 Validation Loss: 1840.79833984375 Test Loss: 2157.803466796875\n",
            "Epoch: 871  Training Loss: 1961.64733887 Validation Loss: 1838.9287109375 Test Loss: 2156.04833984375\n",
            "Epoch: 876  Training Loss: 1960.19177246 Validation Loss: 1837.0689697265625 Test Loss: 2154.302978515625\n",
            "Epoch: 881  Training Loss: 1958.74536133 Validation Loss: 1835.2191162109375 Test Loss: 2152.567138671875\n",
            "Epoch: 886  Training Loss: 1957.30810547 Validation Loss: 1833.3792724609375 Test Loss: 2150.841552734375\n",
            "Epoch: 891  Training Loss: 1955.87976074 Validation Loss: 1831.5487060546875 Test Loss: 2149.125244140625\n",
            "Epoch: 896  Training Loss: 1954.46032715 Validation Loss: 1829.728515625 Test Loss: 2147.419189453125\n",
            "Epoch: 901  Training Loss: 1953.05017090 Validation Loss: 1827.9176025390625 Test Loss: 2145.72216796875\n",
            "Epoch: 906  Training Loss: 1951.64868164 Validation Loss: 1826.116943359375 Test Loss: 2144.03515625\n",
            "Epoch: 911  Training Loss: 1950.25622559 Validation Loss: 1824.32568359375 Test Loss: 2142.357666015625\n",
            "Epoch: 916  Training Loss: 1948.87268066 Validation Loss: 1822.543701171875 Test Loss: 2140.689453125\n",
            "Epoch: 921  Training Loss: 1947.49768066 Validation Loss: 1820.7718505859375 Test Loss: 2139.031005859375\n",
            "Epoch: 926  Training Loss: 1946.13159180 Validation Loss: 1819.0093994140625 Test Loss: 2137.3818359375\n",
            "Epoch: 931  Training Loss: 1944.77429199 Validation Loss: 1817.2562255859375 Test Loss: 2135.742431640625\n",
            "Epoch: 936  Training Loss: 1943.42553711 Validation Loss: 1815.5130615234375 Test Loss: 2134.112060546875\n",
            "Epoch: 941  Training Loss: 1942.08557129 Validation Loss: 1813.779052734375 Test Loss: 2132.4912109375\n",
            "Epoch: 946  Training Loss: 1940.75427246 Validation Loss: 1812.054443359375 Test Loss: 2130.879638671875\n",
            "Epoch: 951  Training Loss: 1939.43139648 Validation Loss: 1810.33935546875 Test Loss: 2129.27734375\n",
            "Epoch: 956  Training Loss: 1938.11743164 Validation Loss: 1808.6334228515625 Test Loss: 2127.684326171875\n",
            "Epoch: 961  Training Loss: 1936.81152344 Validation Loss: 1806.9368896484375 Test Loss: 2126.100341796875\n",
            "Epoch: 966  Training Loss: 1935.51416016 Validation Loss: 1805.25 Test Loss: 2124.525634765625\n",
            "Epoch: 971  Training Loss: 1934.22534180 Validation Loss: 1803.5718994140625 Test Loss: 2122.960205078125\n",
            "Epoch: 976  Training Loss: 1932.94482422 Validation Loss: 1801.9033203125 Test Loss: 2121.403564453125\n",
            "Epoch: 981  Training Loss: 1931.67272949 Validation Loss: 1800.2437744140625 Test Loss: 2119.85595703125\n",
            "Epoch: 986  Training Loss: 1930.40893555 Validation Loss: 1798.5931396484375 Test Loss: 2118.317626953125\n",
            "Epoch: 991  Training Loss: 1929.15332031 Validation Loss: 1796.9517822265625 Test Loss: 2116.7880859375\n",
            "Epoch: 996  Training Loss: 1927.90612793 Validation Loss: 1795.31982421875 Test Loss: 2115.267333984375\n",
            "Epoch: 1000  Training Loss: 1926.91406250 Validation Loss: 1794.0206298828125 Test Loss: 2114.057373046875\n"
          ]
        }
      ],
      "source": [
        "epochs = 1000\n",
        "model = Model(input_features, output_features, hidden_layers, p=0.5)\n",
        "train_loss_1,valid_loss_1,test_loss_1 = train(tr,criterion,optimizer,epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "7wLjIqvrRaEM",
        "outputId": "06d11b74-3103-4887-cd5f-84a087ee7ab9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "show_diagrams(train_loss_1,valid_loss_1,test_loss_1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10000\n",
        "model = Model(input_features, output_features, hidden_layers, p=0.5)\n",
        "train_loss_1,valid_loss_1,test_loss_1 = train(tr,criterion,optimizer,epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFLyfMeWXjNf",
        "outputId": "8149d2a6-f80c-494e-a512-bf588d630e26"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:   1  Training Loss: 1926.66711426 Validation Loss: 1793.696533203125 Test Loss: 2113.755859375\n",
            "Epoch:   6  Training Loss: 1925.43603516 Validation Loss: 1792.0823974609375 Test Loss: 2112.2529296875\n",
            "Epoch:  11  Training Loss: 1924.21337891 Validation Loss: 1790.4774169921875 Test Loss: 2110.759033203125\n",
            "Epoch:  16  Training Loss: 1922.99865723 Validation Loss: 1788.8812255859375 Test Loss: 2109.27392578125\n",
            "Epoch:  21  Training Loss: 1921.79211426 Validation Loss: 1787.2935791015625 Test Loss: 2107.79736328125\n",
            "Epoch:  26  Training Loss: 1920.59338379 Validation Loss: 1785.7152099609375 Test Loss: 2106.32958984375\n",
            "Epoch:  31  Training Loss: 1919.40270996 Validation Loss: 1784.1456298828125 Test Loss: 2104.87060546875\n",
            "Epoch:  36  Training Loss: 1918.21997070 Validation Loss: 1782.5845947265625 Test Loss: 2103.420166015625\n",
            "Epoch:  41  Training Loss: 1917.04528809 Validation Loss: 1781.03271484375 Test Loss: 2101.978271484375\n",
            "Epoch:  46  Training Loss: 1915.87829590 Validation Loss: 1779.4893798828125 Test Loss: 2100.544921875\n",
            "Epoch:  51  Training Loss: 1914.71936035 Validation Loss: 1777.95458984375 Test Loss: 2099.1201171875\n",
            "Epoch:  56  Training Loss: 1913.56787109 Validation Loss: 1776.4287109375 Test Loss: 2097.7041015625\n",
            "Epoch:  61  Training Loss: 1912.42431641 Validation Loss: 1774.911376953125 Test Loss: 2096.296142578125\n",
            "Epoch:  66  Training Loss: 1911.28857422 Validation Loss: 1773.4027099609375 Test Loss: 2094.896728515625\n",
            "Epoch:  71  Training Loss: 1910.16064453 Validation Loss: 1771.90234375 Test Loss: 2093.505859375\n",
            "Epoch:  76  Training Loss: 1909.04016113 Validation Loss: 1770.410888671875 Test Loss: 2092.123046875\n",
            "Epoch:  81  Training Loss: 1907.92712402 Validation Loss: 1768.927734375 Test Loss: 2090.748779296875\n",
            "Epoch:  86  Training Loss: 1906.82189941 Validation Loss: 1767.453125 Test Loss: 2089.382568359375\n",
            "Epoch:  91  Training Loss: 1905.72412109 Validation Loss: 1765.98681640625 Test Loss: 2088.024658203125\n",
            "Epoch:  96  Training Loss: 1904.63391113 Validation Loss: 1764.52880859375 Test Loss: 2086.675048828125\n",
            "Epoch: 101  Training Loss: 1903.55102539 Validation Loss: 1763.07958984375 Test Loss: 2085.333740234375\n",
            "Epoch: 106  Training Loss: 1902.47570801 Validation Loss: 1761.63818359375 Test Loss: 2084.000244140625\n",
            "Epoch: 111  Training Loss: 1901.40759277 Validation Loss: 1760.205322265625 Test Loss: 2082.6748046875\n",
            "Epoch: 116  Training Loss: 1900.34716797 Validation Loss: 1758.7806396484375 Test Loss: 2081.35791015625\n",
            "Epoch: 121  Training Loss: 1899.29382324 Validation Loss: 1757.364501953125 Test Loss: 2080.048583984375\n",
            "Epoch: 126  Training Loss: 1898.24768066 Validation Loss: 1755.956298828125 Test Loss: 2078.747314453125\n",
            "Epoch: 131  Training Loss: 1897.20898438 Validation Loss: 1754.5562744140625 Test Loss: 2077.4541015625\n",
            "Epoch: 136  Training Loss: 1896.17749023 Validation Loss: 1753.16455078125 Test Loss: 2076.168701171875\n",
            "Epoch: 141  Training Loss: 1895.15319824 Validation Loss: 1751.7806396484375 Test Loss: 2074.89111328125\n",
            "Epoch: 146  Training Loss: 1894.13598633 Validation Loss: 1750.4051513671875 Test Loss: 2073.62158203125\n",
            "Epoch: 151  Training Loss: 1893.12585449 Validation Loss: 1749.037109375 Test Loss: 2072.35986328125\n",
            "Epoch: 156  Training Loss: 1892.12292480 Validation Loss: 1747.6776123046875 Test Loss: 2071.10595703125\n",
            "Epoch: 161  Training Loss: 1891.12683105 Validation Loss: 1746.325927734375 Test Loss: 2069.859375\n",
            "Epoch: 166  Training Loss: 1890.13781738 Validation Loss: 1744.982177734375 Test Loss: 2068.62109375\n",
            "Epoch: 171  Training Loss: 1889.15588379 Validation Loss: 1743.6463623046875 Test Loss: 2067.389892578125\n",
            "Epoch: 176  Training Loss: 1888.18078613 Validation Loss: 1742.318359375 Test Loss: 2066.166748046875\n",
            "Epoch: 181  Training Loss: 1887.21240234 Validation Loss: 1740.9979248046875 Test Loss: 2064.950927734375\n",
            "Epoch: 186  Training Loss: 1886.25109863 Validation Loss: 1739.6856689453125 Test Loss: 2063.74267578125\n",
            "Epoch: 191  Training Loss: 1885.29663086 Validation Loss: 1738.381103515625 Test Loss: 2062.542236328125\n",
            "Epoch: 196  Training Loss: 1884.34887695 Validation Loss: 1737.0843505859375 Test Loss: 2061.348876953125\n",
            "Epoch: 201  Training Loss: 1883.40795898 Validation Loss: 1735.79541015625 Test Loss: 2060.1630859375\n",
            "Epoch: 206  Training Loss: 1882.47375488 Validation Loss: 1734.5137939453125 Test Loss: 2058.98486328125\n",
            "Epoch: 211  Training Loss: 1881.54614258 Validation Loss: 1733.2398681640625 Test Loss: 2057.81396484375\n",
            "Epoch: 216  Training Loss: 1880.62512207 Validation Loss: 1731.973876953125 Test Loss: 2056.650390625\n",
            "Epoch: 221  Training Loss: 1879.71093750 Validation Loss: 1730.715087890625 Test Loss: 2055.494140625\n",
            "Epoch: 226  Training Loss: 1878.80310059 Validation Loss: 1729.464111328125 Test Loss: 2054.345458984375\n",
            "Epoch: 231  Training Loss: 1877.90209961 Validation Loss: 1728.2205810546875 Test Loss: 2053.20361328125\n",
            "Epoch: 236  Training Loss: 1877.00720215 Validation Loss: 1726.984619140625 Test Loss: 2052.0693359375\n",
            "Epoch: 241  Training Loss: 1876.11914062 Validation Loss: 1725.755859375 Test Loss: 2050.94189453125\n",
            "Epoch: 246  Training Loss: 1875.23754883 Validation Loss: 1724.5347900390625 Test Loss: 2049.821533203125\n",
            "Epoch: 251  Training Loss: 1874.36206055 Validation Loss: 1723.3209228515625 Test Loss: 2048.708740234375\n",
            "Epoch: 256  Training Loss: 1873.49316406 Validation Loss: 1722.114501953125 Test Loss: 2047.602783203125\n",
            "Epoch: 261  Training Loss: 1872.63037109 Validation Loss: 1720.915283203125 Test Loss: 2046.503662109375\n",
            "Epoch: 266  Training Loss: 1871.77429199 Validation Loss: 1719.7235107421875 Test Loss: 2045.411865234375\n",
            "Epoch: 271  Training Loss: 1870.92407227 Validation Loss: 1718.5389404296875 Test Loss: 2044.326904296875\n",
            "Epoch: 276  Training Loss: 1870.08032227 Validation Loss: 1717.361572265625 Test Loss: 2043.248779296875\n",
            "Epoch: 281  Training Loss: 1869.24279785 Validation Loss: 1716.1915283203125 Test Loss: 2042.177734375\n",
            "Epoch: 286  Training Loss: 1868.41137695 Validation Loss: 1715.0284423828125 Test Loss: 2041.113525390625\n",
            "Epoch: 291  Training Loss: 1867.58593750 Validation Loss: 1713.87255859375 Test Loss: 2040.0562744140625\n",
            "Epoch: 296  Training Loss: 1866.76684570 Validation Loss: 1712.72412109375 Test Loss: 2039.005615234375\n",
            "Epoch: 301  Training Loss: 1865.95373535 Validation Loss: 1711.5823974609375 Test Loss: 2037.961669921875\n",
            "Epoch: 306  Training Loss: 1865.14685059 Validation Loss: 1710.44775390625 Test Loss: 2036.9246826171875\n",
            "Epoch: 311  Training Loss: 1864.34558105 Validation Loss: 1709.3201904296875 Test Loss: 2035.894287109375\n",
            "Epoch: 316  Training Loss: 1863.55053711 Validation Loss: 1708.199462890625 Test Loss: 2034.8707275390625\n",
            "Epoch: 321  Training Loss: 1862.76135254 Validation Loss: 1707.0858154296875 Test Loss: 2033.8536376953125\n",
            "Epoch: 326  Training Loss: 1861.97802734 Validation Loss: 1705.979248046875 Test Loss: 2032.8431396484375\n",
            "Epoch: 331  Training Loss: 1861.20068359 Validation Loss: 1704.87939453125 Test Loss: 2031.83935546875\n",
            "Epoch: 336  Training Loss: 1860.42907715 Validation Loss: 1703.786376953125 Test Loss: 2030.842041015625\n",
            "Epoch: 341  Training Loss: 1859.66333008 Validation Loss: 1702.7003173828125 Test Loss: 2029.85107421875\n",
            "Epoch: 346  Training Loss: 1858.90356445 Validation Loss: 1701.6209716796875 Test Loss: 2028.866943359375\n",
            "Epoch: 351  Training Loss: 1858.14892578 Validation Loss: 1700.54833984375 Test Loss: 2027.888916015625\n",
            "Epoch: 356  Training Loss: 1857.40039062 Validation Loss: 1699.4822998046875 Test Loss: 2026.91748046875\n",
            "Epoch: 361  Training Loss: 1856.65759277 Validation Loss: 1698.42333984375 Test Loss: 2025.9522705078125\n",
            "Epoch: 366  Training Loss: 1855.92028809 Validation Loss: 1697.3709716796875 Test Loss: 2024.9935302734375\n",
            "Epoch: 371  Training Loss: 1855.18884277 Validation Loss: 1696.3251953125 Test Loss: 2024.041015625\n",
            "Epoch: 376  Training Loss: 1854.46264648 Validation Loss: 1695.2862548828125 Test Loss: 2023.0947265625\n",
            "Epoch: 381  Training Loss: 1853.74206543 Validation Loss: 1694.2535400390625 Test Loss: 2022.15478515625\n",
            "Epoch: 386  Training Loss: 1853.02722168 Validation Loss: 1693.2274169921875 Test Loss: 2021.2208251953125\n",
            "Epoch: 391  Training Loss: 1852.31762695 Validation Loss: 1692.2078857421875 Test Loss: 2020.29345703125\n",
            "Epoch: 396  Training Loss: 1851.61352539 Validation Loss: 1691.195068359375 Test Loss: 2019.3720703125\n",
            "Epoch: 401  Training Loss: 1850.91491699 Validation Loss: 1690.1885986328125 Test Loss: 2018.4566650390625\n",
            "Epoch: 406  Training Loss: 1850.22143555 Validation Loss: 1689.1884765625 Test Loss: 2017.547607421875\n",
            "Epoch: 411  Training Loss: 1849.53344727 Validation Loss: 1688.19482421875 Test Loss: 2016.6444091796875\n",
            "Epoch: 416  Training Loss: 1848.85083008 Validation Loss: 1687.20751953125 Test Loss: 2015.747314453125\n",
            "Epoch: 421  Training Loss: 1848.17358398 Validation Loss: 1686.2266845703125 Test Loss: 2014.8560791015625\n",
            "Epoch: 426  Training Loss: 1847.50134277 Validation Loss: 1685.2518310546875 Test Loss: 2013.9708251953125\n",
            "Epoch: 431  Training Loss: 1846.83459473 Validation Loss: 1684.28369140625 Test Loss: 2013.0916748046875\n",
            "Epoch: 436  Training Loss: 1846.17285156 Validation Loss: 1683.321533203125 Test Loss: 2012.2181396484375\n",
            "Epoch: 441  Training Loss: 1845.51623535 Validation Loss: 1682.3658447265625 Test Loss: 2011.350830078125\n",
            "Epoch: 446  Training Loss: 1844.86486816 Validation Loss: 1681.4161376953125 Test Loss: 2010.4891357421875\n",
            "Epoch: 451  Training Loss: 1844.21850586 Validation Loss: 1680.47265625 Test Loss: 2009.633056640625\n",
            "Epoch: 456  Training Loss: 1843.57714844 Validation Loss: 1679.5352783203125 Test Loss: 2008.783203125\n",
            "Epoch: 461  Training Loss: 1842.94091797 Validation Loss: 1678.604248046875 Test Loss: 2007.938720703125\n",
            "Epoch: 466  Training Loss: 1842.30969238 Validation Loss: 1677.67919921875 Test Loss: 2007.0999755859375\n",
            "Epoch: 471  Training Loss: 1841.68334961 Validation Loss: 1676.7601318359375 Test Loss: 2006.26708984375\n",
            "Epoch: 476  Training Loss: 1841.06201172 Validation Loss: 1675.8470458984375 Test Loss: 2005.4395751953125\n",
            "Epoch: 481  Training Loss: 1840.44555664 Validation Loss: 1674.9400634765625 Test Loss: 2004.617919921875\n",
            "Epoch: 486  Training Loss: 1839.83410645 Validation Loss: 1674.0391845703125 Test Loss: 2003.8016357421875\n",
            "Epoch: 491  Training Loss: 1839.22741699 Validation Loss: 1673.1441650390625 Test Loss: 2002.9912109375\n",
            "Epoch: 496  Training Loss: 1838.62561035 Validation Loss: 1672.2550048828125 Test Loss: 2002.18603515625\n",
            "Epoch: 501  Training Loss: 1838.02832031 Validation Loss: 1671.3714599609375 Test Loss: 2001.386474609375\n",
            "Epoch: 506  Training Loss: 1837.43615723 Validation Loss: 1670.494140625 Test Loss: 2000.5921630859375\n",
            "Epoch: 511  Training Loss: 1836.84838867 Validation Loss: 1669.62255859375 Test Loss: 1999.8037109375\n",
            "Epoch: 516  Training Loss: 1836.26538086 Validation Loss: 1668.756591796875 Test Loss: 1999.02001953125\n",
            "Epoch: 521  Training Loss: 1835.68713379 Validation Loss: 1667.896728515625 Test Loss: 1998.2420654296875\n",
            "Epoch: 526  Training Loss: 1835.11352539 Validation Loss: 1667.0426025390625 Test Loss: 1997.469482421875\n",
            "Epoch: 531  Training Loss: 1834.54443359 Validation Loss: 1666.194091796875 Test Loss: 1996.7022705078125\n",
            "Epoch: 536  Training Loss: 1833.97998047 Validation Loss: 1665.351318359375 Test Loss: 1995.940185546875\n",
            "Epoch: 541  Training Loss: 1833.41992188 Validation Loss: 1664.513916015625 Test Loss: 1995.183349609375\n",
            "Epoch: 546  Training Loss: 1832.86450195 Validation Loss: 1663.682373046875 Test Loss: 1994.4317626953125\n",
            "Epoch: 551  Training Loss: 1832.31359863 Validation Loss: 1662.8564453125 Test Loss: 1993.6854248046875\n",
            "Epoch: 556  Training Loss: 1831.76721191 Validation Loss: 1662.0360107421875 Test Loss: 1992.944091796875\n",
            "Epoch: 561  Training Loss: 1831.22485352 Validation Loss: 1661.2213134765625 Test Loss: 1992.2078857421875\n",
            "Epoch: 566  Training Loss: 1830.68737793 Validation Loss: 1660.4119873046875 Test Loss: 1991.476806640625\n",
            "Epoch: 571  Training Loss: 1830.15393066 Validation Loss: 1659.6082763671875 Test Loss: 1990.7508544921875\n",
            "Epoch: 576  Training Loss: 1829.62500000 Validation Loss: 1658.8099365234375 Test Loss: 1990.030029296875\n",
            "Epoch: 581  Training Loss: 1829.10034180 Validation Loss: 1658.0169677734375 Test Loss: 1989.31396484375\n",
            "Epoch: 586  Training Loss: 1828.57983398 Validation Loss: 1657.2293701171875 Test Loss: 1988.603271484375\n",
            "Epoch: 591  Training Loss: 1828.06359863 Validation Loss: 1656.447265625 Test Loss: 1987.8970947265625\n",
            "Epoch: 596  Training Loss: 1827.55151367 Validation Loss: 1655.6705322265625 Test Loss: 1987.196044921875\n",
            "Epoch: 601  Training Loss: 1827.04394531 Validation Loss: 1654.8992919921875 Test Loss: 1986.5\n",
            "Epoch: 606  Training Loss: 1826.54016113 Validation Loss: 1654.1329345703125 Test Loss: 1985.808349609375\n",
            "Epoch: 611  Training Loss: 1826.04089355 Validation Loss: 1653.3721923828125 Test Loss: 1985.1220703125\n",
            "Epoch: 616  Training Loss: 1825.54528809 Validation Loss: 1652.6165771484375 Test Loss: 1984.4403076171875\n",
            "Epoch: 621  Training Loss: 1825.05407715 Validation Loss: 1651.8662109375 Test Loss: 1983.763427734375\n",
            "Epoch: 626  Training Loss: 1824.56677246 Validation Loss: 1651.1209716796875 Test Loss: 1983.0911865234375\n",
            "Epoch: 631  Training Loss: 1824.08337402 Validation Loss: 1650.381103515625 Test Loss: 1982.423583984375\n",
            "Epoch: 636  Training Loss: 1823.60437012 Validation Loss: 1649.6461181640625 Test Loss: 1981.7611083984375\n",
            "Epoch: 641  Training Loss: 1823.12890625 Validation Loss: 1648.91650390625 Test Loss: 1981.102783203125\n",
            "Epoch: 646  Training Loss: 1822.65759277 Validation Loss: 1648.19189453125 Test Loss: 1980.4493408203125\n",
            "Epoch: 651  Training Loss: 1822.19018555 Validation Loss: 1647.4720458984375 Test Loss: 1979.800537109375\n",
            "Epoch: 656  Training Loss: 1821.72631836 Validation Loss: 1646.757568359375 Test Loss: 1979.15625\n",
            "Epoch: 661  Training Loss: 1821.26660156 Validation Loss: 1646.0479736328125 Test Loss: 1978.5162353515625\n",
            "Epoch: 666  Training Loss: 1820.81066895 Validation Loss: 1645.343505859375 Test Loss: 1977.8812255859375\n",
            "Epoch: 671  Training Loss: 1820.35864258 Validation Loss: 1644.643798828125 Test Loss: 1977.250244140625\n",
            "Epoch: 676  Training Loss: 1819.91015625 Validation Loss: 1643.9490966796875 Test Loss: 1976.624267578125\n",
            "Epoch: 681  Training Loss: 1819.46533203 Validation Loss: 1643.2591552734375 Test Loss: 1976.002197265625\n",
            "Epoch: 686  Training Loss: 1819.02441406 Validation Loss: 1642.574462890625 Test Loss: 1975.384765625\n",
            "Epoch: 691  Training Loss: 1818.58715820 Validation Loss: 1641.894287109375 Test Loss: 1974.771728515625\n",
            "Epoch: 696  Training Loss: 1818.15356445 Validation Loss: 1641.218994140625 Test Loss: 1974.1630859375\n",
            "Epoch: 701  Training Loss: 1817.72338867 Validation Loss: 1640.5487060546875 Test Loss: 1973.5587158203125\n",
            "Epoch: 706  Training Loss: 1817.29711914 Validation Loss: 1639.8826904296875 Test Loss: 1972.9583740234375\n",
            "Epoch: 711  Training Loss: 1816.87438965 Validation Loss: 1639.221923828125 Test Loss: 1972.3626708984375\n",
            "Epoch: 716  Training Loss: 1816.45507812 Validation Loss: 1638.565673828125 Test Loss: 1971.771240234375\n",
            "Epoch: 721  Training Loss: 1816.03918457 Validation Loss: 1637.914306640625 Test Loss: 1971.1837158203125\n",
            "Epoch: 726  Training Loss: 1815.62695312 Validation Loss: 1637.2674560546875 Test Loss: 1970.6005859375\n",
            "Epoch: 731  Training Loss: 1815.21813965 Validation Loss: 1636.625244140625 Test Loss: 1970.021484375\n",
            "Epoch: 736  Training Loss: 1814.81262207 Validation Loss: 1635.9876708984375 Test Loss: 1969.4466552734375\n",
            "Epoch: 741  Training Loss: 1814.41076660 Validation Loss: 1635.3546142578125 Test Loss: 1968.8758544921875\n",
            "Epoch: 746  Training Loss: 1814.01208496 Validation Loss: 1634.726318359375 Test Loss: 1968.3092041015625\n",
            "Epoch: 751  Training Loss: 1813.61694336 Validation Loss: 1634.1024169921875 Test Loss: 1967.7464599609375\n",
            "Epoch: 756  Training Loss: 1813.22509766 Validation Loss: 1633.4830322265625 Test Loss: 1967.187744140625\n",
            "Epoch: 761  Training Loss: 1812.83642578 Validation Loss: 1632.868408203125 Test Loss: 1966.63330078125\n",
            "Epoch: 766  Training Loss: 1812.45117188 Validation Loss: 1632.2579345703125 Test Loss: 1966.0826416015625\n",
            "Epoch: 771  Training Loss: 1812.06921387 Validation Loss: 1631.65185546875 Test Loss: 1965.5360107421875\n",
            "Epoch: 776  Training Loss: 1811.69030762 Validation Loss: 1631.0504150390625 Test Loss: 1964.9931640625\n",
            "Epoch: 781  Training Loss: 1811.31481934 Validation Loss: 1630.453125 Test Loss: 1964.454345703125\n",
            "Epoch: 786  Training Loss: 1810.94226074 Validation Loss: 1629.8604736328125 Test Loss: 1963.9195556640625\n",
            "Epoch: 791  Training Loss: 1810.57299805 Validation Loss: 1629.2718505859375 Test Loss: 1963.3883056640625\n",
            "Epoch: 796  Training Loss: 1810.20703125 Validation Loss: 1628.6878662109375 Test Loss: 1962.86083984375\n",
            "Epoch: 801  Training Loss: 1809.84387207 Validation Loss: 1628.1080322265625 Test Loss: 1962.3372802734375\n",
            "Epoch: 806  Training Loss: 1809.48388672 Validation Loss: 1627.53271484375 Test Loss: 1961.817626953125\n",
            "Epoch: 811  Training Loss: 1809.12695312 Validation Loss: 1626.96142578125 Test Loss: 1961.3013916015625\n",
            "Epoch: 816  Training Loss: 1808.77307129 Validation Loss: 1626.3941650390625 Test Loss: 1960.789306640625\n",
            "Epoch: 821  Training Loss: 1808.42224121 Validation Loss: 1625.831298828125 Test Loss: 1960.2806396484375\n",
            "Epoch: 826  Training Loss: 1808.07434082 Validation Loss: 1625.2725830078125 Test Loss: 1959.775634765625\n",
            "Epoch: 831  Training Loss: 1807.72949219 Validation Loss: 1624.7181396484375 Test Loss: 1959.2744140625\n",
            "Epoch: 836  Training Loss: 1807.38745117 Validation Loss: 1624.1676025390625 Test Loss: 1958.77685546875\n",
            "Epoch: 841  Training Loss: 1807.04833984 Validation Loss: 1623.621337890625 Test Loss: 1958.28271484375\n",
            "Epoch: 846  Training Loss: 1806.71215820 Validation Loss: 1623.0792236328125 Test Loss: 1957.7921142578125\n",
            "Epoch: 851  Training Loss: 1806.37890625 Validation Loss: 1622.541015625 Test Loss: 1957.30517578125\n",
            "Epoch: 856  Training Loss: 1806.04833984 Validation Loss: 1622.0068359375 Test Loss: 1956.82177734375\n",
            "Epoch: 861  Training Loss: 1805.72045898 Validation Loss: 1621.4766845703125 Test Loss: 1956.3416748046875\n",
            "Epoch: 866  Training Loss: 1805.39562988 Validation Loss: 1620.9503173828125 Test Loss: 1955.865478515625\n",
            "Epoch: 871  Training Loss: 1805.07360840 Validation Loss: 1620.4283447265625 Test Loss: 1955.392333984375\n",
            "Epoch: 876  Training Loss: 1804.75427246 Validation Loss: 1619.909912109375 Test Loss: 1954.9227294921875\n",
            "Epoch: 881  Training Loss: 1804.43750000 Validation Loss: 1619.3956298828125 Test Loss: 1954.4566650390625\n",
            "Epoch: 886  Training Loss: 1804.12341309 Validation Loss: 1618.8851318359375 Test Loss: 1953.99365234375\n",
            "Epoch: 891  Training Loss: 1803.81201172 Validation Loss: 1618.3785400390625 Test Loss: 1953.5341796875\n",
            "Epoch: 896  Training Loss: 1803.50366211 Validation Loss: 1617.875732421875 Test Loss: 1953.078125\n",
            "Epoch: 901  Training Loss: 1803.19738770 Validation Loss: 1617.3768310546875 Test Loss: 1952.625244140625\n",
            "Epoch: 906  Training Loss: 1802.89404297 Validation Loss: 1616.8818359375 Test Loss: 1952.1756591796875\n",
            "Epoch: 911  Training Loss: 1802.59301758 Validation Loss: 1616.390380859375 Test Loss: 1951.7293701171875\n",
            "Epoch: 916  Training Loss: 1802.29492188 Validation Loss: 1615.9027099609375 Test Loss: 1951.2861328125\n",
            "Epoch: 921  Training Loss: 1801.99914551 Validation Loss: 1615.4189453125 Test Loss: 1950.846435546875\n",
            "Epoch: 926  Training Loss: 1801.70593262 Validation Loss: 1614.9388427734375 Test Loss: 1950.40966796875\n",
            "Epoch: 931  Training Loss: 1801.41503906 Validation Loss: 1614.462158203125 Test Loss: 1949.97607421875\n",
            "Epoch: 936  Training Loss: 1801.12658691 Validation Loss: 1613.989501953125 Test Loss: 1949.5458984375\n",
            "Epoch: 941  Training Loss: 1800.84082031 Validation Loss: 1613.520263671875 Test Loss: 1949.1185302734375\n",
            "Epoch: 946  Training Loss: 1800.55737305 Validation Loss: 1613.0548095703125 Test Loss: 1948.6943359375\n",
            "Epoch: 951  Training Loss: 1800.27661133 Validation Loss: 1612.5926513671875 Test Loss: 1948.2733154296875\n",
            "Epoch: 956  Training Loss: 1799.99768066 Validation Loss: 1612.1343994140625 Test Loss: 1947.855224609375\n",
            "Epoch: 961  Training Loss: 1799.72143555 Validation Loss: 1611.6795654296875 Test Loss: 1947.4403076171875\n",
            "Epoch: 966  Training Loss: 1799.44750977 Validation Loss: 1611.228271484375 Test Loss: 1947.0281982421875\n",
            "Epoch: 971  Training Loss: 1799.17602539 Validation Loss: 1610.780517578125 Test Loss: 1946.6192626953125\n",
            "Epoch: 976  Training Loss: 1798.90661621 Validation Loss: 1610.3360595703125 Test Loss: 1946.213134765625\n",
            "Epoch: 981  Training Loss: 1798.63964844 Validation Loss: 1609.895263671875 Test Loss: 1945.8101806640625\n",
            "Epoch: 986  Training Loss: 1798.37463379 Validation Loss: 1609.457763671875 Test Loss: 1945.4097900390625\n",
            "Epoch: 991  Training Loss: 1798.11206055 Validation Loss: 1609.02392578125 Test Loss: 1945.0126953125\n",
            "Epoch: 996  Training Loss: 1797.85168457 Validation Loss: 1608.5931396484375 Test Loss: 1944.6181640625\n",
            "Epoch: 1001  Training Loss: 1797.59350586 Validation Loss: 1608.166015625 Test Loss: 1944.2266845703125\n",
            "Epoch: 1006  Training Loss: 1797.33764648 Validation Loss: 1607.741943359375 Test Loss: 1943.837890625\n",
            "Epoch: 1011  Training Loss: 1797.08386230 Validation Loss: 1607.3216552734375 Test Loss: 1943.451904296875\n",
            "Epoch: 1016  Training Loss: 1796.83203125 Validation Loss: 1606.9044189453125 Test Loss: 1943.0687255859375\n",
            "Epoch: 1021  Training Loss: 1796.58239746 Validation Loss: 1606.490478515625 Test Loss: 1942.688720703125\n",
            "Epoch: 1026  Training Loss: 1796.33496094 Validation Loss: 1606.079833984375 Test Loss: 1942.3109130859375\n",
            "Epoch: 1031  Training Loss: 1796.08959961 Validation Loss: 1605.6724853515625 Test Loss: 1941.9359130859375\n",
            "Epoch: 1036  Training Loss: 1795.84606934 Validation Loss: 1605.2681884765625 Test Loss: 1941.5638427734375\n",
            "Epoch: 1041  Training Loss: 1795.60461426 Validation Loss: 1604.8673095703125 Test Loss: 1941.1944580078125\n",
            "Epoch: 1046  Training Loss: 1795.36535645 Validation Loss: 1604.469482421875 Test Loss: 1940.827392578125\n",
            "Epoch: 1051  Training Loss: 1795.12792969 Validation Loss: 1604.074951171875 Test Loss: 1940.4635009765625\n",
            "Epoch: 1056  Training Loss: 1794.89257812 Validation Loss: 1603.68359375 Test Loss: 1940.101806640625\n",
            "Epoch: 1061  Training Loss: 1794.65917969 Validation Loss: 1603.295166015625 Test Loss: 1939.742919921875\n",
            "Epoch: 1066  Training Loss: 1794.42761230 Validation Loss: 1602.909912109375 Test Loss: 1939.386474609375\n",
            "Epoch: 1071  Training Loss: 1794.19799805 Validation Loss: 1602.5279541015625 Test Loss: 1939.03271484375\n",
            "Epoch: 1076  Training Loss: 1793.97009277 Validation Loss: 1602.14892578125 Test Loss: 1938.681396484375\n",
            "Epoch: 1081  Training Loss: 1793.74462891 Validation Loss: 1601.77294921875 Test Loss: 1938.3326416015625\n",
            "Epoch: 1086  Training Loss: 1793.52050781 Validation Loss: 1601.4000244140625 Test Loss: 1937.986572265625\n",
            "Epoch: 1091  Training Loss: 1793.29833984 Validation Loss: 1601.0299072265625 Test Loss: 1937.6427001953125\n",
            "Epoch: 1096  Training Loss: 1793.07812500 Validation Loss: 1600.6630859375 Test Loss: 1937.3016357421875\n",
            "Epoch: 1101  Training Loss: 1792.85961914 Validation Loss: 1600.2989501953125 Test Loss: 1936.962646484375\n",
            "Epoch: 1106  Training Loss: 1792.64270020 Validation Loss: 1599.9381103515625 Test Loss: 1936.626220703125\n",
            "Epoch: 1111  Training Loss: 1792.42785645 Validation Loss: 1599.579833984375 Test Loss: 1936.29248046875\n",
            "Epoch: 1116  Training Loss: 1792.21472168 Validation Loss: 1599.2244873046875 Test Loss: 1935.9608154296875\n",
            "Epoch: 1121  Training Loss: 1792.00329590 Validation Loss: 1598.8724365234375 Test Loss: 1935.6314697265625\n",
            "Epoch: 1126  Training Loss: 1791.79333496 Validation Loss: 1598.5228271484375 Test Loss: 1935.3045654296875\n",
            "Epoch: 1131  Training Loss: 1791.58532715 Validation Loss: 1598.17626953125 Test Loss: 1934.97998046875\n",
            "Epoch: 1136  Training Loss: 1791.37915039 Validation Loss: 1597.83251953125 Test Loss: 1934.65771484375\n",
            "Epoch: 1141  Training Loss: 1791.17431641 Validation Loss: 1597.4915771484375 Test Loss: 1934.337890625\n",
            "Epoch: 1146  Training Loss: 1790.97106934 Validation Loss: 1597.1534423828125 Test Loss: 1934.02001953125\n",
            "Epoch: 1151  Training Loss: 1790.76989746 Validation Loss: 1596.8179931640625 Test Loss: 1933.70458984375\n",
            "Epoch: 1156  Training Loss: 1790.56982422 Validation Loss: 1596.4854736328125 Test Loss: 1933.3914794921875\n",
            "Epoch: 1161  Training Loss: 1790.37170410 Validation Loss: 1596.155517578125 Test Loss: 1933.0806884765625\n",
            "Epoch: 1166  Training Loss: 1790.17517090 Validation Loss: 1595.828369140625 Test Loss: 1932.7718505859375\n",
            "Epoch: 1171  Training Loss: 1789.97985840 Validation Loss: 1595.5040283203125 Test Loss: 1932.4652099609375\n",
            "Epoch: 1176  Training Loss: 1789.78625488 Validation Loss: 1595.182373046875 Test Loss: 1932.1610107421875\n",
            "Epoch: 1181  Training Loss: 1789.59436035 Validation Loss: 1594.86328125 Test Loss: 1931.858642578125\n",
            "Epoch: 1186  Training Loss: 1789.40356445 Validation Loss: 1594.546630859375 Test Loss: 1931.5587158203125\n",
            "Epoch: 1191  Training Loss: 1789.21472168 Validation Loss: 1594.23291015625 Test Loss: 1931.2606201171875\n",
            "Epoch: 1196  Training Loss: 1789.02709961 Validation Loss: 1593.921630859375 Test Loss: 1930.964599609375\n",
            "Epoch: 1201  Training Loss: 1788.84106445 Validation Loss: 1593.613037109375 Test Loss: 1930.6710205078125\n",
            "Epoch: 1206  Training Loss: 1788.65637207 Validation Loss: 1593.30712890625 Test Loss: 1930.3792724609375\n",
            "Epoch: 1211  Training Loss: 1788.47314453 Validation Loss: 1593.0035400390625 Test Loss: 1930.08935546875\n",
            "Epoch: 1216  Training Loss: 1788.29138184 Validation Loss: 1592.7027587890625 Test Loss: 1929.8018798828125\n",
            "Epoch: 1221  Training Loss: 1788.11083984 Validation Loss: 1592.4041748046875 Test Loss: 1929.5162353515625\n",
            "Epoch: 1226  Training Loss: 1787.93176270 Validation Loss: 1592.1083984375 Test Loss: 1929.232421875\n",
            "Epoch: 1231  Training Loss: 1787.75415039 Validation Loss: 1591.8150634765625 Test Loss: 1928.950927734375\n",
            "Epoch: 1236  Training Loss: 1787.57775879 Validation Loss: 1591.5240478515625 Test Loss: 1928.671142578125\n",
            "Epoch: 1241  Training Loss: 1787.40270996 Validation Loss: 1591.2357177734375 Test Loss: 1928.3934326171875\n",
            "Epoch: 1246  Training Loss: 1787.22912598 Validation Loss: 1590.9495849609375 Test Loss: 1928.1173095703125\n",
            "Epoch: 1251  Training Loss: 1787.05676270 Validation Loss: 1590.666015625 Test Loss: 1927.8436279296875\n",
            "Epoch: 1256  Training Loss: 1786.88562012 Validation Loss: 1590.384521484375 Test Loss: 1927.571533203125\n",
            "Epoch: 1261  Training Loss: 1786.71582031 Validation Loss: 1590.1058349609375 Test Loss: 1927.3013916015625\n",
            "Epoch: 1266  Training Loss: 1786.54724121 Validation Loss: 1589.8294677734375 Test Loss: 1927.033203125\n",
            "Epoch: 1271  Training Loss: 1786.37988281 Validation Loss: 1589.5552978515625 Test Loss: 1926.7664794921875\n",
            "Epoch: 1276  Training Loss: 1786.21386719 Validation Loss: 1589.283447265625 Test Loss: 1926.5020751953125\n",
            "Epoch: 1281  Training Loss: 1786.04907227 Validation Loss: 1589.01416015625 Test Loss: 1926.2393798828125\n",
            "Epoch: 1286  Training Loss: 1785.88549805 Validation Loss: 1588.7470703125 Test Loss: 1925.9781494140625\n",
            "Epoch: 1291  Training Loss: 1785.72314453 Validation Loss: 1588.48193359375 Test Loss: 1925.71923828125\n",
            "Epoch: 1296  Training Loss: 1785.56164551 Validation Loss: 1588.2197265625 Test Loss: 1925.461669921875\n",
            "Epoch: 1301  Training Loss: 1785.40161133 Validation Loss: 1587.9593505859375 Test Loss: 1925.2059326171875\n",
            "Epoch: 1306  Training Loss: 1785.24279785 Validation Loss: 1587.7010498046875 Test Loss: 1924.9521484375\n",
            "Epoch: 1311  Training Loss: 1785.08496094 Validation Loss: 1587.4454345703125 Test Loss: 1924.6998291015625\n",
            "Epoch: 1316  Training Loss: 1784.92810059 Validation Loss: 1587.1917724609375 Test Loss: 1924.449462890625\n",
            "Epoch: 1321  Training Loss: 1784.77258301 Validation Loss: 1586.9403076171875 Test Loss: 1924.2005615234375\n",
            "Epoch: 1326  Training Loss: 1784.61816406 Validation Loss: 1586.6910400390625 Test Loss: 1923.953369140625\n",
            "Epoch: 1331  Training Loss: 1784.46472168 Validation Loss: 1586.4439697265625 Test Loss: 1923.7081298828125\n",
            "Epoch: 1336  Training Loss: 1784.31250000 Validation Loss: 1586.1988525390625 Test Loss: 1923.46435546875\n",
            "Epoch: 1341  Training Loss: 1784.16113281 Validation Loss: 1585.9560546875 Test Loss: 1923.2220458984375\n",
            "Epoch: 1346  Training Loss: 1784.01086426 Validation Loss: 1585.7152099609375 Test Loss: 1922.981689453125\n",
            "Epoch: 1351  Training Loss: 1783.86181641 Validation Loss: 1585.476806640625 Test Loss: 1922.742431640625\n",
            "Epoch: 1356  Training Loss: 1783.71362305 Validation Loss: 1585.2401123046875 Test Loss: 1922.5052490234375\n",
            "Epoch: 1361  Training Loss: 1783.56628418 Validation Loss: 1585.0054931640625 Test Loss: 1922.2696533203125\n",
            "Epoch: 1366  Training Loss: 1783.42016602 Validation Loss: 1584.7733154296875 Test Loss: 1922.035400390625\n",
            "Epoch: 1371  Training Loss: 1783.27490234 Validation Loss: 1584.5428466796875 Test Loss: 1921.802734375\n",
            "Epoch: 1376  Training Loss: 1783.13061523 Validation Loss: 1584.3145751953125 Test Loss: 1921.5716552734375\n",
            "Epoch: 1381  Training Loss: 1782.98754883 Validation Loss: 1584.0882568359375 Test Loss: 1921.34228515625\n",
            "Epoch: 1386  Training Loss: 1782.84497070 Validation Loss: 1583.86376953125 Test Loss: 1921.1141357421875\n",
            "Epoch: 1391  Training Loss: 1782.70373535 Validation Loss: 1583.6416015625 Test Loss: 1920.8876953125\n",
            "Epoch: 1396  Training Loss: 1782.56311035 Validation Loss: 1583.421142578125 Test Loss: 1920.6624755859375\n",
            "Epoch: 1401  Training Loss: 1782.42333984 Validation Loss: 1583.2025146484375 Test Loss: 1920.43896484375\n",
            "Epoch: 1406  Training Loss: 1782.28479004 Validation Loss: 1582.9862060546875 Test Loss: 1920.216796875\n",
            "Epoch: 1411  Training Loss: 1782.14697266 Validation Loss: 1582.771484375 Test Loss: 1919.9962158203125\n",
            "Epoch: 1416  Training Loss: 1782.01013184 Validation Loss: 1582.558837890625 Test Loss: 1919.77685546875\n",
            "Epoch: 1421  Training Loss: 1781.87402344 Validation Loss: 1582.34814453125 Test Loss: 1919.5589599609375\n",
            "Epoch: 1426  Training Loss: 1781.73864746 Validation Loss: 1582.13916015625 Test Loss: 1919.342529296875\n",
            "Epoch: 1431  Training Loss: 1781.60449219 Validation Loss: 1581.9320068359375 Test Loss: 1919.1273193359375\n",
            "Epoch: 1436  Training Loss: 1781.47082520 Validation Loss: 1581.726806640625 Test Loss: 1918.9139404296875\n",
            "Epoch: 1441  Training Loss: 1781.33825684 Validation Loss: 1581.523681640625 Test Loss: 1918.70166015625\n",
            "Epoch: 1446  Training Loss: 1781.20605469 Validation Loss: 1581.322021484375 Test Loss: 1918.4906005859375\n",
            "Epoch: 1451  Training Loss: 1781.07495117 Validation Loss: 1581.1224365234375 Test Loss: 1918.2811279296875\n",
            "Epoch: 1456  Training Loss: 1780.94458008 Validation Loss: 1580.9244384765625 Test Loss: 1918.0728759765625\n",
            "Epoch: 1461  Training Loss: 1780.81494141 Validation Loss: 1580.728271484375 Test Loss: 1917.8658447265625\n",
            "Epoch: 1466  Training Loss: 1780.68640137 Validation Loss: 1580.5340576171875 Test Loss: 1917.6600341796875\n",
            "Epoch: 1471  Training Loss: 1780.55822754 Validation Loss: 1580.341552734375 Test Loss: 1917.455810546875\n",
            "Epoch: 1476  Training Loss: 1780.43090820 Validation Loss: 1580.1505126953125 Test Loss: 1917.2529296875\n",
            "Epoch: 1481  Training Loss: 1780.30444336 Validation Loss: 1579.9615478515625 Test Loss: 1917.0511474609375\n",
            "Epoch: 1486  Training Loss: 1780.17858887 Validation Loss: 1579.7740478515625 Test Loss: 1916.8505859375\n",
            "Epoch: 1491  Training Loss: 1780.05334473 Validation Loss: 1579.58837890625 Test Loss: 1916.6512451171875\n",
            "Epoch: 1496  Training Loss: 1779.92883301 Validation Loss: 1579.404541015625 Test Loss: 1916.4532470703125\n",
            "Epoch: 1501  Training Loss: 1779.80493164 Validation Loss: 1579.2222900390625 Test Loss: 1916.25634765625\n",
            "Epoch: 1506  Training Loss: 1779.68188477 Validation Loss: 1579.041748046875 Test Loss: 1916.060791015625\n",
            "Epoch: 1511  Training Loss: 1779.55969238 Validation Loss: 1578.862548828125 Test Loss: 1915.866455078125\n",
            "Epoch: 1516  Training Loss: 1779.43798828 Validation Loss: 1578.685302734375 Test Loss: 1915.673095703125\n",
            "Epoch: 1521  Training Loss: 1779.31689453 Validation Loss: 1578.509521484375 Test Loss: 1915.4813232421875\n",
            "Epoch: 1526  Training Loss: 1779.19641113 Validation Loss: 1578.33544921875 Test Loss: 1915.2900390625\n",
            "Epoch: 1531  Training Loss: 1779.07678223 Validation Loss: 1578.1630859375 Test Loss: 1915.1005859375\n",
            "Epoch: 1536  Training Loss: 1778.95715332 Validation Loss: 1577.9920654296875 Test Loss: 1914.9119873046875\n",
            "Epoch: 1541  Training Loss: 1778.83874512 Validation Loss: 1577.82275390625 Test Loss: 1914.7244873046875\n",
            "Epoch: 1546  Training Loss: 1778.72082520 Validation Loss: 1577.6551513671875 Test Loss: 1914.5382080078125\n",
            "Epoch: 1551  Training Loss: 1778.60351562 Validation Loss: 1577.48876953125 Test Loss: 1914.3531494140625\n",
            "Epoch: 1556  Training Loss: 1778.48669434 Validation Loss: 1577.32421875 Test Loss: 1914.1688232421875\n",
            "Epoch: 1561  Training Loss: 1778.37060547 Validation Loss: 1577.1610107421875 Test Loss: 1913.9859619140625\n",
            "Epoch: 1566  Training Loss: 1778.25488281 Validation Loss: 1576.9993896484375 Test Loss: 1913.803955078125\n",
            "Epoch: 1571  Training Loss: 1778.13989258 Validation Loss: 1576.8394775390625 Test Loss: 1913.623291015625\n",
            "Epoch: 1576  Training Loss: 1778.02526855 Validation Loss: 1576.6810302734375 Test Loss: 1913.4432373046875\n",
            "Epoch: 1581  Training Loss: 1777.91137695 Validation Loss: 1576.52392578125 Test Loss: 1913.264404296875\n",
            "Epoch: 1586  Training Loss: 1777.79809570 Validation Loss: 1576.368408203125 Test Loss: 1913.0867919921875\n",
            "Epoch: 1591  Training Loss: 1777.68542480 Validation Loss: 1576.214111328125 Test Loss: 1912.9100341796875\n",
            "Epoch: 1596  Training Loss: 1777.57287598 Validation Loss: 1576.0615234375 Test Loss: 1912.7344970703125\n",
            "Epoch: 1601  Training Loss: 1777.46118164 Validation Loss: 1575.91015625 Test Loss: 1912.559814453125\n",
            "Epoch: 1606  Training Loss: 1777.34960938 Validation Loss: 1575.76025390625 Test Loss: 1912.3861083984375\n",
            "Epoch: 1611  Training Loss: 1777.23889160 Validation Loss: 1575.6119384765625 Test Loss: 1912.2135009765625\n",
            "Epoch: 1616  Training Loss: 1777.12841797 Validation Loss: 1575.465087890625 Test Loss: 1912.0418701171875\n",
            "Epoch: 1621  Training Loss: 1777.01867676 Validation Loss: 1575.3193359375 Test Loss: 1911.8712158203125\n",
            "Epoch: 1626  Training Loss: 1776.90930176 Validation Loss: 1575.1751708984375 Test Loss: 1911.701416015625\n",
            "Epoch: 1631  Training Loss: 1776.80041504 Validation Loss: 1575.0323486328125 Test Loss: 1911.5325927734375\n",
            "Epoch: 1636  Training Loss: 1776.69213867 Validation Loss: 1574.8909912109375 Test Loss: 1911.3648681640625\n",
            "Epoch: 1641  Training Loss: 1776.58398438 Validation Loss: 1574.7509765625 Test Loss: 1911.1978759765625\n",
            "Epoch: 1646  Training Loss: 1776.47631836 Validation Loss: 1574.6121826171875 Test Loss: 1911.0318603515625\n",
            "Epoch: 1651  Training Loss: 1776.36914062 Validation Loss: 1574.474609375 Test Loss: 1910.866943359375\n",
            "Epoch: 1656  Training Loss: 1776.26257324 Validation Loss: 1574.33837890625 Test Loss: 1910.7025146484375\n",
            "Epoch: 1661  Training Loss: 1776.15637207 Validation Loss: 1574.203857421875 Test Loss: 1910.539306640625\n",
            "Epoch: 1666  Training Loss: 1776.05053711 Validation Loss: 1574.0704345703125 Test Loss: 1910.3770751953125\n",
            "Epoch: 1671  Training Loss: 1775.94506836 Validation Loss: 1573.938232421875 Test Loss: 1910.2154541015625\n",
            "Epoch: 1676  Training Loss: 1775.84008789 Validation Loss: 1573.80712890625 Test Loss: 1910.0548095703125\n",
            "Epoch: 1681  Training Loss: 1775.73522949 Validation Loss: 1573.6776123046875 Test Loss: 1909.89501953125\n",
            "Epoch: 1686  Training Loss: 1775.63122559 Validation Loss: 1573.54931640625 Test Loss: 1909.736083984375\n",
            "Epoch: 1691  Training Loss: 1775.52722168 Validation Loss: 1573.421875 Test Loss: 1909.578125\n",
            "Epoch: 1696  Training Loss: 1775.42382812 Validation Loss: 1573.296142578125 Test Loss: 1909.4208984375\n",
            "Epoch: 1701  Training Loss: 1775.32067871 Validation Loss: 1573.1715087890625 Test Loss: 1909.264404296875\n",
            "Epoch: 1706  Training Loss: 1775.21789551 Validation Loss: 1573.0479736328125 Test Loss: 1909.1087646484375\n",
            "Epoch: 1711  Training Loss: 1775.11547852 Validation Loss: 1572.92578125 Test Loss: 1908.9541015625\n",
            "Epoch: 1716  Training Loss: 1775.01342773 Validation Loss: 1572.8048095703125 Test Loss: 1908.8001708984375\n",
            "Epoch: 1721  Training Loss: 1774.91174316 Validation Loss: 1572.6849365234375 Test Loss: 1908.6468505859375\n",
            "Epoch: 1726  Training Loss: 1774.81042480 Validation Loss: 1572.566162109375 Test Loss: 1908.494384765625\n",
            "Epoch: 1731  Training Loss: 1774.70947266 Validation Loss: 1572.4488525390625 Test Loss: 1908.3428955078125\n",
            "Epoch: 1736  Training Loss: 1774.60864258 Validation Loss: 1572.33251953125 Test Loss: 1908.19189453125\n",
            "Epoch: 1741  Training Loss: 1774.50842285 Validation Loss: 1572.21728515625 Test Loss: 1908.0418701171875\n",
            "Epoch: 1746  Training Loss: 1774.40808105 Validation Loss: 1572.103271484375 Test Loss: 1907.8927001953125\n",
            "Epoch: 1751  Training Loss: 1774.30834961 Validation Loss: 1571.990478515625 Test Loss: 1907.743896484375\n",
            "Epoch: 1756  Training Loss: 1774.20898438 Validation Loss: 1571.878662109375 Test Loss: 1907.5960693359375\n",
            "Epoch: 1761  Training Loss: 1774.10974121 Validation Loss: 1571.76806640625 Test Loss: 1907.448974609375\n",
            "Epoch: 1766  Training Loss: 1774.01086426 Validation Loss: 1571.65869140625 Test Loss: 1907.302490234375\n",
            "Epoch: 1771  Training Loss: 1773.91223145 Validation Loss: 1571.5501708984375 Test Loss: 1907.1571044921875\n",
            "Epoch: 1776  Training Loss: 1773.81396484 Validation Loss: 1571.44287109375 Test Loss: 1907.0120849609375\n",
            "Epoch: 1781  Training Loss: 1773.71582031 Validation Loss: 1571.3365478515625 Test Loss: 1906.867919921875\n",
            "Epoch: 1786  Training Loss: 1773.61816406 Validation Loss: 1571.2314453125 Test Loss: 1906.724365234375\n",
            "Epoch: 1791  Training Loss: 1773.52062988 Validation Loss: 1571.12744140625 Test Loss: 1906.5811767578125\n",
            "Epoch: 1796  Training Loss: 1773.42333984 Validation Loss: 1571.0242919921875 Test Loss: 1906.4390869140625\n",
            "Epoch: 1801  Training Loss: 1773.32629395 Validation Loss: 1570.9222412109375 Test Loss: 1906.2977294921875\n",
            "Epoch: 1806  Training Loss: 1773.22961426 Validation Loss: 1570.8212890625 Test Loss: 1906.15673828125\n",
            "Epoch: 1811  Training Loss: 1773.13317871 Validation Loss: 1570.7213134765625 Test Loss: 1906.0164794921875\n",
            "Epoch: 1816  Training Loss: 1773.03686523 Validation Loss: 1570.6224365234375 Test Loss: 1905.876953125\n",
            "Epoch: 1821  Training Loss: 1772.94091797 Validation Loss: 1570.524658203125 Test Loss: 1905.7381591796875\n",
            "Epoch: 1826  Training Loss: 1772.84533691 Validation Loss: 1570.427734375 Test Loss: 1905.5999755859375\n",
            "Epoch: 1831  Training Loss: 1772.74951172 Validation Loss: 1570.331787109375 Test Loss: 1905.4622802734375\n",
            "Epoch: 1836  Training Loss: 1772.65417480 Validation Loss: 1570.23681640625 Test Loss: 1905.3251953125\n",
            "Epoch: 1841  Training Loss: 1772.55908203 Validation Loss: 1570.14306640625 Test Loss: 1905.1888427734375\n",
            "Epoch: 1846  Training Loss: 1772.46423340 Validation Loss: 1570.050048828125 Test Loss: 1905.0531005859375\n",
            "Epoch: 1851  Training Loss: 1772.36938477 Validation Loss: 1569.9581298828125 Test Loss: 1904.9178466796875\n",
            "Epoch: 1856  Training Loss: 1772.27490234 Validation Loss: 1569.866943359375 Test Loss: 1904.7833251953125\n",
            "Epoch: 1861  Training Loss: 1772.18066406 Validation Loss: 1569.77685546875 Test Loss: 1904.6494140625\n",
            "Epoch: 1866  Training Loss: 1772.08642578 Validation Loss: 1569.687744140625 Test Loss: 1904.515869140625\n",
            "Epoch: 1871  Training Loss: 1771.99243164 Validation Loss: 1569.5994873046875 Test Loss: 1904.38330078125\n",
            "Epoch: 1876  Training Loss: 1771.89868164 Validation Loss: 1569.5123291015625 Test Loss: 1904.2508544921875\n",
            "Epoch: 1881  Training Loss: 1771.80517578 Validation Loss: 1569.4259033203125 Test Loss: 1904.1192626953125\n",
            "Epoch: 1886  Training Loss: 1771.71179199 Validation Loss: 1569.3402099609375 Test Loss: 1903.9881591796875\n",
            "Epoch: 1891  Training Loss: 1771.61865234 Validation Loss: 1569.255859375 Test Loss: 1903.857666015625\n",
            "Epoch: 1896  Training Loss: 1771.52539062 Validation Loss: 1569.1722412109375 Test Loss: 1903.7275390625\n",
            "Epoch: 1901  Training Loss: 1771.43261719 Validation Loss: 1569.0894775390625 Test Loss: 1903.59814453125\n",
            "Epoch: 1906  Training Loss: 1771.33984375 Validation Loss: 1569.007568359375 Test Loss: 1903.46923828125\n",
            "Epoch: 1911  Training Loss: 1771.24731445 Validation Loss: 1568.926513671875 Test Loss: 1903.3408203125\n",
            "Epoch: 1916  Training Loss: 1771.15490723 Validation Loss: 1568.8463134765625 Test Loss: 1903.2130126953125\n",
            "Epoch: 1921  Training Loss: 1771.06250000 Validation Loss: 1568.7669677734375 Test Loss: 1903.08544921875\n",
            "Epoch: 1926  Training Loss: 1770.97045898 Validation Loss: 1568.6885986328125 Test Loss: 1902.9588623046875\n",
            "Epoch: 1931  Training Loss: 1770.87866211 Validation Loss: 1568.61083984375 Test Loss: 1902.83251953125\n",
            "Epoch: 1936  Training Loss: 1770.78674316 Validation Loss: 1568.5340576171875 Test Loss: 1902.70654296875\n",
            "Epoch: 1941  Training Loss: 1770.69482422 Validation Loss: 1568.4581298828125 Test Loss: 1902.5811767578125\n",
            "Epoch: 1946  Training Loss: 1770.60339355 Validation Loss: 1568.383056640625 Test Loss: 1902.45654296875\n",
            "Epoch: 1951  Training Loss: 1770.51196289 Validation Loss: 1568.30859375 Test Loss: 1902.3321533203125\n",
            "Epoch: 1956  Training Loss: 1770.42065430 Validation Loss: 1568.235107421875 Test Loss: 1902.208251953125\n",
            "Epoch: 1961  Training Loss: 1770.32922363 Validation Loss: 1568.1624755859375 Test Loss: 1902.084716796875\n",
            "Epoch: 1966  Training Loss: 1770.23840332 Validation Loss: 1568.0904541015625 Test Loss: 1901.9619140625\n",
            "Epoch: 1971  Training Loss: 1770.14709473 Validation Loss: 1568.01953125 Test Loss: 1901.839599609375\n",
            "Epoch: 1976  Training Loss: 1770.05639648 Validation Loss: 1567.94921875 Test Loss: 1901.717529296875\n",
            "Epoch: 1981  Training Loss: 1769.96557617 Validation Loss: 1567.879638671875 Test Loss: 1901.595947265625\n",
            "Epoch: 1986  Training Loss: 1769.87500000 Validation Loss: 1567.810791015625 Test Loss: 1901.474853515625\n",
            "Epoch: 1991  Training Loss: 1769.78442383 Validation Loss: 1567.74267578125 Test Loss: 1901.3543701171875\n",
            "Epoch: 1996  Training Loss: 1769.69409180 Validation Loss: 1567.675537109375 Test Loss: 1901.2342529296875\n",
            "Epoch: 2001  Training Loss: 1769.60363770 Validation Loss: 1567.6087646484375 Test Loss: 1901.1143798828125\n",
            "Epoch: 2006  Training Loss: 1769.51342773 Validation Loss: 1567.54296875 Test Loss: 1900.9949951171875\n",
            "Epoch: 2011  Training Loss: 1769.42321777 Validation Loss: 1567.4779052734375 Test Loss: 1900.876220703125\n",
            "Epoch: 2016  Training Loss: 1769.33300781 Validation Loss: 1567.41357421875 Test Loss: 1900.7576904296875\n",
            "Epoch: 2021  Training Loss: 1769.24316406 Validation Loss: 1567.3499755859375 Test Loss: 1900.639404296875\n",
            "Epoch: 2026  Training Loss: 1769.15319824 Validation Loss: 1567.287353515625 Test Loss: 1900.52197265625\n",
            "Epoch: 2031  Training Loss: 1769.06335449 Validation Loss: 1567.2249755859375 Test Loss: 1900.404541015625\n",
            "Epoch: 2036  Training Loss: 1768.97363281 Validation Loss: 1567.1634521484375 Test Loss: 1900.2877197265625\n",
            "Epoch: 2041  Training Loss: 1768.88391113 Validation Loss: 1567.102783203125 Test Loss: 1900.1712646484375\n",
            "Epoch: 2046  Training Loss: 1768.79406738 Validation Loss: 1567.0426025390625 Test Loss: 1900.055419921875\n",
            "Epoch: 2051  Training Loss: 1768.70471191 Validation Loss: 1566.9832763671875 Test Loss: 1899.939697265625\n",
            "Epoch: 2056  Training Loss: 1768.61511230 Validation Loss: 1566.9244384765625 Test Loss: 1899.824462890625\n",
            "Epoch: 2061  Training Loss: 1768.52575684 Validation Loss: 1566.8663330078125 Test Loss: 1899.70947265625\n",
            "Epoch: 2066  Training Loss: 1768.43640137 Validation Loss: 1566.8089599609375 Test Loss: 1899.594970703125\n",
            "Epoch: 2071  Training Loss: 1768.34716797 Validation Loss: 1566.752197265625 Test Loss: 1899.4808349609375\n",
            "Epoch: 2076  Training Loss: 1768.25793457 Validation Loss: 1566.6961669921875 Test Loss: 1899.366943359375\n",
            "Epoch: 2081  Training Loss: 1768.16857910 Validation Loss: 1566.640625 Test Loss: 1899.2535400390625\n",
            "Epoch: 2086  Training Loss: 1768.07934570 Validation Loss: 1566.5858154296875 Test Loss: 1899.140380859375\n",
            "Epoch: 2091  Training Loss: 1767.99047852 Validation Loss: 1566.5318603515625 Test Loss: 1899.0277099609375\n",
            "Epoch: 2096  Training Loss: 1767.90124512 Validation Loss: 1566.478271484375 Test Loss: 1898.915283203125\n",
            "Epoch: 2101  Training Loss: 1767.81237793 Validation Loss: 1566.4254150390625 Test Loss: 1898.8033447265625\n",
            "Epoch: 2106  Training Loss: 1767.72326660 Validation Loss: 1566.3729248046875 Test Loss: 1898.691650390625\n",
            "Epoch: 2111  Training Loss: 1767.63452148 Validation Loss: 1566.321533203125 Test Loss: 1898.5802001953125\n",
            "Epoch: 2116  Training Loss: 1767.54541016 Validation Loss: 1566.270263671875 Test Loss: 1898.4693603515625\n",
            "Epoch: 2121  Training Loss: 1767.45666504 Validation Loss: 1566.2200927734375 Test Loss: 1898.3585205078125\n",
            "Epoch: 2126  Training Loss: 1767.36791992 Validation Loss: 1566.1697998046875 Test Loss: 1898.248291015625\n",
            "Epoch: 2131  Training Loss: 1767.27917480 Validation Loss: 1566.1207275390625 Test Loss: 1898.1383056640625\n",
            "Epoch: 2136  Training Loss: 1767.19042969 Validation Loss: 1566.072021484375 Test Loss: 1898.0283203125\n",
            "Epoch: 2141  Training Loss: 1767.10168457 Validation Loss: 1566.0240478515625 Test Loss: 1897.9190673828125\n",
            "Epoch: 2146  Training Loss: 1767.01318359 Validation Loss: 1565.9766845703125 Test Loss: 1897.809814453125\n",
            "Epoch: 2151  Training Loss: 1766.92456055 Validation Loss: 1565.9295654296875 Test Loss: 1897.7010498046875\n",
            "Epoch: 2156  Training Loss: 1766.83593750 Validation Loss: 1565.88330078125 Test Loss: 1897.592529296875\n",
            "Epoch: 2161  Training Loss: 1766.74719238 Validation Loss: 1565.8375244140625 Test Loss: 1897.484375\n",
            "Epoch: 2166  Training Loss: 1766.65844727 Validation Loss: 1565.792236328125 Test Loss: 1897.37646484375\n",
            "Epoch: 2171  Training Loss: 1766.57006836 Validation Loss: 1565.74755859375 Test Loss: 1897.2689208984375\n",
            "Epoch: 2176  Training Loss: 1766.48144531 Validation Loss: 1565.703369140625 Test Loss: 1897.1614990234375\n",
            "Epoch: 2181  Training Loss: 1766.39282227 Validation Loss: 1565.6595458984375 Test Loss: 1897.0545654296875\n",
            "Epoch: 2186  Training Loss: 1766.30444336 Validation Loss: 1565.61669921875 Test Loss: 1896.9478759765625\n",
            "Epoch: 2191  Training Loss: 1766.21582031 Validation Loss: 1565.57421875 Test Loss: 1896.8414306640625\n",
            "Epoch: 2196  Training Loss: 1766.12731934 Validation Loss: 1565.5322265625 Test Loss: 1896.735107421875\n",
            "Epoch: 2201  Training Loss: 1766.03869629 Validation Loss: 1565.4906005859375 Test Loss: 1896.629150390625\n",
            "Epoch: 2206  Training Loss: 1765.95043945 Validation Loss: 1565.4495849609375 Test Loss: 1896.5234375\n",
            "Epoch: 2211  Training Loss: 1765.86169434 Validation Loss: 1565.4091796875 Test Loss: 1896.418212890625\n",
            "Epoch: 2216  Training Loss: 1765.77331543 Validation Loss: 1565.3692626953125 Test Loss: 1896.31298828125\n",
            "Epoch: 2221  Training Loss: 1765.68505859 Validation Loss: 1565.329833984375 Test Loss: 1896.2080078125\n",
            "Epoch: 2226  Training Loss: 1765.59631348 Validation Loss: 1565.2906494140625 Test Loss: 1896.103515625\n",
            "Epoch: 2231  Training Loss: 1765.50805664 Validation Loss: 1565.25244140625 Test Loss: 1895.9990234375\n",
            "Epoch: 2236  Training Loss: 1765.41943359 Validation Loss: 1565.2144775390625 Test Loss: 1895.8948974609375\n",
            "Epoch: 2241  Training Loss: 1765.33105469 Validation Loss: 1565.1768798828125 Test Loss: 1895.791015625\n",
            "Epoch: 2246  Training Loss: 1765.24267578 Validation Loss: 1565.139892578125 Test Loss: 1895.6875\n",
            "Epoch: 2251  Training Loss: 1765.15417480 Validation Loss: 1565.103271484375 Test Loss: 1895.584228515625\n",
            "Epoch: 2256  Training Loss: 1765.06542969 Validation Loss: 1565.0672607421875 Test Loss: 1895.480712890625\n",
            "Epoch: 2261  Training Loss: 1764.97717285 Validation Loss: 1565.0316162109375 Test Loss: 1895.3779296875\n",
            "Epoch: 2266  Training Loss: 1764.88867188 Validation Loss: 1564.9964599609375 Test Loss: 1895.275146484375\n",
            "Epoch: 2271  Training Loss: 1764.79992676 Validation Loss: 1564.9617919921875 Test Loss: 1895.172607421875\n",
            "Epoch: 2276  Training Loss: 1764.71142578 Validation Loss: 1564.927734375 Test Loss: 1895.0704345703125\n",
            "Epoch: 2281  Training Loss: 1764.62292480 Validation Loss: 1564.8939208984375 Test Loss: 1894.968505859375\n",
            "Epoch: 2286  Training Loss: 1764.53442383 Validation Loss: 1564.8604736328125 Test Loss: 1894.8665771484375\n",
            "Epoch: 2291  Training Loss: 1764.44567871 Validation Loss: 1564.8275146484375 Test Loss: 1894.764892578125\n",
            "Epoch: 2296  Training Loss: 1764.35729980 Validation Loss: 1564.795166015625 Test Loss: 1894.6634521484375\n",
            "Epoch: 2301  Training Loss: 1764.26867676 Validation Loss: 1564.76318359375 Test Loss: 1894.5623779296875\n",
            "Epoch: 2306  Training Loss: 1764.17993164 Validation Loss: 1564.7315673828125 Test Loss: 1894.46142578125\n",
            "Epoch: 2311  Training Loss: 1764.09143066 Validation Loss: 1564.7003173828125 Test Loss: 1894.360595703125\n",
            "Epoch: 2316  Training Loss: 1764.00268555 Validation Loss: 1564.669677734375 Test Loss: 1894.260009765625\n",
            "Epoch: 2321  Training Loss: 1763.91381836 Validation Loss: 1564.63916015625 Test Loss: 1894.15966796875\n",
            "Epoch: 2326  Training Loss: 1763.82543945 Validation Loss: 1564.609375 Test Loss: 1894.0595703125\n",
            "Epoch: 2331  Training Loss: 1763.73669434 Validation Loss: 1564.5799560546875 Test Loss: 1893.95947265625\n",
            "Epoch: 2336  Training Loss: 1763.64782715 Validation Loss: 1564.55078125 Test Loss: 1893.859619140625\n",
            "Epoch: 2341  Training Loss: 1763.55908203 Validation Loss: 1564.522216796875 Test Loss: 1893.7601318359375\n",
            "Epoch: 2346  Training Loss: 1763.47033691 Validation Loss: 1564.4937744140625 Test Loss: 1893.66064453125\n",
            "Epoch: 2351  Training Loss: 1763.38122559 Validation Loss: 1564.4658203125 Test Loss: 1893.5616455078125\n",
            "Epoch: 2356  Training Loss: 1763.29248047 Validation Loss: 1564.4384765625 Test Loss: 1893.4622802734375\n",
            "Epoch: 2361  Training Loss: 1763.20361328 Validation Loss: 1564.4112548828125 Test Loss: 1893.363525390625\n",
            "Epoch: 2366  Training Loss: 1763.11486816 Validation Loss: 1564.3843994140625 Test Loss: 1893.2647705078125\n",
            "Epoch: 2371  Training Loss: 1763.02575684 Validation Loss: 1564.3580322265625 Test Loss: 1893.166259765625\n",
            "Epoch: 2376  Training Loss: 1762.93688965 Validation Loss: 1564.33203125 Test Loss: 1893.0677490234375\n",
            "Epoch: 2381  Training Loss: 1762.84790039 Validation Loss: 1564.306396484375 Test Loss: 1892.9697265625\n",
            "Epoch: 2386  Training Loss: 1762.75891113 Validation Loss: 1564.281005859375 Test Loss: 1892.8714599609375\n",
            "Epoch: 2391  Training Loss: 1762.66979980 Validation Loss: 1564.256103515625 Test Loss: 1892.773681640625\n",
            "Epoch: 2396  Training Loss: 1762.58068848 Validation Loss: 1564.2315673828125 Test Loss: 1892.676025390625\n",
            "Epoch: 2401  Training Loss: 1762.49145508 Validation Loss: 1564.2073974609375 Test Loss: 1892.578369140625\n",
            "Epoch: 2406  Training Loss: 1762.40234375 Validation Loss: 1564.1834716796875 Test Loss: 1892.4810791015625\n",
            "Epoch: 2411  Training Loss: 1762.31335449 Validation Loss: 1564.159912109375 Test Loss: 1892.384033203125\n",
            "Epoch: 2416  Training Loss: 1762.22387695 Validation Loss: 1564.13671875 Test Loss: 1892.2867431640625\n",
            "Epoch: 2421  Training Loss: 1762.13452148 Validation Loss: 1564.114013671875 Test Loss: 1892.1898193359375\n",
            "Epoch: 2426  Training Loss: 1762.04541016 Validation Loss: 1564.091552734375 Test Loss: 1892.093017578125\n",
            "Epoch: 2431  Training Loss: 1761.95605469 Validation Loss: 1564.0693359375 Test Loss: 1891.9964599609375\n",
            "Epoch: 2436  Training Loss: 1761.86657715 Validation Loss: 1564.0474853515625 Test Loss: 1891.9000244140625\n",
            "Epoch: 2441  Training Loss: 1761.77722168 Validation Loss: 1564.026123046875 Test Loss: 1891.8033447265625\n",
            "Epoch: 2446  Training Loss: 1761.68762207 Validation Loss: 1564.0047607421875 Test Loss: 1891.7071533203125\n",
            "Epoch: 2451  Training Loss: 1761.59826660 Validation Loss: 1563.9840087890625 Test Loss: 1891.611083984375\n",
            "Epoch: 2456  Training Loss: 1761.50866699 Validation Loss: 1563.963623046875 Test Loss: 1891.515380859375\n",
            "Epoch: 2461  Training Loss: 1761.41906738 Validation Loss: 1563.943359375 Test Loss: 1891.4195556640625\n",
            "Epoch: 2466  Training Loss: 1761.32958984 Validation Loss: 1563.923583984375 Test Loss: 1891.3238525390625\n",
            "Epoch: 2471  Training Loss: 1761.23986816 Validation Loss: 1563.90380859375 Test Loss: 1891.2281494140625\n",
            "Epoch: 2476  Training Loss: 1761.15014648 Validation Loss: 1563.884521484375 Test Loss: 1891.1326904296875\n",
            "Epoch: 2481  Training Loss: 1761.06042480 Validation Loss: 1563.8656005859375 Test Loss: 1891.0374755859375\n",
            "Epoch: 2486  Training Loss: 1760.97058105 Validation Loss: 1563.8466796875 Test Loss: 1890.9422607421875\n",
            "Epoch: 2491  Training Loss: 1760.88073730 Validation Loss: 1563.8284912109375 Test Loss: 1890.84716796875\n",
            "Epoch: 2496  Training Loss: 1760.79089355 Validation Loss: 1563.810302734375 Test Loss: 1890.7520751953125\n",
            "Epoch: 2501  Training Loss: 1760.70080566 Validation Loss: 1563.79248046875 Test Loss: 1890.657470703125\n",
            "Epoch: 2506  Training Loss: 1760.61083984 Validation Loss: 1563.7750244140625 Test Loss: 1890.562744140625\n",
            "Epoch: 2511  Training Loss: 1760.52099609 Validation Loss: 1563.7576904296875 Test Loss: 1890.468017578125\n",
            "Epoch: 2516  Training Loss: 1760.43078613 Validation Loss: 1563.7408447265625 Test Loss: 1890.37353515625\n",
            "Epoch: 2521  Training Loss: 1760.34057617 Validation Loss: 1563.72412109375 Test Loss: 1890.2791748046875\n",
            "Epoch: 2526  Training Loss: 1760.25048828 Validation Loss: 1563.707763671875 Test Loss: 1890.1849365234375\n",
            "Epoch: 2531  Training Loss: 1760.16027832 Validation Loss: 1563.6915283203125 Test Loss: 1890.0908203125\n",
            "Epoch: 2536  Training Loss: 1760.07006836 Validation Loss: 1563.67578125 Test Loss: 1889.9967041015625\n",
            "Epoch: 2541  Training Loss: 1759.97961426 Validation Loss: 1563.659912109375 Test Loss: 1889.9027099609375\n",
            "Epoch: 2546  Training Loss: 1759.88940430 Validation Loss: 1563.6446533203125 Test Loss: 1889.8089599609375\n",
            "Epoch: 2551  Training Loss: 1759.79882812 Validation Loss: 1563.6297607421875 Test Loss: 1889.7152099609375\n",
            "Epoch: 2556  Training Loss: 1759.70861816 Validation Loss: 1563.61474609375 Test Loss: 1889.6214599609375\n",
            "Epoch: 2561  Training Loss: 1759.61816406 Validation Loss: 1563.6002197265625 Test Loss: 1889.52783203125\n",
            "Epoch: 2566  Training Loss: 1759.52734375 Validation Loss: 1563.5860595703125 Test Loss: 1889.4342041015625\n",
            "Epoch: 2571  Training Loss: 1759.43664551 Validation Loss: 1563.5718994140625 Test Loss: 1889.341064453125\n",
            "Epoch: 2576  Training Loss: 1759.34606934 Validation Loss: 1563.55810546875 Test Loss: 1889.2476806640625\n",
            "Epoch: 2581  Training Loss: 1759.25561523 Validation Loss: 1563.54443359375 Test Loss: 1889.154541015625\n",
            "Epoch: 2586  Training Loss: 1759.16467285 Validation Loss: 1563.53125 Test Loss: 1889.061279296875\n",
            "Epoch: 2591  Training Loss: 1759.07397461 Validation Loss: 1563.5181884765625 Test Loss: 1888.96826171875\n",
            "Epoch: 2596  Training Loss: 1758.98291016 Validation Loss: 1563.50537109375 Test Loss: 1888.87548828125\n",
            "Epoch: 2601  Training Loss: 1758.89208984 Validation Loss: 1563.492431640625 Test Loss: 1888.7825927734375\n",
            "Epoch: 2606  Training Loss: 1758.80114746 Validation Loss: 1563.4801025390625 Test Loss: 1888.6898193359375\n",
            "Epoch: 2611  Training Loss: 1758.71008301 Validation Loss: 1563.468017578125 Test Loss: 1888.5970458984375\n",
            "Epoch: 2616  Training Loss: 1758.61914062 Validation Loss: 1563.4560546875 Test Loss: 1888.50439453125\n",
            "Epoch: 2621  Training Loss: 1758.52783203 Validation Loss: 1563.4443359375 Test Loss: 1888.411865234375\n",
            "Epoch: 2626  Training Loss: 1758.43688965 Validation Loss: 1563.432861328125 Test Loss: 1888.3193359375\n",
            "Epoch: 2631  Training Loss: 1758.34558105 Validation Loss: 1563.421630859375 Test Loss: 1888.22705078125\n",
            "Epoch: 2636  Training Loss: 1758.25427246 Validation Loss: 1563.41064453125 Test Loss: 1888.134765625\n",
            "Epoch: 2641  Training Loss: 1758.16308594 Validation Loss: 1563.399658203125 Test Loss: 1888.04248046875\n",
            "Epoch: 2646  Training Loss: 1758.07153320 Validation Loss: 1563.388916015625 Test Loss: 1887.9501953125\n",
            "Epoch: 2651  Training Loss: 1757.98010254 Validation Loss: 1563.378662109375 Test Loss: 1887.858154296875\n",
            "Epoch: 2656  Training Loss: 1757.88867188 Validation Loss: 1563.3685302734375 Test Loss: 1887.7662353515625\n",
            "Epoch: 2661  Training Loss: 1757.79711914 Validation Loss: 1563.3583984375 Test Loss: 1887.674072265625\n",
            "Epoch: 2666  Training Loss: 1757.70568848 Validation Loss: 1563.3487548828125 Test Loss: 1887.582275390625\n",
            "Epoch: 2671  Training Loss: 1757.61389160 Validation Loss: 1563.3389892578125 Test Loss: 1887.490478515625\n",
            "Epoch: 2676  Training Loss: 1757.52233887 Validation Loss: 1563.329833984375 Test Loss: 1887.3985595703125\n",
            "Epoch: 2681  Training Loss: 1757.43041992 Validation Loss: 1563.320556640625 Test Loss: 1887.306884765625\n",
            "Epoch: 2686  Training Loss: 1757.33862305 Validation Loss: 1563.3115234375 Test Loss: 1887.2152099609375\n",
            "Epoch: 2691  Training Loss: 1757.24670410 Validation Loss: 1563.3026123046875 Test Loss: 1887.12353515625\n",
            "Epoch: 2696  Training Loss: 1757.15466309 Validation Loss: 1563.2939453125 Test Loss: 1887.031982421875\n",
            "Epoch: 2701  Training Loss: 1757.06286621 Validation Loss: 1563.28564453125 Test Loss: 1886.9404296875\n",
            "Epoch: 2706  Training Loss: 1756.97058105 Validation Loss: 1563.2772216796875 Test Loss: 1886.848876953125\n",
            "Epoch: 2711  Training Loss: 1756.87866211 Validation Loss: 1563.2691650390625 Test Loss: 1886.7576904296875\n",
            "Epoch: 2716  Training Loss: 1756.78662109 Validation Loss: 1563.26123046875 Test Loss: 1886.666259765625\n",
            "Epoch: 2721  Training Loss: 1756.69433594 Validation Loss: 1563.2535400390625 Test Loss: 1886.5748291015625\n",
            "Epoch: 2726  Training Loss: 1756.60217285 Validation Loss: 1563.2459716796875 Test Loss: 1886.4837646484375\n",
            "Epoch: 2731  Training Loss: 1756.50988770 Validation Loss: 1563.23876953125 Test Loss: 1886.392578125\n",
            "Epoch: 2736  Training Loss: 1756.41735840 Validation Loss: 1563.2314453125 Test Loss: 1886.30126953125\n",
            "Epoch: 2741  Training Loss: 1756.32495117 Validation Loss: 1563.2244873046875 Test Loss: 1886.210205078125\n",
            "Epoch: 2746  Training Loss: 1756.23242188 Validation Loss: 1563.2174072265625 Test Loss: 1886.119140625\n",
            "Epoch: 2751  Training Loss: 1756.13977051 Validation Loss: 1563.2108154296875 Test Loss: 1886.0283203125\n",
            "Epoch: 2756  Training Loss: 1756.04724121 Validation Loss: 1563.2044677734375 Test Loss: 1885.937255859375\n",
            "Epoch: 2761  Training Loss: 1755.95434570 Validation Loss: 1563.197998046875 Test Loss: 1885.8463134765625\n",
            "Epoch: 2766  Training Loss: 1755.86181641 Validation Loss: 1563.1917724609375 Test Loss: 1885.7554931640625\n",
            "Epoch: 2771  Training Loss: 1755.76916504 Validation Loss: 1563.1856689453125 Test Loss: 1885.66455078125\n",
            "Epoch: 2776  Training Loss: 1755.67614746 Validation Loss: 1563.179931640625 Test Loss: 1885.57373046875\n",
            "Epoch: 2781  Training Loss: 1755.58349609 Validation Loss: 1563.174072265625 Test Loss: 1885.48291015625\n",
            "Epoch: 2786  Training Loss: 1755.49047852 Validation Loss: 1563.1685791015625 Test Loss: 1885.3922119140625\n",
            "Epoch: 2791  Training Loss: 1755.39746094 Validation Loss: 1563.1632080078125 Test Loss: 1885.30126953125\n",
            "Epoch: 2796  Training Loss: 1755.30444336 Validation Loss: 1563.15771484375 Test Loss: 1885.2108154296875\n",
            "Epoch: 2801  Training Loss: 1755.21130371 Validation Loss: 1563.1527099609375 Test Loss: 1885.1201171875\n",
            "Epoch: 2806  Training Loss: 1755.11828613 Validation Loss: 1563.1478271484375 Test Loss: 1885.029541015625\n",
            "Epoch: 2811  Training Loss: 1755.02490234 Validation Loss: 1563.14306640625 Test Loss: 1884.93896484375\n",
            "Epoch: 2816  Training Loss: 1754.93176270 Validation Loss: 1563.13818359375 Test Loss: 1884.8482666015625\n",
            "Epoch: 2821  Training Loss: 1754.83825684 Validation Loss: 1563.1336669921875 Test Loss: 1884.7579345703125\n",
            "Epoch: 2826  Training Loss: 1754.74487305 Validation Loss: 1563.1292724609375 Test Loss: 1884.66748046875\n",
            "Epoch: 2831  Training Loss: 1754.65136719 Validation Loss: 1563.125 Test Loss: 1884.576904296875\n",
            "Epoch: 2836  Training Loss: 1754.55810547 Validation Loss: 1563.1209716796875 Test Loss: 1884.4864501953125\n",
            "Epoch: 2841  Training Loss: 1754.46435547 Validation Loss: 1563.11669921875 Test Loss: 1884.396240234375\n",
            "Epoch: 2846  Training Loss: 1754.37072754 Validation Loss: 1563.113037109375 Test Loss: 1884.3056640625\n",
            "Epoch: 2851  Training Loss: 1754.27709961 Validation Loss: 1563.109130859375 Test Loss: 1884.2152099609375\n",
            "Epoch: 2856  Training Loss: 1754.18322754 Validation Loss: 1563.105712890625 Test Loss: 1884.125\n",
            "Epoch: 2861  Training Loss: 1754.08959961 Validation Loss: 1563.1021728515625 Test Loss: 1884.0347900390625\n",
            "Epoch: 2866  Training Loss: 1753.99572754 Validation Loss: 1563.098876953125 Test Loss: 1883.9444580078125\n",
            "Epoch: 2871  Training Loss: 1753.90173340 Validation Loss: 1563.0955810546875 Test Loss: 1883.854248046875\n",
            "Epoch: 2876  Training Loss: 1753.80786133 Validation Loss: 1563.0924072265625 Test Loss: 1883.7640380859375\n",
            "Epoch: 2881  Training Loss: 1753.71374512 Validation Loss: 1563.0894775390625 Test Loss: 1883.673583984375\n",
            "Epoch: 2886  Training Loss: 1753.61975098 Validation Loss: 1563.0867919921875 Test Loss: 1883.5833740234375\n",
            "Epoch: 2891  Training Loss: 1753.52563477 Validation Loss: 1563.083984375 Test Loss: 1883.4931640625\n",
            "Epoch: 2896  Training Loss: 1753.43139648 Validation Loss: 1563.081298828125 Test Loss: 1883.4029541015625\n",
            "Epoch: 2901  Training Loss: 1753.33728027 Validation Loss: 1563.078857421875 Test Loss: 1883.31298828125\n",
            "Epoch: 2906  Training Loss: 1753.24279785 Validation Loss: 1563.076416015625 Test Loss: 1883.2227783203125\n",
            "Epoch: 2911  Training Loss: 1753.14843750 Validation Loss: 1563.07421875 Test Loss: 1883.1326904296875\n",
            "Epoch: 2916  Training Loss: 1753.05407715 Validation Loss: 1563.072021484375 Test Loss: 1883.04248046875\n",
            "Epoch: 2921  Training Loss: 1752.95959473 Validation Loss: 1563.070068359375 Test Loss: 1882.9522705078125\n",
            "Epoch: 2926  Training Loss: 1752.86499023 Validation Loss: 1563.0682373046875 Test Loss: 1882.8623046875\n",
            "Epoch: 2931  Training Loss: 1752.77050781 Validation Loss: 1563.066162109375 Test Loss: 1882.7720947265625\n",
            "Epoch: 2936  Training Loss: 1752.67590332 Validation Loss: 1563.0645751953125 Test Loss: 1882.6820068359375\n",
            "Epoch: 2941  Training Loss: 1752.58105469 Validation Loss: 1563.0628662109375 Test Loss: 1882.592041015625\n",
            "Epoch: 2946  Training Loss: 1752.48632812 Validation Loss: 1563.0614013671875 Test Loss: 1882.501953125\n",
            "Epoch: 2951  Training Loss: 1752.39135742 Validation Loss: 1563.0599365234375 Test Loss: 1882.411865234375\n",
            "Epoch: 2956  Training Loss: 1752.29663086 Validation Loss: 1563.05859375 Test Loss: 1882.3216552734375\n",
            "Epoch: 2961  Training Loss: 1752.20178223 Validation Loss: 1563.0574951171875 Test Loss: 1882.231689453125\n",
            "Epoch: 2966  Training Loss: 1752.10681152 Validation Loss: 1563.0562744140625 Test Loss: 1882.141845703125\n",
            "Epoch: 2971  Training Loss: 1752.01184082 Validation Loss: 1563.0552978515625 Test Loss: 1882.0516357421875\n",
            "Epoch: 2976  Training Loss: 1751.91650391 Validation Loss: 1563.054443359375 Test Loss: 1881.961669921875\n",
            "Epoch: 2981  Training Loss: 1751.82153320 Validation Loss: 1563.0537109375 Test Loss: 1881.87158203125\n",
            "Epoch: 2986  Training Loss: 1751.72619629 Validation Loss: 1563.052734375 Test Loss: 1881.78173828125\n",
            "Epoch: 2991  Training Loss: 1751.63110352 Validation Loss: 1563.0523681640625 Test Loss: 1881.691650390625\n",
            "Epoch: 2996  Training Loss: 1751.53564453 Validation Loss: 1563.0516357421875 Test Loss: 1881.6016845703125\n",
            "Epoch: 3001  Training Loss: 1751.44030762 Validation Loss: 1563.05126953125 Test Loss: 1881.5115966796875\n",
            "Epoch: 3006  Training Loss: 1751.34484863 Validation Loss: 1563.0509033203125 Test Loss: 1881.421630859375\n",
            "Epoch: 3011  Training Loss: 1751.24938965 Validation Loss: 1563.05078125 Test Loss: 1881.3314208984375\n",
            "Epoch: 3016  Training Loss: 1751.15393066 Validation Loss: 1563.0504150390625 Test Loss: 1881.241455078125\n",
            "Epoch: 3021  Training Loss: 1751.05810547 Validation Loss: 1563.0504150390625 Test Loss: 1881.1514892578125\n",
            "Epoch: 3026  Training Loss: 1750.96264648 Validation Loss: 1563.0504150390625 Test Loss: 1881.0616455078125\n",
            "Epoch: 3031  Training Loss: 1750.86694336 Validation Loss: 1563.0504150390625 Test Loss: 1880.971435546875\n",
            "Epoch: 3036  Training Loss: 1750.77111816 Validation Loss: 1563.050537109375 Test Loss: 1880.8814697265625\n",
            "Epoch: 3041  Training Loss: 1750.67517090 Validation Loss: 1563.05078125 Test Loss: 1880.7913818359375\n",
            "Epoch: 3046  Training Loss: 1750.57934570 Validation Loss: 1563.0511474609375 Test Loss: 1880.70166015625\n",
            "Epoch: 3051  Training Loss: 1750.48339844 Validation Loss: 1563.051513671875 Test Loss: 1880.6114501953125\n",
            "Epoch: 3056  Training Loss: 1750.38745117 Validation Loss: 1563.052001953125 Test Loss: 1880.521484375\n",
            "Epoch: 3061  Training Loss: 1750.29138184 Validation Loss: 1563.0526123046875 Test Loss: 1880.4312744140625\n",
            "Epoch: 3066  Training Loss: 1750.19543457 Validation Loss: 1563.0533447265625 Test Loss: 1880.3411865234375\n",
            "Epoch: 3071  Training Loss: 1750.09912109 Validation Loss: 1563.0538330078125 Test Loss: 1880.2513427734375\n",
            "Epoch: 3076  Training Loss: 1750.00292969 Validation Loss: 1563.0548095703125 Test Loss: 1880.1612548828125\n",
            "Epoch: 3081  Training Loss: 1749.90661621 Validation Loss: 1563.0555419921875 Test Loss: 1880.0712890625\n",
            "Epoch: 3086  Training Loss: 1749.81042480 Validation Loss: 1563.056396484375 Test Loss: 1879.9810791015625\n",
            "Epoch: 3091  Training Loss: 1749.71386719 Validation Loss: 1563.0574951171875 Test Loss: 1879.890869140625\n",
            "Epoch: 3096  Training Loss: 1749.61743164 Validation Loss: 1563.0584716796875 Test Loss: 1879.801025390625\n",
            "Epoch: 3101  Training Loss: 1749.52099609 Validation Loss: 1563.0595703125 Test Loss: 1879.7110595703125\n",
            "Epoch: 3106  Training Loss: 1749.42456055 Validation Loss: 1563.060791015625 Test Loss: 1879.620849609375\n",
            "Epoch: 3111  Training Loss: 1749.32812500 Validation Loss: 1563.0621337890625 Test Loss: 1879.5306396484375\n",
            "Epoch: 3116  Training Loss: 1749.23132324 Validation Loss: 1563.0634765625 Test Loss: 1879.4404296875\n",
            "Epoch: 3121  Training Loss: 1749.13452148 Validation Loss: 1563.06494140625 Test Loss: 1879.3505859375\n",
            "Epoch: 3126  Training Loss: 1749.03784180 Validation Loss: 1563.06640625 Test Loss: 1879.2601318359375\n",
            "Epoch: 3131  Training Loss: 1748.94116211 Validation Loss: 1563.0679931640625 Test Loss: 1879.170166015625\n",
            "Epoch: 3136  Training Loss: 1748.84411621 Validation Loss: 1563.0694580078125 Test Loss: 1879.080078125\n",
            "Epoch: 3141  Training Loss: 1748.74719238 Validation Loss: 1563.0709228515625 Test Loss: 1878.9896240234375\n",
            "Epoch: 3146  Training Loss: 1748.65026855 Validation Loss: 1563.072998046875 Test Loss: 1878.899658203125\n",
            "Epoch: 3151  Training Loss: 1748.55334473 Validation Loss: 1563.0745849609375 Test Loss: 1878.8092041015625\n",
            "Epoch: 3156  Training Loss: 1748.45617676 Validation Loss: 1563.076416015625 Test Loss: 1878.71923828125\n",
            "Epoch: 3161  Training Loss: 1748.35913086 Validation Loss: 1563.078369140625 Test Loss: 1878.629150390625\n",
            "Epoch: 3166  Training Loss: 1748.26196289 Validation Loss: 1563.080322265625 Test Loss: 1878.538818359375\n",
            "Epoch: 3171  Training Loss: 1748.16467285 Validation Loss: 1563.08251953125 Test Loss: 1878.4486083984375\n",
            "Epoch: 3176  Training Loss: 1748.06738281 Validation Loss: 1563.0845947265625 Test Loss: 1878.358154296875\n",
            "Epoch: 3181  Training Loss: 1747.97009277 Validation Loss: 1563.0865478515625 Test Loss: 1878.2679443359375\n",
            "Epoch: 3186  Training Loss: 1747.87292480 Validation Loss: 1563.0887451171875 Test Loss: 1878.177734375\n",
            "Epoch: 3191  Training Loss: 1747.77526855 Validation Loss: 1563.0911865234375 Test Loss: 1878.0872802734375\n",
            "Epoch: 3196  Training Loss: 1747.67785645 Validation Loss: 1563.0933837890625 Test Loss: 1877.9970703125\n",
            "Epoch: 3201  Training Loss: 1747.58032227 Validation Loss: 1563.0955810546875 Test Loss: 1877.90673828125\n",
            "Epoch: 3206  Training Loss: 1747.48266602 Validation Loss: 1563.0980224609375 Test Loss: 1877.816162109375\n",
            "Epoch: 3211  Training Loss: 1747.38500977 Validation Loss: 1563.1007080078125 Test Loss: 1877.725830078125\n",
            "Epoch: 3216  Training Loss: 1747.28735352 Validation Loss: 1563.1029052734375 Test Loss: 1877.635498046875\n",
            "Epoch: 3221  Training Loss: 1747.18957520 Validation Loss: 1563.105712890625 Test Loss: 1877.545166015625\n",
            "Epoch: 3226  Training Loss: 1747.09191895 Validation Loss: 1563.1082763671875 Test Loss: 1877.4547119140625\n",
            "Epoch: 3231  Training Loss: 1746.99414062 Validation Loss: 1563.11083984375 Test Loss: 1877.3641357421875\n",
            "Epoch: 3236  Training Loss: 1746.89599609 Validation Loss: 1563.1136474609375 Test Loss: 1877.273681640625\n",
            "Epoch: 3241  Training Loss: 1746.79821777 Validation Loss: 1563.1163330078125 Test Loss: 1877.1832275390625\n",
            "Epoch: 3246  Training Loss: 1746.70007324 Validation Loss: 1563.119140625 Test Loss: 1877.0927734375\n",
            "Epoch: 3251  Training Loss: 1746.60205078 Validation Loss: 1563.1220703125 Test Loss: 1877.0023193359375\n",
            "Epoch: 3256  Training Loss: 1746.50390625 Validation Loss: 1563.124755859375 Test Loss: 1876.9114990234375\n",
            "Epoch: 3261  Training Loss: 1746.40563965 Validation Loss: 1563.1279296875 Test Loss: 1876.821044921875\n",
            "Epoch: 3266  Training Loss: 1746.30773926 Validation Loss: 1563.1307373046875 Test Loss: 1876.7305908203125\n",
            "Epoch: 3271  Training Loss: 1746.20935059 Validation Loss: 1563.1337890625 Test Loss: 1876.639892578125\n",
            "Epoch: 3276  Training Loss: 1746.11108398 Validation Loss: 1563.136962890625 Test Loss: 1876.5491943359375\n",
            "Epoch: 3281  Training Loss: 1746.01281738 Validation Loss: 1563.1400146484375 Test Loss: 1876.4586181640625\n",
            "Epoch: 3286  Training Loss: 1745.91442871 Validation Loss: 1563.1431884765625 Test Loss: 1876.367919921875\n",
            "Epoch: 3291  Training Loss: 1745.81591797 Validation Loss: 1563.146484375 Test Loss: 1876.2772216796875\n",
            "Epoch: 3296  Training Loss: 1745.71740723 Validation Loss: 1563.149658203125 Test Loss: 1876.1866455078125\n",
            "Epoch: 3301  Training Loss: 1745.61877441 Validation Loss: 1563.1527099609375 Test Loss: 1876.0958251953125\n",
            "Epoch: 3306  Training Loss: 1745.52014160 Validation Loss: 1563.15625 Test Loss: 1876.0050048828125\n",
            "Epoch: 3311  Training Loss: 1745.42163086 Validation Loss: 1563.1595458984375 Test Loss: 1875.9141845703125\n",
            "Epoch: 3316  Training Loss: 1745.32287598 Validation Loss: 1563.162841796875 Test Loss: 1875.8236083984375\n",
            "Epoch: 3321  Training Loss: 1745.22412109 Validation Loss: 1563.1663818359375 Test Loss: 1875.732666015625\n",
            "Epoch: 3326  Training Loss: 1745.12536621 Validation Loss: 1563.1697998046875 Test Loss: 1875.641845703125\n",
            "Epoch: 3331  Training Loss: 1745.02661133 Validation Loss: 1563.1732177734375 Test Loss: 1875.551025390625\n",
            "Epoch: 3336  Training Loss: 1744.92749023 Validation Loss: 1563.1768798828125 Test Loss: 1875.460205078125\n",
            "Epoch: 3341  Training Loss: 1744.82873535 Validation Loss: 1563.1805419921875 Test Loss: 1875.3692626953125\n",
            "Epoch: 3346  Training Loss: 1744.72961426 Validation Loss: 1563.1839599609375 Test Loss: 1875.2783203125\n",
            "Epoch: 3351  Training Loss: 1744.63061523 Validation Loss: 1563.187744140625 Test Loss: 1875.1873779296875\n",
            "Epoch: 3356  Training Loss: 1744.53149414 Validation Loss: 1563.19140625 Test Loss: 1875.096435546875\n",
            "Epoch: 3361  Training Loss: 1744.43237305 Validation Loss: 1563.195068359375 Test Loss: 1875.0052490234375\n",
            "Epoch: 3366  Training Loss: 1744.33312988 Validation Loss: 1563.1988525390625 Test Loss: 1874.914306640625\n",
            "Epoch: 3371  Training Loss: 1744.23388672 Validation Loss: 1563.2025146484375 Test Loss: 1874.8232421875\n",
            "Epoch: 3376  Training Loss: 1744.13452148 Validation Loss: 1563.2064208984375 Test Loss: 1874.7320556640625\n",
            "Epoch: 3381  Training Loss: 1744.03527832 Validation Loss: 1563.2103271484375 Test Loss: 1874.64111328125\n",
            "Epoch: 3386  Training Loss: 1743.93603516 Validation Loss: 1563.214111328125 Test Loss: 1874.5499267578125\n",
            "Epoch: 3391  Training Loss: 1743.83642578 Validation Loss: 1563.2177734375 Test Loss: 1874.458984375\n",
            "Epoch: 3396  Training Loss: 1743.73718262 Validation Loss: 1563.2220458984375 Test Loss: 1874.3677978515625\n",
            "Epoch: 3401  Training Loss: 1743.63757324 Validation Loss: 1563.2259521484375 Test Loss: 1874.2764892578125\n",
            "Epoch: 3406  Training Loss: 1743.53808594 Validation Loss: 1563.22998046875 Test Loss: 1874.185302734375\n",
            "Epoch: 3411  Training Loss: 1743.43835449 Validation Loss: 1563.2337646484375 Test Loss: 1874.093994140625\n",
            "Epoch: 3416  Training Loss: 1743.33886719 Validation Loss: 1563.238037109375 Test Loss: 1874.002685546875\n",
            "Epoch: 3421  Training Loss: 1743.23913574 Validation Loss: 1563.2420654296875 Test Loss: 1873.911376953125\n",
            "Epoch: 3426  Training Loss: 1743.13940430 Validation Loss: 1563.246337890625 Test Loss: 1873.8201904296875\n",
            "Epoch: 3431  Training Loss: 1743.03967285 Validation Loss: 1563.2503662109375 Test Loss: 1873.728759765625\n",
            "Epoch: 3436  Training Loss: 1742.93994141 Validation Loss: 1563.25439453125 Test Loss: 1873.63720703125\n",
            "Epoch: 3441  Training Loss: 1742.83984375 Validation Loss: 1563.2586669921875 Test Loss: 1873.5460205078125\n",
            "Epoch: 3446  Training Loss: 1742.73999023 Validation Loss: 1563.2628173828125 Test Loss: 1873.45458984375\n",
            "Epoch: 3451  Training Loss: 1742.64013672 Validation Loss: 1563.267333984375 Test Loss: 1873.363037109375\n",
            "Epoch: 3456  Training Loss: 1742.54016113 Validation Loss: 1563.2713623046875 Test Loss: 1873.271728515625\n",
            "Epoch: 3461  Training Loss: 1742.43994141 Validation Loss: 1563.2755126953125 Test Loss: 1873.18017578125\n",
            "Epoch: 3466  Training Loss: 1742.33984375 Validation Loss: 1563.2799072265625 Test Loss: 1873.088623046875\n",
            "Epoch: 3471  Training Loss: 1742.23962402 Validation Loss: 1563.284423828125 Test Loss: 1872.9970703125\n",
            "Epoch: 3476  Training Loss: 1742.13964844 Validation Loss: 1563.28857421875 Test Loss: 1872.9053955078125\n",
            "Epoch: 3481  Training Loss: 1742.03942871 Validation Loss: 1563.29296875 Test Loss: 1872.8138427734375\n",
            "Epoch: 3486  Training Loss: 1741.93908691 Validation Loss: 1563.2974853515625 Test Loss: 1872.7222900390625\n",
            "Epoch: 3491  Training Loss: 1741.83874512 Validation Loss: 1563.3018798828125 Test Loss: 1872.630615234375\n",
            "Epoch: 3496  Training Loss: 1741.73840332 Validation Loss: 1563.306396484375 Test Loss: 1872.5389404296875\n",
            "Epoch: 3501  Training Loss: 1741.63818359 Validation Loss: 1563.310791015625 Test Loss: 1872.447021484375\n",
            "Epoch: 3506  Training Loss: 1741.53759766 Validation Loss: 1563.3153076171875 Test Loss: 1872.3553466796875\n",
            "Epoch: 3511  Training Loss: 1741.43701172 Validation Loss: 1563.31982421875 Test Loss: 1872.2635498046875\n",
            "Epoch: 3516  Training Loss: 1741.33642578 Validation Loss: 1563.32421875 Test Loss: 1872.1717529296875\n",
            "Epoch: 3521  Training Loss: 1741.23583984 Validation Loss: 1563.328857421875 Test Loss: 1872.079833984375\n",
            "Epoch: 3526  Training Loss: 1741.13537598 Validation Loss: 1563.33349609375 Test Loss: 1871.9881591796875\n",
            "Epoch: 3531  Training Loss: 1741.03466797 Validation Loss: 1563.3380126953125 Test Loss: 1871.896240234375\n",
            "Epoch: 3536  Training Loss: 1740.93408203 Validation Loss: 1563.3426513671875 Test Loss: 1871.80419921875\n",
            "Epoch: 3541  Training Loss: 1740.83312988 Validation Loss: 1563.3472900390625 Test Loss: 1871.71240234375\n",
            "Epoch: 3546  Training Loss: 1740.73242188 Validation Loss: 1563.351806640625 Test Loss: 1871.6204833984375\n",
            "Epoch: 3551  Training Loss: 1740.63171387 Validation Loss: 1563.3565673828125 Test Loss: 1871.5283203125\n",
            "Epoch: 3556  Training Loss: 1740.53063965 Validation Loss: 1563.3614501953125 Test Loss: 1871.436279296875\n",
            "Epoch: 3561  Training Loss: 1740.42980957 Validation Loss: 1563.365966796875 Test Loss: 1871.34423828125\n",
            "Epoch: 3566  Training Loss: 1740.32873535 Validation Loss: 1563.3707275390625 Test Loss: 1871.2523193359375\n",
            "Epoch: 3571  Training Loss: 1740.22790527 Validation Loss: 1563.3753662109375 Test Loss: 1871.1600341796875\n",
            "Epoch: 3576  Training Loss: 1740.12683105 Validation Loss: 1563.380126953125 Test Loss: 1871.0679931640625\n",
            "Epoch: 3581  Training Loss: 1740.02563477 Validation Loss: 1563.3848876953125 Test Loss: 1870.975830078125\n",
            "Epoch: 3586  Training Loss: 1739.92456055 Validation Loss: 1563.3896484375 Test Loss: 1870.883544921875\n",
            "Epoch: 3591  Training Loss: 1739.82348633 Validation Loss: 1563.39453125 Test Loss: 1870.7913818359375\n",
            "Epoch: 3596  Training Loss: 1739.72229004 Validation Loss: 1563.399658203125 Test Loss: 1870.6993408203125\n",
            "Epoch: 3601  Training Loss: 1739.62109375 Validation Loss: 1563.4041748046875 Test Loss: 1870.6070556640625\n",
            "Epoch: 3606  Training Loss: 1739.51989746 Validation Loss: 1563.4090576171875 Test Loss: 1870.5146484375\n",
            "Epoch: 3611  Training Loss: 1739.41845703 Validation Loss: 1563.4139404296875 Test Loss: 1870.422119140625\n",
            "Epoch: 3616  Training Loss: 1739.31713867 Validation Loss: 1563.418701171875 Test Loss: 1870.330078125\n",
            "Epoch: 3621  Training Loss: 1739.21582031 Validation Loss: 1563.4239501953125 Test Loss: 1870.2376708984375\n",
            "Epoch: 3626  Training Loss: 1739.11413574 Validation Loss: 1563.4287109375 Test Loss: 1870.1451416015625\n",
            "Epoch: 3631  Training Loss: 1739.01281738 Validation Loss: 1563.43359375 Test Loss: 1870.052734375\n",
            "Epoch: 3636  Training Loss: 1738.91137695 Validation Loss: 1563.4385986328125 Test Loss: 1869.96044921875\n",
            "Epoch: 3641  Training Loss: 1738.80969238 Validation Loss: 1563.443359375 Test Loss: 1869.8677978515625\n",
            "Epoch: 3646  Training Loss: 1738.70812988 Validation Loss: 1563.448486328125 Test Loss: 1869.775390625\n",
            "Epoch: 3651  Training Loss: 1738.60644531 Validation Loss: 1563.4534912109375 Test Loss: 1869.6827392578125\n",
            "Epoch: 3656  Training Loss: 1738.50512695 Validation Loss: 1563.45849609375 Test Loss: 1869.5902099609375\n",
            "Epoch: 3661  Training Loss: 1738.40319824 Validation Loss: 1563.46337890625 Test Loss: 1869.4976806640625\n",
            "Epoch: 3666  Training Loss: 1738.30151367 Validation Loss: 1563.4683837890625 Test Loss: 1869.405029296875\n",
            "Epoch: 3671  Training Loss: 1738.19982910 Validation Loss: 1563.4735107421875 Test Loss: 1869.312255859375\n",
            "Epoch: 3676  Training Loss: 1738.09777832 Validation Loss: 1563.478515625 Test Loss: 1869.2196044921875\n",
            "Epoch: 3681  Training Loss: 1737.99609375 Validation Loss: 1563.4833984375 Test Loss: 1869.126708984375\n",
            "Epoch: 3686  Training Loss: 1737.89404297 Validation Loss: 1563.4886474609375 Test Loss: 1869.0341796875\n",
            "Epoch: 3691  Training Loss: 1737.79211426 Validation Loss: 1563.4935302734375 Test Loss: 1868.9412841796875\n",
            "Epoch: 3696  Training Loss: 1737.69018555 Validation Loss: 1563.4986572265625 Test Loss: 1868.8486328125\n",
            "Epoch: 3701  Training Loss: 1737.58825684 Validation Loss: 1563.50390625 Test Loss: 1868.7557373046875\n",
            "Epoch: 3706  Training Loss: 1737.48620605 Validation Loss: 1563.509033203125 Test Loss: 1868.6630859375\n",
            "Epoch: 3711  Training Loss: 1737.38415527 Validation Loss: 1563.51416015625 Test Loss: 1868.5701904296875\n",
            "Epoch: 3716  Training Loss: 1737.28210449 Validation Loss: 1563.5191650390625 Test Loss: 1868.4771728515625\n",
            "Epoch: 3721  Training Loss: 1737.17993164 Validation Loss: 1563.5242919921875 Test Loss: 1868.38427734375\n",
            "Epoch: 3726  Training Loss: 1737.07763672 Validation Loss: 1563.529541015625 Test Loss: 1868.291259765625\n",
            "Epoch: 3731  Training Loss: 1736.97534180 Validation Loss: 1563.5345458984375 Test Loss: 1868.1983642578125\n",
            "Epoch: 3736  Training Loss: 1736.87316895 Validation Loss: 1563.5396728515625 Test Loss: 1868.1053466796875\n",
            "Epoch: 3741  Training Loss: 1736.77099609 Validation Loss: 1563.5447998046875 Test Loss: 1868.01220703125\n",
            "Epoch: 3746  Training Loss: 1736.66857910 Validation Loss: 1563.550048828125 Test Loss: 1867.919189453125\n",
            "Epoch: 3751  Training Loss: 1736.56628418 Validation Loss: 1563.5552978515625 Test Loss: 1867.826171875\n",
            "Epoch: 3756  Training Loss: 1736.46386719 Validation Loss: 1563.5604248046875 Test Loss: 1867.7330322265625\n",
            "Epoch: 3761  Training Loss: 1736.36132812 Validation Loss: 1563.565673828125 Test Loss: 1867.6397705078125\n",
            "Epoch: 3766  Training Loss: 1736.25915527 Validation Loss: 1563.5709228515625 Test Loss: 1867.5467529296875\n",
            "Epoch: 3771  Training Loss: 1736.15649414 Validation Loss: 1563.5760498046875 Test Loss: 1867.4537353515625\n",
            "Epoch: 3776  Training Loss: 1736.05395508 Validation Loss: 1563.5814208984375 Test Loss: 1867.3602294921875\n",
            "Epoch: 3781  Training Loss: 1735.95129395 Validation Loss: 1563.5865478515625 Test Loss: 1867.26708984375\n",
            "Epoch: 3786  Training Loss: 1735.84875488 Validation Loss: 1563.5919189453125 Test Loss: 1867.173828125\n",
            "Epoch: 3791  Training Loss: 1735.74621582 Validation Loss: 1563.5970458984375 Test Loss: 1867.0804443359375\n",
            "Epoch: 3796  Training Loss: 1735.64355469 Validation Loss: 1563.6021728515625 Test Loss: 1866.9871826171875\n",
            "Epoch: 3801  Training Loss: 1735.54089355 Validation Loss: 1563.6075439453125 Test Loss: 1866.8939208984375\n",
            "Epoch: 3806  Training Loss: 1735.43811035 Validation Loss: 1563.6129150390625 Test Loss: 1866.8004150390625\n",
            "Epoch: 3811  Training Loss: 1735.33544922 Validation Loss: 1563.6180419921875 Test Loss: 1866.7071533203125\n",
            "Epoch: 3816  Training Loss: 1735.23266602 Validation Loss: 1563.6231689453125 Test Loss: 1866.613525390625\n",
            "Epoch: 3821  Training Loss: 1735.12988281 Validation Loss: 1563.628662109375 Test Loss: 1866.52001953125\n",
            "Epoch: 3826  Training Loss: 1735.02685547 Validation Loss: 1563.634033203125 Test Loss: 1866.4266357421875\n",
            "Epoch: 3831  Training Loss: 1734.92407227 Validation Loss: 1563.63916015625 Test Loss: 1866.3330078125\n",
            "Epoch: 3836  Training Loss: 1734.82116699 Validation Loss: 1563.64453125 Test Loss: 1866.2396240234375\n",
            "Epoch: 3841  Training Loss: 1734.71813965 Validation Loss: 1563.6497802734375 Test Loss: 1866.1461181640625\n",
            "Epoch: 3846  Training Loss: 1734.61511230 Validation Loss: 1563.6549072265625 Test Loss: 1866.052490234375\n",
            "Epoch: 3851  Training Loss: 1734.51232910 Validation Loss: 1563.6602783203125 Test Loss: 1865.9588623046875\n",
            "Epoch: 3856  Training Loss: 1734.40930176 Validation Loss: 1563.6654052734375 Test Loss: 1865.8651123046875\n",
            "Epoch: 3861  Training Loss: 1734.30615234 Validation Loss: 1563.6708984375 Test Loss: 1865.771484375\n",
            "Epoch: 3866  Training Loss: 1734.20312500 Validation Loss: 1563.6761474609375 Test Loss: 1865.677978515625\n",
            "Epoch: 3871  Training Loss: 1734.09985352 Validation Loss: 1563.681640625 Test Loss: 1865.584228515625\n",
            "Epoch: 3876  Training Loss: 1733.99682617 Validation Loss: 1563.6868896484375 Test Loss: 1865.490478515625\n",
            "Epoch: 3881  Training Loss: 1733.89355469 Validation Loss: 1563.6922607421875 Test Loss: 1865.396484375\n",
            "Epoch: 3886  Training Loss: 1733.79040527 Validation Loss: 1563.6976318359375 Test Loss: 1865.302734375\n",
            "Epoch: 3891  Training Loss: 1733.68701172 Validation Loss: 1563.702880859375 Test Loss: 1865.208984375\n",
            "Epoch: 3896  Training Loss: 1733.58361816 Validation Loss: 1563.7081298828125 Test Loss: 1865.114990234375\n",
            "Epoch: 3901  Training Loss: 1733.48034668 Validation Loss: 1563.713623046875 Test Loss: 1865.021240234375\n",
            "Epoch: 3906  Training Loss: 1733.37707520 Validation Loss: 1563.718994140625 Test Loss: 1864.9273681640625\n",
            "Epoch: 3911  Training Loss: 1733.27380371 Validation Loss: 1563.72412109375 Test Loss: 1864.8333740234375\n",
            "Epoch: 3916  Training Loss: 1733.17028809 Validation Loss: 1563.7296142578125 Test Loss: 1864.7393798828125\n",
            "Epoch: 3921  Training Loss: 1733.06689453 Validation Loss: 1563.7349853515625 Test Loss: 1864.6456298828125\n",
            "Epoch: 3926  Training Loss: 1732.96337891 Validation Loss: 1563.740234375 Test Loss: 1864.5516357421875\n",
            "Epoch: 3931  Training Loss: 1732.85986328 Validation Loss: 1563.74560546875 Test Loss: 1864.45751953125\n",
            "Epoch: 3936  Training Loss: 1732.75634766 Validation Loss: 1563.7509765625 Test Loss: 1864.363525390625\n",
            "Epoch: 3941  Training Loss: 1732.65283203 Validation Loss: 1563.7562255859375 Test Loss: 1864.269287109375\n",
            "Epoch: 3946  Training Loss: 1732.54931641 Validation Loss: 1563.76171875 Test Loss: 1864.17529296875\n",
            "Epoch: 3951  Training Loss: 1732.44567871 Validation Loss: 1563.76708984375 Test Loss: 1864.0811767578125\n",
            "Epoch: 3956  Training Loss: 1732.34204102 Validation Loss: 1563.7724609375 Test Loss: 1863.987060546875\n",
            "Epoch: 3961  Training Loss: 1732.23852539 Validation Loss: 1563.7779541015625 Test Loss: 1863.8929443359375\n",
            "Epoch: 3966  Training Loss: 1732.13488770 Validation Loss: 1563.7830810546875 Test Loss: 1863.798583984375\n",
            "Epoch: 3971  Training Loss: 1732.03112793 Validation Loss: 1563.78857421875 Test Loss: 1863.70458984375\n",
            "Epoch: 3976  Training Loss: 1731.92749023 Validation Loss: 1563.793701171875 Test Loss: 1863.6102294921875\n",
            "Epoch: 3981  Training Loss: 1731.82360840 Validation Loss: 1563.79931640625 Test Loss: 1863.51611328125\n",
            "Epoch: 3986  Training Loss: 1731.71984863 Validation Loss: 1563.804443359375 Test Loss: 1863.4217529296875\n",
            "Epoch: 3991  Training Loss: 1731.61596680 Validation Loss: 1563.81005859375 Test Loss: 1863.3272705078125\n",
            "Epoch: 3996  Training Loss: 1731.51208496 Validation Loss: 1563.8154296875 Test Loss: 1863.2330322265625\n",
            "Epoch: 4001  Training Loss: 1731.40832520 Validation Loss: 1563.820556640625 Test Loss: 1863.138671875\n",
            "Epoch: 4006  Training Loss: 1731.30444336 Validation Loss: 1563.8260498046875 Test Loss: 1863.044189453125\n",
            "Epoch: 4011  Training Loss: 1731.20043945 Validation Loss: 1563.8314208984375 Test Loss: 1862.9498291015625\n",
            "Epoch: 4016  Training Loss: 1731.09667969 Validation Loss: 1563.8365478515625 Test Loss: 1862.8555908203125\n",
            "Epoch: 4021  Training Loss: 1730.99267578 Validation Loss: 1563.842041015625 Test Loss: 1862.7611083984375\n",
            "Epoch: 4026  Training Loss: 1730.88867188 Validation Loss: 1563.847412109375 Test Loss: 1862.6666259765625\n",
            "Epoch: 4031  Training Loss: 1730.78466797 Validation Loss: 1563.8525390625 Test Loss: 1862.572021484375\n",
            "Epoch: 4036  Training Loss: 1730.68078613 Validation Loss: 1563.85791015625 Test Loss: 1862.4775390625\n",
            "Epoch: 4041  Training Loss: 1730.57641602 Validation Loss: 1563.8634033203125 Test Loss: 1862.3829345703125\n",
            "Epoch: 4046  Training Loss: 1730.47241211 Validation Loss: 1563.8687744140625 Test Loss: 1862.288330078125\n",
            "Epoch: 4051  Training Loss: 1730.36840820 Validation Loss: 1563.8740234375 Test Loss: 1862.19384765625\n",
            "Epoch: 4056  Training Loss: 1730.26428223 Validation Loss: 1563.87939453125 Test Loss: 1862.099365234375\n",
            "Epoch: 4061  Training Loss: 1730.16015625 Validation Loss: 1563.8848876953125 Test Loss: 1862.0047607421875\n",
            "Epoch: 4066  Training Loss: 1730.05590820 Validation Loss: 1563.8900146484375 Test Loss: 1861.909912109375\n",
            "Epoch: 4071  Training Loss: 1729.95178223 Validation Loss: 1563.895263671875 Test Loss: 1861.8154296875\n",
            "Epoch: 4076  Training Loss: 1729.84765625 Validation Loss: 1563.90087890625 Test Loss: 1861.720458984375\n",
            "Epoch: 4081  Training Loss: 1729.74328613 Validation Loss: 1563.906005859375 Test Loss: 1861.6259765625\n",
            "Epoch: 4086  Training Loss: 1729.63916016 Validation Loss: 1563.911376953125 Test Loss: 1861.5313720703125\n",
            "Epoch: 4091  Training Loss: 1729.53479004 Validation Loss: 1563.916748046875 Test Loss: 1861.4365234375\n",
            "Epoch: 4096  Training Loss: 1729.43041992 Validation Loss: 1563.921875 Test Loss: 1861.3416748046875\n",
            "Epoch: 4101  Training Loss: 1729.32617188 Validation Loss: 1563.9273681640625 Test Loss: 1861.2467041015625\n",
            "Epoch: 4106  Training Loss: 1729.22180176 Validation Loss: 1563.9327392578125 Test Loss: 1861.152099609375\n",
            "Epoch: 4111  Training Loss: 1729.11743164 Validation Loss: 1563.9378662109375 Test Loss: 1861.057373046875\n",
            "Epoch: 4116  Training Loss: 1729.01293945 Validation Loss: 1563.9432373046875 Test Loss: 1860.96240234375\n",
            "Epoch: 4121  Training Loss: 1728.90856934 Validation Loss: 1563.948486328125 Test Loss: 1860.867431640625\n",
            "Epoch: 4126  Training Loss: 1728.80407715 Validation Loss: 1563.9537353515625 Test Loss: 1860.7725830078125\n",
            "Epoch: 4131  Training Loss: 1728.69958496 Validation Loss: 1563.958984375 Test Loss: 1860.6776123046875\n",
            "Epoch: 4136  Training Loss: 1728.59509277 Validation Loss: 1563.96435546875 Test Loss: 1860.5826416015625\n",
            "Epoch: 4141  Training Loss: 1728.49084473 Validation Loss: 1563.9697265625 Test Loss: 1860.48779296875\n",
            "Epoch: 4146  Training Loss: 1728.38610840 Validation Loss: 1563.9749755859375 Test Loss: 1860.392822265625\n",
            "Epoch: 4151  Training Loss: 1728.28186035 Validation Loss: 1563.9801025390625 Test Loss: 1860.2978515625\n",
            "Epoch: 4156  Training Loss: 1728.17712402 Validation Loss: 1563.9854736328125 Test Loss: 1860.202880859375\n",
            "Epoch: 4161  Training Loss: 1728.07263184 Validation Loss: 1563.9906005859375 Test Loss: 1860.107666015625\n",
            "Epoch: 4166  Training Loss: 1727.96801758 Validation Loss: 1563.9957275390625 Test Loss: 1860.0126953125\n",
            "Epoch: 4171  Training Loss: 1727.86340332 Validation Loss: 1564.0010986328125 Test Loss: 1859.91748046875\n",
            "Epoch: 4176  Training Loss: 1727.75854492 Validation Loss: 1564.0062255859375 Test Loss: 1859.8223876953125\n",
            "Epoch: 4181  Training Loss: 1727.65405273 Validation Loss: 1564.0115966796875 Test Loss: 1859.727294921875\n",
            "Epoch: 4186  Training Loss: 1727.54919434 Validation Loss: 1564.0169677734375 Test Loss: 1859.632080078125\n",
            "Epoch: 4191  Training Loss: 1727.44470215 Validation Loss: 1564.022216796875 Test Loss: 1859.537109375\n",
            "Epoch: 4196  Training Loss: 1727.33984375 Validation Loss: 1564.0272216796875 Test Loss: 1859.44189453125\n",
            "Epoch: 4201  Training Loss: 1727.23522949 Validation Loss: 1564.0325927734375 Test Loss: 1859.3466796875\n",
            "Epoch: 4206  Training Loss: 1727.13037109 Validation Loss: 1564.0377197265625 Test Loss: 1859.2513427734375\n",
            "Epoch: 4211  Training Loss: 1727.02575684 Validation Loss: 1564.0428466796875 Test Loss: 1859.15625\n",
            "Epoch: 4216  Training Loss: 1726.92089844 Validation Loss: 1564.0479736328125 Test Loss: 1859.0609130859375\n",
            "Epoch: 4221  Training Loss: 1726.81628418 Validation Loss: 1564.0533447265625 Test Loss: 1858.9656982421875\n",
            "Epoch: 4226  Training Loss: 1726.71130371 Validation Loss: 1564.0584716796875 Test Loss: 1858.8702392578125\n",
            "Epoch: 4231  Training Loss: 1726.60644531 Validation Loss: 1564.0634765625 Test Loss: 1858.77490234375\n",
            "Epoch: 4236  Training Loss: 1726.50183105 Validation Loss: 1564.0687255859375 Test Loss: 1858.6795654296875\n",
            "Epoch: 4241  Training Loss: 1726.39660645 Validation Loss: 1564.0738525390625 Test Loss: 1858.58447265625\n",
            "Epoch: 4246  Training Loss: 1726.29199219 Validation Loss: 1564.0792236328125 Test Loss: 1858.4891357421875\n",
            "Epoch: 4251  Training Loss: 1726.18713379 Validation Loss: 1564.084228515625 Test Loss: 1858.3936767578125\n",
            "Epoch: 4256  Training Loss: 1726.08203125 Validation Loss: 1564.08935546875 Test Loss: 1858.2982177734375\n",
            "Epoch: 4261  Training Loss: 1725.97717285 Validation Loss: 1564.094482421875 Test Loss: 1858.202880859375\n",
            "Epoch: 4266  Training Loss: 1725.87219238 Validation Loss: 1564.099609375 Test Loss: 1858.1072998046875\n",
            "Epoch: 4271  Training Loss: 1725.76721191 Validation Loss: 1564.1046142578125 Test Loss: 1858.011962890625\n",
            "Epoch: 4276  Training Loss: 1725.66223145 Validation Loss: 1564.1097412109375 Test Loss: 1857.9163818359375\n",
            "Epoch: 4281  Training Loss: 1725.55737305 Validation Loss: 1564.11474609375 Test Loss: 1857.821044921875\n",
            "Epoch: 4286  Training Loss: 1725.45227051 Validation Loss: 1564.119873046875 Test Loss: 1857.7255859375\n",
            "Epoch: 4291  Training Loss: 1725.34741211 Validation Loss: 1564.125 Test Loss: 1857.6298828125\n",
            "Epoch: 4296  Training Loss: 1725.24230957 Validation Loss: 1564.1300048828125 Test Loss: 1857.5343017578125\n",
            "Epoch: 4301  Training Loss: 1725.13708496 Validation Loss: 1564.1351318359375 Test Loss: 1857.4388427734375\n",
            "Epoch: 4306  Training Loss: 1725.03210449 Validation Loss: 1564.1400146484375 Test Loss: 1857.3431396484375\n",
            "Epoch: 4311  Training Loss: 1724.92700195 Validation Loss: 1564.145263671875 Test Loss: 1857.2476806640625\n",
            "Epoch: 4316  Training Loss: 1724.82202148 Validation Loss: 1564.150146484375 Test Loss: 1857.152099609375\n",
            "Epoch: 4321  Training Loss: 1724.71691895 Validation Loss: 1564.1551513671875 Test Loss: 1857.0562744140625\n",
            "Epoch: 4326  Training Loss: 1724.61181641 Validation Loss: 1564.16015625 Test Loss: 1856.960693359375\n",
            "Epoch: 4331  Training Loss: 1724.50659180 Validation Loss: 1564.1650390625 Test Loss: 1856.864990234375\n",
            "Epoch: 4336  Training Loss: 1724.40161133 Validation Loss: 1564.1700439453125 Test Loss: 1856.7691650390625\n",
            "Epoch: 4341  Training Loss: 1724.29626465 Validation Loss: 1564.175048828125 Test Loss: 1856.673583984375\n",
            "Epoch: 4346  Training Loss: 1724.19128418 Validation Loss: 1564.179931640625 Test Loss: 1856.5780029296875\n",
            "Epoch: 4351  Training Loss: 1724.08593750 Validation Loss: 1564.1849365234375 Test Loss: 1856.4822998046875\n",
            "Epoch: 4356  Training Loss: 1723.98083496 Validation Loss: 1564.18994140625 Test Loss: 1856.386474609375\n",
            "Epoch: 4361  Training Loss: 1723.87548828 Validation Loss: 1564.1947021484375 Test Loss: 1856.2906494140625\n",
            "Epoch: 4366  Training Loss: 1723.77038574 Validation Loss: 1564.1995849609375 Test Loss: 1856.195068359375\n",
            "Epoch: 4371  Training Loss: 1723.66503906 Validation Loss: 1564.2044677734375 Test Loss: 1856.099365234375\n",
            "Epoch: 4376  Training Loss: 1723.55981445 Validation Loss: 1564.2095947265625 Test Loss: 1856.0035400390625\n",
            "Epoch: 4381  Training Loss: 1723.45458984 Validation Loss: 1564.21435546875 Test Loss: 1855.9075927734375\n",
            "Epoch: 4386  Training Loss: 1723.34936523 Validation Loss: 1564.2193603515625 Test Loss: 1855.811767578125\n",
            "Epoch: 4391  Training Loss: 1723.24414062 Validation Loss: 1564.2242431640625 Test Loss: 1855.716064453125\n",
            "Epoch: 4396  Training Loss: 1723.13891602 Validation Loss: 1564.22900390625 Test Loss: 1855.6199951171875\n",
            "Epoch: 4401  Training Loss: 1723.03332520 Validation Loss: 1564.2337646484375 Test Loss: 1855.524169921875\n",
            "Epoch: 4406  Training Loss: 1722.92810059 Validation Loss: 1564.2386474609375 Test Loss: 1855.4283447265625\n",
            "Epoch: 4411  Training Loss: 1722.82287598 Validation Loss: 1564.243408203125 Test Loss: 1855.3323974609375\n",
            "Epoch: 4416  Training Loss: 1722.71752930 Validation Loss: 1564.2481689453125 Test Loss: 1855.2364501953125\n",
            "Epoch: 4421  Training Loss: 1722.61218262 Validation Loss: 1564.253173828125 Test Loss: 1855.140625\n",
            "Epoch: 4426  Training Loss: 1722.50683594 Validation Loss: 1564.2576904296875 Test Loss: 1855.044677734375\n",
            "Epoch: 4431  Training Loss: 1722.40136719 Validation Loss: 1564.262451171875 Test Loss: 1854.9486083984375\n",
            "Epoch: 4436  Training Loss: 1722.29626465 Validation Loss: 1564.267333984375 Test Loss: 1854.8525390625\n",
            "Epoch: 4441  Training Loss: 1722.19067383 Validation Loss: 1564.2718505859375 Test Loss: 1854.7568359375\n",
            "Epoch: 4446  Training Loss: 1722.08532715 Validation Loss: 1564.27685546875 Test Loss: 1854.66064453125\n",
            "Epoch: 4451  Training Loss: 1721.97998047 Validation Loss: 1564.28125 Test Loss: 1854.5648193359375\n",
            "Epoch: 4456  Training Loss: 1721.87438965 Validation Loss: 1564.2862548828125 Test Loss: 1854.46875\n",
            "Epoch: 4461  Training Loss: 1721.76916504 Validation Loss: 1564.2906494140625 Test Loss: 1854.3726806640625\n",
            "Epoch: 4466  Training Loss: 1721.66369629 Validation Loss: 1564.29541015625 Test Loss: 1854.2764892578125\n",
            "Epoch: 4471  Training Loss: 1721.55822754 Validation Loss: 1564.300048828125 Test Loss: 1854.1805419921875\n",
            "Epoch: 4476  Training Loss: 1721.45288086 Validation Loss: 1564.3048095703125 Test Loss: 1854.0843505859375\n",
            "Epoch: 4481  Training Loss: 1721.34741211 Validation Loss: 1564.309326171875 Test Loss: 1853.9881591796875\n",
            "Epoch: 4486  Training Loss: 1721.24194336 Validation Loss: 1564.3138427734375 Test Loss: 1853.89208984375\n",
            "Epoch: 4491  Training Loss: 1721.13647461 Validation Loss: 1564.318359375 Test Loss: 1853.796142578125\n",
            "Epoch: 4496  Training Loss: 1721.03088379 Validation Loss: 1564.3231201171875 Test Loss: 1853.6998291015625\n",
            "Epoch: 4501  Training Loss: 1720.92553711 Validation Loss: 1564.3275146484375 Test Loss: 1853.6038818359375\n",
            "Epoch: 4506  Training Loss: 1720.82006836 Validation Loss: 1564.3321533203125 Test Loss: 1853.5076904296875\n",
            "Epoch: 4511  Training Loss: 1720.71435547 Validation Loss: 1564.3367919921875 Test Loss: 1853.4114990234375\n",
            "Epoch: 4516  Training Loss: 1720.60913086 Validation Loss: 1564.3411865234375 Test Loss: 1853.3154296875\n",
            "Epoch: 4521  Training Loss: 1720.50366211 Validation Loss: 1564.3458251953125 Test Loss: 1853.21923828125\n",
            "Epoch: 4526  Training Loss: 1720.39794922 Validation Loss: 1564.350341796875 Test Loss: 1853.1229248046875\n",
            "Epoch: 4531  Training Loss: 1720.29260254 Validation Loss: 1564.3546142578125 Test Loss: 1853.02685546875\n",
            "Epoch: 4536  Training Loss: 1720.18701172 Validation Loss: 1564.359130859375 Test Loss: 1852.9305419921875\n",
            "Epoch: 4541  Training Loss: 1720.08154297 Validation Loss: 1564.3636474609375 Test Loss: 1852.8343505859375\n",
            "Epoch: 4546  Training Loss: 1719.97607422 Validation Loss: 1564.3680419921875 Test Loss: 1852.7379150390625\n",
            "Epoch: 4551  Training Loss: 1719.87036133 Validation Loss: 1564.3721923828125 Test Loss: 1852.641845703125\n",
            "Epoch: 4556  Training Loss: 1719.76477051 Validation Loss: 1564.376708984375 Test Loss: 1852.54541015625\n",
            "Epoch: 4561  Training Loss: 1719.65930176 Validation Loss: 1564.381103515625 Test Loss: 1852.4493408203125\n",
            "Epoch: 4566  Training Loss: 1719.55383301 Validation Loss: 1564.38525390625 Test Loss: 1852.3529052734375\n",
            "Epoch: 4571  Training Loss: 1719.44812012 Validation Loss: 1564.389892578125 Test Loss: 1852.2564697265625\n",
            "Epoch: 4576  Training Loss: 1719.34265137 Validation Loss: 1564.3941650390625 Test Loss: 1852.160400390625\n",
            "Epoch: 4581  Training Loss: 1719.23706055 Validation Loss: 1564.3983154296875 Test Loss: 1852.06396484375\n",
            "Epoch: 4586  Training Loss: 1719.13134766 Validation Loss: 1564.402587890625 Test Loss: 1851.967529296875\n",
            "Epoch: 4591  Training Loss: 1719.02587891 Validation Loss: 1564.4068603515625 Test Loss: 1851.8714599609375\n",
            "Epoch: 4596  Training Loss: 1718.92041016 Validation Loss: 1564.4112548828125 Test Loss: 1851.7750244140625\n",
            "Epoch: 4601  Training Loss: 1718.81457520 Validation Loss: 1564.4154052734375 Test Loss: 1851.678466796875\n",
            "Epoch: 4606  Training Loss: 1718.70910645 Validation Loss: 1564.419677734375 Test Loss: 1851.582275390625\n",
            "Epoch: 4611  Training Loss: 1718.60363770 Validation Loss: 1564.4239501953125 Test Loss: 1851.48583984375\n",
            "Epoch: 4616  Training Loss: 1718.49804688 Validation Loss: 1564.427978515625 Test Loss: 1851.389404296875\n",
            "Epoch: 4621  Training Loss: 1718.39233398 Validation Loss: 1564.4320068359375 Test Loss: 1851.2930908203125\n",
            "Epoch: 4626  Training Loss: 1718.28674316 Validation Loss: 1564.43603515625 Test Loss: 1851.19677734375\n",
            "Epoch: 4631  Training Loss: 1718.18115234 Validation Loss: 1564.4403076171875 Test Loss: 1851.1002197265625\n",
            "Epoch: 4636  Training Loss: 1718.07543945 Validation Loss: 1564.4444580078125 Test Loss: 1851.0037841796875\n",
            "Epoch: 4641  Training Loss: 1717.96997070 Validation Loss: 1564.448486328125 Test Loss: 1850.907470703125\n",
            "Epoch: 4646  Training Loss: 1717.86450195 Validation Loss: 1564.452392578125 Test Loss: 1850.8109130859375\n",
            "Epoch: 4651  Training Loss: 1717.75891113 Validation Loss: 1564.4566650390625 Test Loss: 1850.7144775390625\n",
            "Epoch: 4656  Training Loss: 1717.65319824 Validation Loss: 1564.46044921875 Test Loss: 1850.617919921875\n",
            "Epoch: 4661  Training Loss: 1717.54760742 Validation Loss: 1564.46435546875 Test Loss: 1850.521484375\n",
            "Epoch: 4666  Training Loss: 1717.44177246 Validation Loss: 1564.4683837890625 Test Loss: 1850.4251708984375\n",
            "Epoch: 4671  Training Loss: 1717.33630371 Validation Loss: 1564.4722900390625 Test Loss: 1850.3287353515625\n",
            "Epoch: 4676  Training Loss: 1717.23059082 Validation Loss: 1564.4764404296875 Test Loss: 1850.232177734375\n",
            "Epoch: 4681  Training Loss: 1717.12500000 Validation Loss: 1564.4803466796875 Test Loss: 1850.1356201171875\n",
            "Epoch: 4686  Training Loss: 1717.01940918 Validation Loss: 1564.484130859375 Test Loss: 1850.0390625\n",
            "Epoch: 4691  Training Loss: 1716.91369629 Validation Loss: 1564.488037109375 Test Loss: 1849.9425048828125\n",
            "Epoch: 4696  Training Loss: 1716.80822754 Validation Loss: 1564.491943359375 Test Loss: 1849.8460693359375\n",
            "Epoch: 4701  Training Loss: 1716.70263672 Validation Loss: 1564.4957275390625 Test Loss: 1849.7493896484375\n",
            "Epoch: 4706  Training Loss: 1716.59692383 Validation Loss: 1564.4996337890625 Test Loss: 1849.6529541015625\n",
            "Epoch: 4711  Training Loss: 1716.49133301 Validation Loss: 1564.5032958984375 Test Loss: 1849.5562744140625\n",
            "Epoch: 4716  Training Loss: 1716.38562012 Validation Loss: 1564.5072021484375 Test Loss: 1849.459716796875\n",
            "Epoch: 4721  Training Loss: 1716.28015137 Validation Loss: 1564.5108642578125 Test Loss: 1849.3631591796875\n",
            "Epoch: 4726  Training Loss: 1716.17431641 Validation Loss: 1564.5145263671875 Test Loss: 1849.2664794921875\n",
            "Epoch: 4731  Training Loss: 1716.06884766 Validation Loss: 1564.5181884765625 Test Loss: 1849.169921875\n",
            "Epoch: 4736  Training Loss: 1715.96325684 Validation Loss: 1564.5218505859375 Test Loss: 1849.0733642578125\n",
            "Epoch: 4741  Training Loss: 1715.85742188 Validation Loss: 1564.5255126953125 Test Loss: 1848.9766845703125\n",
            "Epoch: 4746  Training Loss: 1715.75195312 Validation Loss: 1564.529052734375 Test Loss: 1848.880126953125\n",
            "Epoch: 4751  Training Loss: 1715.64648438 Validation Loss: 1564.53271484375 Test Loss: 1848.7838134765625\n",
            "Epoch: 4756  Training Loss: 1715.54064941 Validation Loss: 1564.5362548828125 Test Loss: 1848.68701171875\n",
            "Epoch: 4761  Training Loss: 1715.43505859 Validation Loss: 1564.5399169921875 Test Loss: 1848.59033203125\n",
            "Epoch: 4766  Training Loss: 1715.32934570 Validation Loss: 1564.5433349609375 Test Loss: 1848.4937744140625\n",
            "Epoch: 4771  Training Loss: 1715.22375488 Validation Loss: 1564.547119140625 Test Loss: 1848.3970947265625\n",
            "Epoch: 4776  Training Loss: 1715.11828613 Validation Loss: 1564.5504150390625 Test Loss: 1848.3004150390625\n",
            "Epoch: 4781  Training Loss: 1715.01257324 Validation Loss: 1564.5538330078125 Test Loss: 1848.2037353515625\n",
            "Epoch: 4786  Training Loss: 1714.90698242 Validation Loss: 1564.557373046875 Test Loss: 1848.1070556640625\n",
            "Epoch: 4791  Training Loss: 1714.80151367 Validation Loss: 1564.560791015625 Test Loss: 1848.010498046875\n",
            "Epoch: 4796  Training Loss: 1714.69592285 Validation Loss: 1564.564208984375 Test Loss: 1847.913818359375\n",
            "Epoch: 4801  Training Loss: 1714.59020996 Validation Loss: 1564.5675048828125 Test Loss: 1847.81689453125\n",
            "Epoch: 4806  Training Loss: 1714.48474121 Validation Loss: 1564.5709228515625 Test Loss: 1847.720458984375\n",
            "Epoch: 4811  Training Loss: 1714.37915039 Validation Loss: 1564.5740966796875 Test Loss: 1847.6236572265625\n",
            "Epoch: 4816  Training Loss: 1714.27343750 Validation Loss: 1564.577392578125 Test Loss: 1847.5269775390625\n",
            "Epoch: 4821  Training Loss: 1714.16784668 Validation Loss: 1564.5806884765625 Test Loss: 1847.430419921875\n",
            "Epoch: 4826  Training Loss: 1714.06237793 Validation Loss: 1564.583984375 Test Loss: 1847.3333740234375\n",
            "Epoch: 4831  Training Loss: 1713.95666504 Validation Loss: 1564.587158203125 Test Loss: 1847.237060546875\n",
            "Epoch: 4836  Training Loss: 1713.85119629 Validation Loss: 1564.5904541015625 Test Loss: 1847.140380859375\n",
            "Epoch: 4841  Training Loss: 1713.74560547 Validation Loss: 1564.5933837890625 Test Loss: 1847.0433349609375\n",
            "Epoch: 4846  Training Loss: 1713.64013672 Validation Loss: 1564.5966796875 Test Loss: 1846.94677734375\n",
            "Epoch: 4851  Training Loss: 1713.53442383 Validation Loss: 1564.599853515625 Test Loss: 1846.8499755859375\n",
            "Epoch: 4856  Training Loss: 1713.42883301 Validation Loss: 1564.6029052734375 Test Loss: 1846.75341796875\n",
            "Epoch: 4861  Training Loss: 1713.32336426 Validation Loss: 1564.6058349609375 Test Loss: 1846.656494140625\n",
            "Epoch: 4866  Training Loss: 1713.21789551 Validation Loss: 1564.609130859375 Test Loss: 1846.559814453125\n",
            "Epoch: 4871  Training Loss: 1713.11218262 Validation Loss: 1564.6119384765625 Test Loss: 1846.463134765625\n",
            "Epoch: 4876  Training Loss: 1713.00671387 Validation Loss: 1564.6151123046875 Test Loss: 1846.3663330078125\n",
            "Epoch: 4881  Training Loss: 1712.90124512 Validation Loss: 1564.6177978515625 Test Loss: 1846.2694091796875\n",
            "Epoch: 4886  Training Loss: 1712.79565430 Validation Loss: 1564.6209716796875 Test Loss: 1846.1728515625\n",
            "Epoch: 4891  Training Loss: 1712.68994141 Validation Loss: 1564.6236572265625 Test Loss: 1846.076171875\n",
            "Epoch: 4896  Training Loss: 1712.58447266 Validation Loss: 1564.626708984375 Test Loss: 1845.9793701171875\n",
            "Epoch: 4901  Training Loss: 1712.47900391 Validation Loss: 1564.62939453125 Test Loss: 1845.882568359375\n",
            "Epoch: 4906  Training Loss: 1712.37353516 Validation Loss: 1564.63232421875 Test Loss: 1845.7857666015625\n",
            "Epoch: 4911  Training Loss: 1712.26806641 Validation Loss: 1564.6351318359375 Test Loss: 1845.68896484375\n",
            "Epoch: 4916  Training Loss: 1712.16259766 Validation Loss: 1564.6376953125 Test Loss: 1845.59228515625\n",
            "Epoch: 4921  Training Loss: 1712.05712891 Validation Loss: 1564.640380859375 Test Loss: 1845.49560546875\n",
            "Epoch: 4926  Training Loss: 1711.95166016 Validation Loss: 1564.6431884765625 Test Loss: 1845.3985595703125\n",
            "Epoch: 4931  Training Loss: 1711.84606934 Validation Loss: 1564.645751953125 Test Loss: 1845.3017578125\n",
            "Epoch: 4936  Training Loss: 1711.74060059 Validation Loss: 1564.6485595703125 Test Loss: 1845.205078125\n",
            "Epoch: 4941  Training Loss: 1711.63513184 Validation Loss: 1564.651123046875 Test Loss: 1845.108154296875\n",
            "Epoch: 4946  Training Loss: 1711.52990723 Validation Loss: 1564.6536865234375 Test Loss: 1845.011474609375\n",
            "Epoch: 4951  Training Loss: 1711.42431641 Validation Loss: 1564.65625 Test Loss: 1844.9146728515625\n",
            "Epoch: 4956  Training Loss: 1711.31884766 Validation Loss: 1564.6588134765625 Test Loss: 1844.8177490234375\n",
            "Epoch: 4961  Training Loss: 1711.21362305 Validation Loss: 1564.6612548828125 Test Loss: 1844.7210693359375\n",
            "Epoch: 4966  Training Loss: 1711.10791016 Validation Loss: 1564.6639404296875 Test Loss: 1844.6241455078125\n",
            "Epoch: 4971  Training Loss: 1711.00268555 Validation Loss: 1564.6663818359375 Test Loss: 1844.5274658203125\n",
            "Epoch: 4976  Training Loss: 1710.89709473 Validation Loss: 1564.6685791015625 Test Loss: 1844.4305419921875\n",
            "Epoch: 4981  Training Loss: 1710.79162598 Validation Loss: 1564.6708984375 Test Loss: 1844.3338623046875\n",
            "Epoch: 4986  Training Loss: 1710.68652344 Validation Loss: 1564.67333984375 Test Loss: 1844.237060546875\n",
            "Epoch: 4991  Training Loss: 1710.58105469 Validation Loss: 1564.67578125 Test Loss: 1844.14013671875\n",
            "Epoch: 4996  Training Loss: 1710.47570801 Validation Loss: 1564.677734375 Test Loss: 1844.0433349609375\n",
            "Epoch: 5001  Training Loss: 1710.37023926 Validation Loss: 1564.6802978515625 Test Loss: 1843.9466552734375\n",
            "Epoch: 5006  Training Loss: 1710.26513672 Validation Loss: 1564.682373046875 Test Loss: 1843.849609375\n",
            "Epoch: 5011  Training Loss: 1710.15942383 Validation Loss: 1564.6845703125 Test Loss: 1843.7529296875\n",
            "Epoch: 5016  Training Loss: 1710.05432129 Validation Loss: 1564.686767578125 Test Loss: 1843.656005859375\n",
            "Epoch: 5021  Training Loss: 1709.94909668 Validation Loss: 1564.68896484375 Test Loss: 1843.5592041015625\n",
            "Epoch: 5026  Training Loss: 1709.84387207 Validation Loss: 1564.6910400390625 Test Loss: 1843.4622802734375\n",
            "Epoch: 5031  Training Loss: 1709.73840332 Validation Loss: 1564.6929931640625 Test Loss: 1843.3656005859375\n",
            "Epoch: 5036  Training Loss: 1709.63305664 Validation Loss: 1564.695068359375 Test Loss: 1843.2685546875\n",
            "Epoch: 5041  Training Loss: 1709.52783203 Validation Loss: 1564.697021484375 Test Loss: 1843.171875\n",
            "Epoch: 5046  Training Loss: 1709.42236328 Validation Loss: 1564.6988525390625 Test Loss: 1843.0748291015625\n",
            "Epoch: 5051  Training Loss: 1709.31713867 Validation Loss: 1564.700927734375 Test Loss: 1842.97802734375\n",
            "Epoch: 5056  Training Loss: 1709.21191406 Validation Loss: 1564.702880859375 Test Loss: 1842.88134765625\n",
            "Epoch: 5061  Training Loss: 1709.10656738 Validation Loss: 1564.704833984375 Test Loss: 1842.7845458984375\n",
            "Epoch: 5066  Training Loss: 1709.00146484 Validation Loss: 1564.7064208984375 Test Loss: 1842.6876220703125\n",
            "Epoch: 5071  Training Loss: 1708.89611816 Validation Loss: 1564.708251953125 Test Loss: 1842.5909423828125\n",
            "Epoch: 5076  Training Loss: 1708.79101562 Validation Loss: 1564.7100830078125 Test Loss: 1842.493896484375\n",
            "Epoch: 5081  Training Loss: 1708.68591309 Validation Loss: 1564.7119140625 Test Loss: 1842.397216796875\n",
            "Epoch: 5086  Training Loss: 1708.58056641 Validation Loss: 1564.713623046875 Test Loss: 1842.3004150390625\n",
            "Epoch: 5091  Training Loss: 1708.47534180 Validation Loss: 1564.7152099609375 Test Loss: 1842.203369140625\n",
            "Epoch: 5096  Training Loss: 1708.37011719 Validation Loss: 1564.7166748046875 Test Loss: 1842.106689453125\n",
            "Epoch: 5101  Training Loss: 1708.26525879 Validation Loss: 1564.718505859375 Test Loss: 1842.009765625\n",
            "Epoch: 5106  Training Loss: 1708.15991211 Validation Loss: 1564.7198486328125 Test Loss: 1841.9130859375\n",
            "Epoch: 5111  Training Loss: 1708.05480957 Validation Loss: 1564.7215576171875 Test Loss: 1841.8160400390625\n",
            "Epoch: 5116  Training Loss: 1707.94982910 Validation Loss: 1564.7230224609375 Test Loss: 1841.7193603515625\n",
            "Epoch: 5121  Training Loss: 1707.84448242 Validation Loss: 1564.7244873046875 Test Loss: 1841.6224365234375\n",
            "Epoch: 5126  Training Loss: 1707.73950195 Validation Loss: 1564.7259521484375 Test Loss: 1841.525634765625\n",
            "Epoch: 5131  Training Loss: 1707.63452148 Validation Loss: 1564.7271728515625 Test Loss: 1841.4288330078125\n",
            "Epoch: 5136  Training Loss: 1707.52917480 Validation Loss: 1564.728515625 Test Loss: 1841.3319091796875\n",
            "Epoch: 5141  Training Loss: 1707.42419434 Validation Loss: 1564.72998046875 Test Loss: 1841.2352294921875\n",
            "Epoch: 5146  Training Loss: 1707.31921387 Validation Loss: 1564.7310791015625 Test Loss: 1841.138427734375\n",
            "Epoch: 5151  Training Loss: 1707.21423340 Validation Loss: 1564.7322998046875 Test Loss: 1841.0416259765625\n",
            "Epoch: 5156  Training Loss: 1707.10925293 Validation Loss: 1564.7333984375 Test Loss: 1840.9447021484375\n",
            "Epoch: 5161  Training Loss: 1707.00427246 Validation Loss: 1564.7347412109375 Test Loss: 1840.847900390625\n",
            "Epoch: 5166  Training Loss: 1706.89916992 Validation Loss: 1564.73583984375 Test Loss: 1840.7508544921875\n",
            "Epoch: 5171  Training Loss: 1706.79406738 Validation Loss: 1564.7369384765625 Test Loss: 1840.654052734375\n",
            "Epoch: 5176  Training Loss: 1706.68908691 Validation Loss: 1564.7379150390625 Test Loss: 1840.5572509765625\n",
            "Epoch: 5181  Training Loss: 1706.58435059 Validation Loss: 1564.7391357421875 Test Loss: 1840.46044921875\n",
            "Epoch: 5186  Training Loss: 1706.47937012 Validation Loss: 1564.7398681640625 Test Loss: 1840.3636474609375\n",
            "Epoch: 5191  Training Loss: 1706.37438965 Validation Loss: 1564.7408447265625 Test Loss: 1840.266845703125\n",
            "Epoch: 5196  Training Loss: 1706.26940918 Validation Loss: 1564.741943359375 Test Loss: 1840.170166015625\n",
            "Epoch: 5201  Training Loss: 1706.16455078 Validation Loss: 1564.74267578125 Test Loss: 1840.0731201171875\n",
            "Epoch: 5206  Training Loss: 1706.05969238 Validation Loss: 1564.7435302734375 Test Loss: 1839.976318359375\n",
            "Epoch: 5211  Training Loss: 1705.95483398 Validation Loss: 1564.7442626953125 Test Loss: 1839.87939453125\n",
            "Epoch: 5216  Training Loss: 1705.84985352 Validation Loss: 1564.7449951171875 Test Loss: 1839.7828369140625\n",
            "Epoch: 5221  Training Loss: 1705.74487305 Validation Loss: 1564.74560546875 Test Loss: 1839.6859130859375\n",
            "Epoch: 5226  Training Loss: 1705.64013672 Validation Loss: 1564.746337890625 Test Loss: 1839.5892333984375\n",
            "Epoch: 5231  Training Loss: 1705.53527832 Validation Loss: 1564.746826171875 Test Loss: 1839.49267578125\n",
            "Epoch: 5236  Training Loss: 1705.43066406 Validation Loss: 1564.7474365234375 Test Loss: 1839.3953857421875\n",
            "Epoch: 5241  Training Loss: 1705.32568359 Validation Loss: 1564.7481689453125 Test Loss: 1839.2989501953125\n",
            "Epoch: 5246  Training Loss: 1705.22106934 Validation Loss: 1564.7486572265625 Test Loss: 1839.2017822265625\n",
            "Epoch: 5251  Training Loss: 1705.11633301 Validation Loss: 1564.749267578125 Test Loss: 1839.1051025390625\n",
            "Epoch: 5256  Training Loss: 1705.01147461 Validation Loss: 1564.7496337890625 Test Loss: 1839.0084228515625\n",
            "Epoch: 5261  Training Loss: 1704.90661621 Validation Loss: 1564.75 Test Loss: 1838.911376953125\n",
            "Epoch: 5266  Training Loss: 1704.80212402 Validation Loss: 1564.7503662109375 Test Loss: 1838.8148193359375\n",
            "Epoch: 5271  Training Loss: 1704.69738770 Validation Loss: 1564.750732421875 Test Loss: 1838.7178955078125\n",
            "Epoch: 5276  Training Loss: 1704.59265137 Validation Loss: 1564.7510986328125 Test Loss: 1838.6212158203125\n",
            "Epoch: 5281  Training Loss: 1704.48803711 Validation Loss: 1564.7510986328125 Test Loss: 1838.5244140625\n",
            "Epoch: 5286  Training Loss: 1704.38342285 Validation Loss: 1564.75146484375 Test Loss: 1838.427734375\n",
            "Epoch: 5291  Training Loss: 1704.27856445 Validation Loss: 1564.75146484375 Test Loss: 1838.330810546875\n",
            "Epoch: 5296  Training Loss: 1704.17419434 Validation Loss: 1564.751708984375 Test Loss: 1838.234130859375\n",
            "Epoch: 5301  Training Loss: 1704.06958008 Validation Loss: 1564.751708984375 Test Loss: 1838.13720703125\n",
            "Epoch: 5306  Training Loss: 1703.96484375 Validation Loss: 1564.7518310546875 Test Loss: 1838.0406494140625\n",
            "Epoch: 5311  Training Loss: 1703.86035156 Validation Loss: 1564.7518310546875 Test Loss: 1837.94384765625\n",
            "Epoch: 5316  Training Loss: 1703.75573730 Validation Loss: 1564.751708984375 Test Loss: 1837.8470458984375\n",
            "Epoch: 5321  Training Loss: 1703.65124512 Validation Loss: 1564.75146484375 Test Loss: 1837.7501220703125\n",
            "Epoch: 5326  Training Loss: 1703.54663086 Validation Loss: 1564.7513427734375 Test Loss: 1837.653564453125\n",
            "Epoch: 5331  Training Loss: 1703.44226074 Validation Loss: 1564.7513427734375 Test Loss: 1837.556884765625\n",
            "Epoch: 5336  Training Loss: 1703.33789062 Validation Loss: 1564.7510986328125 Test Loss: 1837.4599609375\n",
            "Epoch: 5341  Training Loss: 1703.23327637 Validation Loss: 1564.750732421875 Test Loss: 1837.3634033203125\n",
            "Epoch: 5346  Training Loss: 1703.12890625 Validation Loss: 1564.7503662109375 Test Loss: 1837.2664794921875\n",
            "Epoch: 5351  Training Loss: 1703.02465820 Validation Loss: 1564.750244140625 Test Loss: 1837.1697998046875\n",
            "Epoch: 5356  Training Loss: 1702.91992188 Validation Loss: 1564.7496337890625 Test Loss: 1837.0731201171875\n",
            "Epoch: 5361  Training Loss: 1702.81567383 Validation Loss: 1564.749267578125 Test Loss: 1836.9764404296875\n",
            "Epoch: 5366  Training Loss: 1702.71130371 Validation Loss: 1564.74853515625 Test Loss: 1836.8797607421875\n",
            "Epoch: 5371  Training Loss: 1702.60693359 Validation Loss: 1564.7481689453125 Test Loss: 1836.782958984375\n",
            "Epoch: 5376  Training Loss: 1702.50268555 Validation Loss: 1564.74755859375 Test Loss: 1836.6861572265625\n",
            "Epoch: 5381  Training Loss: 1702.39831543 Validation Loss: 1564.746826171875 Test Loss: 1836.5894775390625\n",
            "Epoch: 5386  Training Loss: 1702.29394531 Validation Loss: 1564.74609375 Test Loss: 1836.4927978515625\n",
            "Epoch: 5391  Training Loss: 1702.18957520 Validation Loss: 1564.74560546875 Test Loss: 1836.3961181640625\n",
            "Epoch: 5396  Training Loss: 1702.08544922 Validation Loss: 1564.744873046875 Test Loss: 1836.299560546875\n",
            "Epoch: 5401  Training Loss: 1701.98132324 Validation Loss: 1564.743896484375 Test Loss: 1836.202880859375\n",
            "Epoch: 5406  Training Loss: 1701.87695312 Validation Loss: 1564.7431640625 Test Loss: 1836.1060791015625\n",
            "Epoch: 5411  Training Loss: 1701.77282715 Validation Loss: 1564.7423095703125 Test Loss: 1836.0093994140625\n",
            "Epoch: 5416  Training Loss: 1701.66857910 Validation Loss: 1564.7412109375 Test Loss: 1835.9127197265625\n",
            "Epoch: 5421  Training Loss: 1701.56445312 Validation Loss: 1564.740478515625 Test Loss: 1835.8160400390625\n",
            "Epoch: 5426  Training Loss: 1701.46032715 Validation Loss: 1564.7393798828125 Test Loss: 1835.7196044921875\n",
            "Epoch: 5431  Training Loss: 1701.35607910 Validation Loss: 1564.73828125 Test Loss: 1835.622802734375\n",
            "Epoch: 5436  Training Loss: 1701.25195312 Validation Loss: 1564.7371826171875 Test Loss: 1835.5262451171875\n",
            "Epoch: 5441  Training Loss: 1701.14794922 Validation Loss: 1564.73583984375 Test Loss: 1835.429443359375\n",
            "Epoch: 5446  Training Loss: 1701.04382324 Validation Loss: 1564.7349853515625 Test Loss: 1835.3328857421875\n",
            "Epoch: 5451  Training Loss: 1700.93957520 Validation Loss: 1564.7332763671875 Test Loss: 1835.236083984375\n",
            "Epoch: 5456  Training Loss: 1700.83581543 Validation Loss: 1564.73193359375 Test Loss: 1835.1397705078125\n",
            "Epoch: 5461  Training Loss: 1700.73156738 Validation Loss: 1564.730712890625 Test Loss: 1835.0430908203125\n",
            "Epoch: 5466  Training Loss: 1700.62780762 Validation Loss: 1564.7293701171875 Test Loss: 1834.9462890625\n",
            "Epoch: 5471  Training Loss: 1700.52392578 Validation Loss: 1564.7279052734375 Test Loss: 1834.849609375\n",
            "Epoch: 5476  Training Loss: 1700.41979980 Validation Loss: 1564.7264404296875 Test Loss: 1834.7530517578125\n",
            "Epoch: 5481  Training Loss: 1700.31579590 Validation Loss: 1564.7249755859375 Test Loss: 1834.656494140625\n",
            "Epoch: 5486  Training Loss: 1700.21191406 Validation Loss: 1564.72314453125 Test Loss: 1834.559814453125\n",
            "Epoch: 5491  Training Loss: 1700.10791016 Validation Loss: 1564.7215576171875 Test Loss: 1834.4635009765625\n",
            "Epoch: 5496  Training Loss: 1700.00415039 Validation Loss: 1564.7198486328125 Test Loss: 1834.366943359375\n",
            "Epoch: 5501  Training Loss: 1699.90026855 Validation Loss: 1564.718017578125 Test Loss: 1834.2701416015625\n",
            "Epoch: 5506  Training Loss: 1699.79638672 Validation Loss: 1564.71630859375 Test Loss: 1834.173583984375\n",
            "Epoch: 5511  Training Loss: 1699.69262695 Validation Loss: 1564.7144775390625 Test Loss: 1834.076904296875\n",
            "Epoch: 5516  Training Loss: 1699.58874512 Validation Loss: 1564.712646484375 Test Loss: 1833.9805908203125\n",
            "Epoch: 5521  Training Loss: 1699.48522949 Validation Loss: 1564.710693359375 Test Loss: 1833.8837890625\n",
            "Epoch: 5526  Training Loss: 1699.38134766 Validation Loss: 1564.7086181640625 Test Loss: 1833.7874755859375\n",
            "Epoch: 5531  Training Loss: 1699.27770996 Validation Loss: 1564.7066650390625 Test Loss: 1833.69091796875\n",
            "Epoch: 5536  Training Loss: 1699.17370605 Validation Loss: 1564.70458984375 Test Loss: 1833.5943603515625\n",
            "Epoch: 5541  Training Loss: 1699.07043457 Validation Loss: 1564.702392578125 Test Loss: 1833.4979248046875\n",
            "Epoch: 5546  Training Loss: 1698.96655273 Validation Loss: 1564.7003173828125 Test Loss: 1833.4013671875\n",
            "Epoch: 5551  Training Loss: 1698.86291504 Validation Loss: 1564.6981201171875 Test Loss: 1833.304931640625\n",
            "Epoch: 5556  Training Loss: 1698.75915527 Validation Loss: 1564.69580078125 Test Loss: 1833.2081298828125\n",
            "Epoch: 5561  Training Loss: 1698.65563965 Validation Loss: 1564.693359375 Test Loss: 1833.1119384765625\n",
            "Epoch: 5566  Training Loss: 1698.55200195 Validation Loss: 1564.6910400390625 Test Loss: 1833.015380859375\n",
            "Epoch: 5571  Training Loss: 1698.44848633 Validation Loss: 1564.6885986328125 Test Loss: 1832.9189453125\n",
            "Epoch: 5576  Training Loss: 1698.34497070 Validation Loss: 1564.68603515625 Test Loss: 1832.822509765625\n",
            "Epoch: 5581  Training Loss: 1698.24145508 Validation Loss: 1564.68359375 Test Loss: 1832.7259521484375\n",
            "Epoch: 5586  Training Loss: 1698.13818359 Validation Loss: 1564.6810302734375 Test Loss: 1832.62939453125\n",
            "Epoch: 5591  Training Loss: 1698.03442383 Validation Loss: 1564.6783447265625 Test Loss: 1832.533203125\n",
            "Epoch: 5596  Training Loss: 1697.93090820 Validation Loss: 1564.67578125 Test Loss: 1832.4366455078125\n",
            "Epoch: 5601  Training Loss: 1697.82763672 Validation Loss: 1564.6728515625 Test Loss: 1832.3402099609375\n",
            "Epoch: 5606  Training Loss: 1697.72424316 Validation Loss: 1564.6700439453125 Test Loss: 1832.2437744140625\n",
            "Epoch: 5611  Training Loss: 1697.62072754 Validation Loss: 1564.667236328125 Test Loss: 1832.147216796875\n",
            "Epoch: 5616  Training Loss: 1697.51757812 Validation Loss: 1564.664306640625 Test Loss: 1832.051025390625\n",
            "Epoch: 5621  Training Loss: 1697.41418457 Validation Loss: 1564.661376953125 Test Loss: 1831.95458984375\n",
            "Epoch: 5626  Training Loss: 1697.31066895 Validation Loss: 1564.6583251953125 Test Loss: 1831.858154296875\n",
            "Epoch: 5631  Training Loss: 1697.20764160 Validation Loss: 1564.6551513671875 Test Loss: 1831.7618408203125\n",
            "Epoch: 5636  Training Loss: 1697.10412598 Validation Loss: 1564.6522216796875 Test Loss: 1831.6654052734375\n",
            "Epoch: 5641  Training Loss: 1697.00097656 Validation Loss: 1564.64892578125 Test Loss: 1831.569091796875\n",
            "Epoch: 5646  Training Loss: 1696.89782715 Validation Loss: 1564.645751953125 Test Loss: 1831.4727783203125\n",
            "Epoch: 5651  Training Loss: 1696.79467773 Validation Loss: 1564.6424560546875 Test Loss: 1831.3763427734375\n",
            "Epoch: 5656  Training Loss: 1696.69128418 Validation Loss: 1564.638916015625 Test Loss: 1831.280029296875\n",
            "Epoch: 5661  Training Loss: 1696.58825684 Validation Loss: 1564.6356201171875 Test Loss: 1831.1834716796875\n",
            "Epoch: 5666  Training Loss: 1696.48522949 Validation Loss: 1564.63232421875 Test Loss: 1831.0872802734375\n",
            "Epoch: 5671  Training Loss: 1696.38208008 Validation Loss: 1564.62890625 Test Loss: 1830.9908447265625\n",
            "Epoch: 5676  Training Loss: 1696.27905273 Validation Loss: 1564.6253662109375 Test Loss: 1830.8946533203125\n",
            "Epoch: 5681  Training Loss: 1696.17602539 Validation Loss: 1564.6214599609375 Test Loss: 1830.79833984375\n",
            "Epoch: 5686  Training Loss: 1696.07299805 Validation Loss: 1564.6180419921875 Test Loss: 1830.701904296875\n",
            "Epoch: 5691  Training Loss: 1695.96997070 Validation Loss: 1564.6143798828125 Test Loss: 1830.6058349609375\n",
            "Epoch: 5696  Training Loss: 1695.86694336 Validation Loss: 1564.6104736328125 Test Loss: 1830.5093994140625\n",
            "Epoch: 5701  Training Loss: 1695.76391602 Validation Loss: 1564.6065673828125 Test Loss: 1830.4132080078125\n",
            "Epoch: 5706  Training Loss: 1695.66088867 Validation Loss: 1564.6029052734375 Test Loss: 1830.3167724609375\n",
            "Epoch: 5711  Training Loss: 1695.55810547 Validation Loss: 1564.598876953125 Test Loss: 1830.220703125\n",
            "Epoch: 5716  Training Loss: 1695.45520020 Validation Loss: 1564.5948486328125 Test Loss: 1830.124267578125\n",
            "Epoch: 5721  Training Loss: 1695.35217285 Validation Loss: 1564.5908203125 Test Loss: 1830.0281982421875\n",
            "Epoch: 5726  Training Loss: 1695.24951172 Validation Loss: 1564.5867919921875 Test Loss: 1829.931884765625\n",
            "Epoch: 5731  Training Loss: 1695.14660645 Validation Loss: 1564.58251953125 Test Loss: 1829.8358154296875\n",
            "Epoch: 5736  Training Loss: 1695.04394531 Validation Loss: 1564.5784912109375 Test Loss: 1829.739501953125\n",
            "Epoch: 5741  Training Loss: 1694.94116211 Validation Loss: 1564.5738525390625 Test Loss: 1829.643310546875\n",
            "Epoch: 5746  Training Loss: 1694.83825684 Validation Loss: 1564.5697021484375 Test Loss: 1829.547119140625\n",
            "Epoch: 5751  Training Loss: 1694.73571777 Validation Loss: 1564.5654296875 Test Loss: 1829.4510498046875\n",
            "Epoch: 5756  Training Loss: 1694.63305664 Validation Loss: 1564.560791015625 Test Loss: 1829.35498046875\n",
            "Epoch: 5761  Training Loss: 1694.53039551 Validation Loss: 1564.556396484375 Test Loss: 1829.2586669921875\n",
            "Epoch: 5766  Training Loss: 1694.42761230 Validation Loss: 1564.5518798828125 Test Loss: 1829.1624755859375\n",
            "Epoch: 5771  Training Loss: 1694.32495117 Validation Loss: 1564.5474853515625 Test Loss: 1829.0665283203125\n",
            "Epoch: 5776  Training Loss: 1694.22241211 Validation Loss: 1564.54248046875 Test Loss: 1828.97021484375\n",
            "Epoch: 5781  Training Loss: 1694.11987305 Validation Loss: 1564.5377197265625 Test Loss: 1828.8741455078125\n",
            "Epoch: 5786  Training Loss: 1694.01733398 Validation Loss: 1564.5330810546875 Test Loss: 1828.7779541015625\n",
            "Epoch: 5791  Training Loss: 1693.91491699 Validation Loss: 1564.528076171875 Test Loss: 1828.681884765625\n",
            "Epoch: 5796  Training Loss: 1693.81250000 Validation Loss: 1564.523193359375 Test Loss: 1828.5858154296875\n",
            "Epoch: 5801  Training Loss: 1693.70983887 Validation Loss: 1564.5184326171875 Test Loss: 1828.4896240234375\n",
            "Epoch: 5806  Training Loss: 1693.60742188 Validation Loss: 1564.51318359375 Test Loss: 1828.3935546875\n",
            "Epoch: 5811  Training Loss: 1693.50488281 Validation Loss: 1564.50830078125 Test Loss: 1828.297607421875\n",
            "Epoch: 5816  Training Loss: 1693.40270996 Validation Loss: 1564.503173828125 Test Loss: 1828.2015380859375\n",
            "Epoch: 5821  Training Loss: 1693.30041504 Validation Loss: 1564.497802734375 Test Loss: 1828.105224609375\n",
            "Epoch: 5826  Training Loss: 1693.19799805 Validation Loss: 1564.49267578125 Test Loss: 1828.0093994140625\n",
            "Epoch: 5831  Training Loss: 1693.09558105 Validation Loss: 1564.4873046875 Test Loss: 1827.913330078125\n",
            "Epoch: 5836  Training Loss: 1692.99328613 Validation Loss: 1564.4818115234375 Test Loss: 1827.8172607421875\n",
            "Epoch: 5841  Training Loss: 1692.89111328 Validation Loss: 1564.4764404296875 Test Loss: 1827.721435546875\n",
            "Epoch: 5846  Training Loss: 1692.78881836 Validation Loss: 1564.47119140625 Test Loss: 1827.62548828125\n",
            "Epoch: 5851  Training Loss: 1692.68664551 Validation Loss: 1564.4654541015625 Test Loss: 1827.529296875\n",
            "Epoch: 5856  Training Loss: 1692.58447266 Validation Loss: 1564.4599609375 Test Loss: 1827.433349609375\n",
            "Epoch: 5861  Training Loss: 1692.48229980 Validation Loss: 1564.4542236328125 Test Loss: 1827.3375244140625\n",
            "Epoch: 5866  Training Loss: 1692.38024902 Validation Loss: 1564.4483642578125 Test Loss: 1827.2413330078125\n",
            "Epoch: 5871  Training Loss: 1692.27819824 Validation Loss: 1564.442626953125 Test Loss: 1827.1453857421875\n",
            "Epoch: 5876  Training Loss: 1692.17602539 Validation Loss: 1564.4368896484375 Test Loss: 1827.0494384765625\n",
            "Epoch: 5881  Training Loss: 1692.07397461 Validation Loss: 1564.4306640625 Test Loss: 1826.95361328125\n",
            "Epoch: 5886  Training Loss: 1691.97192383 Validation Loss: 1564.4248046875 Test Loss: 1826.8577880859375\n",
            "Epoch: 5891  Training Loss: 1691.86987305 Validation Loss: 1564.418701171875 Test Loss: 1826.7618408203125\n",
            "Epoch: 5896  Training Loss: 1691.76818848 Validation Loss: 1564.4127197265625 Test Loss: 1826.665771484375\n",
            "Epoch: 5901  Training Loss: 1691.66613770 Validation Loss: 1564.4066162109375 Test Loss: 1826.5701904296875\n",
            "Epoch: 5906  Training Loss: 1691.56408691 Validation Loss: 1564.400146484375 Test Loss: 1826.4739990234375\n",
            "Epoch: 5911  Training Loss: 1691.46228027 Validation Loss: 1564.3941650390625 Test Loss: 1826.3782958984375\n",
            "Epoch: 5916  Training Loss: 1691.36035156 Validation Loss: 1564.3876953125 Test Loss: 1826.282470703125\n",
            "Epoch: 5921  Training Loss: 1691.25866699 Validation Loss: 1564.3812255859375 Test Loss: 1826.1866455078125\n",
            "Epoch: 5926  Training Loss: 1691.15686035 Validation Loss: 1564.374755859375 Test Loss: 1826.0908203125\n",
            "Epoch: 5931  Training Loss: 1691.05517578 Validation Loss: 1564.368408203125 Test Loss: 1825.9951171875\n",
            "Epoch: 5936  Training Loss: 1690.95324707 Validation Loss: 1564.36181640625 Test Loss: 1825.899169921875\n",
            "Epoch: 5941  Training Loss: 1690.85156250 Validation Loss: 1564.35498046875 Test Loss: 1825.803466796875\n",
            "Epoch: 5946  Training Loss: 1690.74963379 Validation Loss: 1564.348388671875 Test Loss: 1825.70751953125\n",
            "Epoch: 5951  Training Loss: 1690.64831543 Validation Loss: 1564.341552734375 Test Loss: 1825.6119384765625\n",
            "Epoch: 5956  Training Loss: 1690.54663086 Validation Loss: 1564.334716796875 Test Loss: 1825.515869140625\n",
            "Epoch: 5961  Training Loss: 1690.44482422 Validation Loss: 1564.3277587890625 Test Loss: 1825.42041015625\n",
            "Epoch: 5966  Training Loss: 1690.34338379 Validation Loss: 1564.32080078125 Test Loss: 1825.3245849609375\n",
            "Epoch: 5971  Training Loss: 1690.24182129 Validation Loss: 1564.3138427734375 Test Loss: 1825.228759765625\n",
            "Epoch: 5976  Training Loss: 1690.14025879 Validation Loss: 1564.306640625 Test Loss: 1825.13330078125\n",
            "Epoch: 5981  Training Loss: 1690.03881836 Validation Loss: 1564.2994384765625 Test Loss: 1825.037353515625\n",
            "Epoch: 5986  Training Loss: 1689.93737793 Validation Loss: 1564.292236328125 Test Loss: 1824.9417724609375\n",
            "Epoch: 5991  Training Loss: 1689.83581543 Validation Loss: 1564.284912109375 Test Loss: 1824.8460693359375\n",
            "Epoch: 5996  Training Loss: 1689.73437500 Validation Loss: 1564.277587890625 Test Loss: 1824.750244140625\n",
            "Epoch: 6001  Training Loss: 1689.63305664 Validation Loss: 1564.270263671875 Test Loss: 1824.65478515625\n",
            "Epoch: 6006  Training Loss: 1689.53161621 Validation Loss: 1564.2626953125 Test Loss: 1824.5589599609375\n",
            "Epoch: 6011  Training Loss: 1689.43041992 Validation Loss: 1564.255126953125 Test Loss: 1824.4632568359375\n",
            "Epoch: 6016  Training Loss: 1689.32910156 Validation Loss: 1564.2474365234375 Test Loss: 1824.367919921875\n",
            "Epoch: 6021  Training Loss: 1689.22790527 Validation Loss: 1564.2398681640625 Test Loss: 1824.2720947265625\n",
            "Epoch: 6026  Training Loss: 1689.12646484 Validation Loss: 1564.23193359375 Test Loss: 1824.1766357421875\n",
            "Epoch: 6031  Training Loss: 1689.02526855 Validation Loss: 1564.2242431640625 Test Loss: 1824.0810546875\n",
            "Epoch: 6036  Training Loss: 1688.92419434 Validation Loss: 1564.21630859375 Test Loss: 1823.9853515625\n",
            "Epoch: 6041  Training Loss: 1688.82299805 Validation Loss: 1564.20849609375 Test Loss: 1823.889892578125\n",
            "Epoch: 6046  Training Loss: 1688.72180176 Validation Loss: 1564.2003173828125 Test Loss: 1823.794189453125\n",
            "Epoch: 6051  Training Loss: 1688.62072754 Validation Loss: 1564.1925048828125 Test Loss: 1823.698974609375\n",
            "Epoch: 6056  Training Loss: 1688.51965332 Validation Loss: 1564.1842041015625 Test Loss: 1823.6031494140625\n",
            "Epoch: 6061  Training Loss: 1688.41857910 Validation Loss: 1564.1761474609375 Test Loss: 1823.5076904296875\n",
            "Epoch: 6066  Training Loss: 1688.31738281 Validation Loss: 1564.1678466796875 Test Loss: 1823.412353515625\n",
            "Epoch: 6071  Training Loss: 1688.21667480 Validation Loss: 1564.1595458984375 Test Loss: 1823.3167724609375\n",
            "Epoch: 6076  Training Loss: 1688.11560059 Validation Loss: 1564.151123046875 Test Loss: 1823.2213134765625\n",
            "Epoch: 6081  Training Loss: 1688.01477051 Validation Loss: 1564.1427001953125 Test Loss: 1823.125732421875\n",
            "Epoch: 6086  Training Loss: 1687.91381836 Validation Loss: 1564.1341552734375 Test Loss: 1823.0303955078125\n",
            "Epoch: 6091  Training Loss: 1687.81286621 Validation Loss: 1564.1256103515625 Test Loss: 1822.934814453125\n",
            "Epoch: 6096  Training Loss: 1687.71228027 Validation Loss: 1564.116943359375 Test Loss: 1822.83935546875\n",
            "Epoch: 6101  Training Loss: 1687.61132812 Validation Loss: 1564.1080322265625 Test Loss: 1822.744140625\n",
            "Epoch: 6106  Training Loss: 1687.51049805 Validation Loss: 1564.0992431640625 Test Loss: 1822.6485595703125\n",
            "Epoch: 6111  Training Loss: 1687.40979004 Validation Loss: 1564.0902099609375 Test Loss: 1822.55322265625\n",
            "Epoch: 6116  Training Loss: 1687.30908203 Validation Loss: 1564.0814208984375 Test Loss: 1822.4578857421875\n",
            "Epoch: 6121  Training Loss: 1687.20837402 Validation Loss: 1564.0726318359375 Test Loss: 1822.3624267578125\n",
            "Epoch: 6126  Training Loss: 1687.10766602 Validation Loss: 1564.0634765625 Test Loss: 1822.26708984375\n",
            "Epoch: 6131  Training Loss: 1687.00708008 Validation Loss: 1564.054443359375 Test Loss: 1822.171630859375\n",
            "Epoch: 6136  Training Loss: 1686.90649414 Validation Loss: 1564.045166015625 Test Loss: 1822.0765380859375\n",
            "Epoch: 6141  Training Loss: 1686.80578613 Validation Loss: 1564.035888671875 Test Loss: 1821.98095703125\n",
            "Epoch: 6146  Training Loss: 1686.70520020 Validation Loss: 1564.0264892578125 Test Loss: 1821.8857421875\n",
            "Epoch: 6151  Training Loss: 1686.60461426 Validation Loss: 1564.01708984375 Test Loss: 1821.79052734375\n",
            "Epoch: 6156  Training Loss: 1686.50427246 Validation Loss: 1564.0076904296875 Test Loss: 1821.6953125\n",
            "Epoch: 6161  Training Loss: 1686.40368652 Validation Loss: 1563.9981689453125 Test Loss: 1821.5999755859375\n",
            "Epoch: 6166  Training Loss: 1686.30334473 Validation Loss: 1563.9884033203125 Test Loss: 1821.5047607421875\n",
            "Epoch: 6171  Training Loss: 1686.20275879 Validation Loss: 1563.9788818359375 Test Loss: 1821.4095458984375\n",
            "Epoch: 6176  Training Loss: 1686.10253906 Validation Loss: 1563.9691162109375 Test Loss: 1821.314208984375\n",
            "Epoch: 6181  Training Loss: 1686.00231934 Validation Loss: 1563.9593505859375 Test Loss: 1821.218994140625\n",
            "Epoch: 6186  Training Loss: 1685.90185547 Validation Loss: 1563.949462890625 Test Loss: 1821.123779296875\n",
            "Epoch: 6191  Training Loss: 1685.80163574 Validation Loss: 1563.9393310546875 Test Loss: 1821.0286865234375\n",
            "Epoch: 6196  Training Loss: 1685.70141602 Validation Loss: 1563.929443359375 Test Loss: 1820.933349609375\n",
            "Epoch: 6201  Training Loss: 1685.60119629 Validation Loss: 1563.9193115234375 Test Loss: 1820.838134765625\n",
            "Epoch: 6206  Training Loss: 1685.50097656 Validation Loss: 1563.9091796875 Test Loss: 1820.7431640625\n",
            "Epoch: 6211  Training Loss: 1685.40087891 Validation Loss: 1563.89892578125 Test Loss: 1820.64794921875\n",
            "Epoch: 6216  Training Loss: 1685.30065918 Validation Loss: 1563.8887939453125 Test Loss: 1820.552978515625\n",
            "Epoch: 6221  Training Loss: 1685.20043945 Validation Loss: 1563.8782958984375 Test Loss: 1820.4576416015625\n",
            "Epoch: 6226  Training Loss: 1685.10058594 Validation Loss: 1563.8680419921875 Test Loss: 1820.3626708984375\n",
            "Epoch: 6231  Training Loss: 1685.00048828 Validation Loss: 1563.8572998046875 Test Loss: 1820.2677001953125\n",
            "Epoch: 6236  Training Loss: 1684.90039062 Validation Loss: 1563.8466796875 Test Loss: 1820.1724853515625\n",
            "Epoch: 6241  Training Loss: 1684.80041504 Validation Loss: 1563.836181640625 Test Loss: 1820.0775146484375\n",
            "Epoch: 6246  Training Loss: 1684.70043945 Validation Loss: 1563.8253173828125 Test Loss: 1819.982421875\n",
            "Epoch: 6251  Training Loss: 1684.60058594 Validation Loss: 1563.814697265625 Test Loss: 1819.8873291015625\n",
            "Epoch: 6256  Training Loss: 1684.50085449 Validation Loss: 1563.8037109375 Test Loss: 1819.7923583984375\n",
            "Epoch: 6261  Training Loss: 1684.40087891 Validation Loss: 1563.79296875 Test Loss: 1819.6973876953125\n",
            "Epoch: 6266  Training Loss: 1684.30102539 Validation Loss: 1563.781982421875 Test Loss: 1819.6024169921875\n",
            "Epoch: 6271  Training Loss: 1684.20129395 Validation Loss: 1563.770751953125 Test Loss: 1819.50732421875\n",
            "Epoch: 6276  Training Loss: 1684.10131836 Validation Loss: 1563.759765625 Test Loss: 1819.4127197265625\n",
            "Epoch: 6281  Training Loss: 1684.00158691 Validation Loss: 1563.74853515625 Test Loss: 1819.3175048828125\n",
            "Epoch: 6286  Training Loss: 1683.90209961 Validation Loss: 1563.7373046875 Test Loss: 1819.2225341796875\n",
            "Epoch: 6291  Training Loss: 1683.80212402 Validation Loss: 1563.7259521484375 Test Loss: 1819.127685546875\n",
            "Epoch: 6296  Training Loss: 1683.70275879 Validation Loss: 1563.7144775390625 Test Loss: 1819.03271484375\n",
            "Epoch: 6301  Training Loss: 1683.60302734 Validation Loss: 1563.702880859375 Test Loss: 1818.937744140625\n",
            "Epoch: 6306  Training Loss: 1683.50366211 Validation Loss: 1563.6915283203125 Test Loss: 1818.843017578125\n",
            "Epoch: 6311  Training Loss: 1683.40405273 Validation Loss: 1563.6798095703125 Test Loss: 1818.748291015625\n",
            "Epoch: 6316  Training Loss: 1683.30444336 Validation Loss: 1563.668212890625 Test Loss: 1818.6531982421875\n",
            "Epoch: 6321  Training Loss: 1683.20507812 Validation Loss: 1563.65625 Test Loss: 1818.558349609375\n",
            "Epoch: 6326  Training Loss: 1683.10546875 Validation Loss: 1563.64453125 Test Loss: 1818.463623046875\n",
            "Epoch: 6331  Training Loss: 1683.00622559 Validation Loss: 1563.632568359375 Test Loss: 1818.3687744140625\n",
            "Epoch: 6336  Training Loss: 1682.90661621 Validation Loss: 1563.62060546875 Test Loss: 1818.2740478515625\n",
            "Epoch: 6341  Training Loss: 1682.80737305 Validation Loss: 1563.608642578125 Test Loss: 1818.17919921875\n",
            "Epoch: 6346  Training Loss: 1682.70800781 Validation Loss: 1563.5965576171875 Test Loss: 1818.0843505859375\n",
            "Epoch: 6351  Training Loss: 1682.60876465 Validation Loss: 1563.5843505859375 Test Loss: 1817.9896240234375\n",
            "Epoch: 6356  Training Loss: 1682.50952148 Validation Loss: 1563.572021484375 Test Loss: 1817.8948974609375\n",
            "Epoch: 6361  Training Loss: 1682.41015625 Validation Loss: 1563.5596923828125 Test Loss: 1817.8001708984375\n",
            "Epoch: 6366  Training Loss: 1682.31103516 Validation Loss: 1563.5472412109375 Test Loss: 1817.705322265625\n",
            "Epoch: 6371  Training Loss: 1682.21179199 Validation Loss: 1563.5347900390625 Test Loss: 1817.6107177734375\n",
            "Epoch: 6376  Training Loss: 1682.11254883 Validation Loss: 1563.522216796875 Test Loss: 1817.5162353515625\n",
            "Epoch: 6381  Training Loss: 1682.01342773 Validation Loss: 1563.509765625 Test Loss: 1817.421630859375\n",
            "Epoch: 6386  Training Loss: 1681.91442871 Validation Loss: 1563.496826171875 Test Loss: 1817.3267822265625\n",
            "Epoch: 6391  Training Loss: 1681.81530762 Validation Loss: 1563.484130859375 Test Loss: 1817.2320556640625\n",
            "Epoch: 6396  Training Loss: 1681.71655273 Validation Loss: 1563.4713134765625 Test Loss: 1817.1375732421875\n",
            "Epoch: 6401  Training Loss: 1681.61743164 Validation Loss: 1563.4586181640625 Test Loss: 1817.042724609375\n",
            "Epoch: 6406  Training Loss: 1681.51855469 Validation Loss: 1563.4454345703125 Test Loss: 1816.9483642578125\n",
            "Epoch: 6411  Training Loss: 1681.41943359 Validation Loss: 1563.432373046875 Test Loss: 1816.853759765625\n",
            "Epoch: 6416  Training Loss: 1681.32067871 Validation Loss: 1563.4193115234375 Test Loss: 1816.7591552734375\n",
            "Epoch: 6421  Training Loss: 1681.22180176 Validation Loss: 1563.406005859375 Test Loss: 1816.66455078125\n",
            "Epoch: 6426  Training Loss: 1681.12292480 Validation Loss: 1563.3927001953125 Test Loss: 1816.5701904296875\n",
            "Epoch: 6431  Training Loss: 1681.02416992 Validation Loss: 1563.37939453125 Test Loss: 1816.4755859375\n",
            "Epoch: 6436  Training Loss: 1680.92553711 Validation Loss: 1563.365966796875 Test Loss: 1816.381103515625\n",
            "Epoch: 6441  Training Loss: 1680.82666016 Validation Loss: 1563.3524169921875 Test Loss: 1816.2864990234375\n",
            "Epoch: 6446  Training Loss: 1680.72790527 Validation Loss: 1563.3387451171875 Test Loss: 1816.1920166015625\n",
            "Epoch: 6451  Training Loss: 1680.62927246 Validation Loss: 1563.3251953125 Test Loss: 1816.0975341796875\n",
            "Epoch: 6456  Training Loss: 1680.53051758 Validation Loss: 1563.3115234375 Test Loss: 1816.0032958984375\n",
            "Epoch: 6461  Training Loss: 1680.43212891 Validation Loss: 1563.2978515625 Test Loss: 1815.9085693359375\n",
            "Epoch: 6466  Training Loss: 1680.33349609 Validation Loss: 1563.2838134765625 Test Loss: 1815.814208984375\n",
            "Epoch: 6471  Training Loss: 1680.23486328 Validation Loss: 1563.27001953125 Test Loss: 1815.719970703125\n",
            "Epoch: 6476  Training Loss: 1680.13647461 Validation Loss: 1563.256103515625 Test Loss: 1815.62548828125\n",
            "Epoch: 6481  Training Loss: 1680.03808594 Validation Loss: 1563.241943359375 Test Loss: 1815.5311279296875\n",
            "Epoch: 6486  Training Loss: 1679.93957520 Validation Loss: 1563.2279052734375 Test Loss: 1815.4366455078125\n",
            "Epoch: 6491  Training Loss: 1679.84106445 Validation Loss: 1563.213623046875 Test Loss: 1815.342529296875\n",
            "Epoch: 6496  Training Loss: 1679.74279785 Validation Loss: 1563.19921875 Test Loss: 1815.2479248046875\n",
            "Epoch: 6501  Training Loss: 1679.64440918 Validation Loss: 1563.1849365234375 Test Loss: 1815.1536865234375\n",
            "Epoch: 6506  Training Loss: 1679.54589844 Validation Loss: 1563.1705322265625 Test Loss: 1815.0595703125\n",
            "Epoch: 6511  Training Loss: 1679.44787598 Validation Loss: 1563.156005859375 Test Loss: 1814.965087890625\n",
            "Epoch: 6516  Training Loss: 1679.34960938 Validation Loss: 1563.141357421875 Test Loss: 1814.870849609375\n",
            "Epoch: 6521  Training Loss: 1679.25134277 Validation Loss: 1563.1268310546875 Test Loss: 1814.7764892578125\n",
            "Epoch: 6526  Training Loss: 1679.15319824 Validation Loss: 1563.1121826171875 Test Loss: 1814.682373046875\n",
            "Epoch: 6531  Training Loss: 1679.05493164 Validation Loss: 1563.0972900390625 Test Loss: 1814.588134765625\n",
            "Epoch: 6536  Training Loss: 1678.95703125 Validation Loss: 1563.08251953125 Test Loss: 1814.4937744140625\n",
            "Epoch: 6541  Training Loss: 1678.85888672 Validation Loss: 1563.067626953125 Test Loss: 1814.3997802734375\n",
            "Epoch: 6546  Training Loss: 1678.76062012 Validation Loss: 1563.0526123046875 Test Loss: 1814.305419921875\n",
            "Epoch: 6551  Training Loss: 1678.66271973 Validation Loss: 1563.037353515625 Test Loss: 1814.21142578125\n",
            "Epoch: 6556  Training Loss: 1678.56457520 Validation Loss: 1563.0220947265625 Test Loss: 1814.1171875\n",
            "Epoch: 6561  Training Loss: 1678.46691895 Validation Loss: 1563.0068359375 Test Loss: 1814.023193359375\n",
            "Epoch: 6566  Training Loss: 1678.36877441 Validation Loss: 1562.9915771484375 Test Loss: 1813.92919921875\n",
            "Epoch: 6571  Training Loss: 1678.27099609 Validation Loss: 1562.97607421875 Test Loss: 1813.8349609375\n",
            "Epoch: 6576  Training Loss: 1678.17321777 Validation Loss: 1562.9608154296875 Test Loss: 1813.740966796875\n",
            "Epoch: 6581  Training Loss: 1678.07531738 Validation Loss: 1562.945068359375 Test Loss: 1813.6468505859375\n",
            "Epoch: 6586  Training Loss: 1677.97741699 Validation Loss: 1562.9295654296875 Test Loss: 1813.552734375\n",
            "Epoch: 6591  Training Loss: 1677.87988281 Validation Loss: 1562.913818359375 Test Loss: 1813.458740234375\n",
            "Epoch: 6596  Training Loss: 1677.78210449 Validation Loss: 1562.898193359375 Test Loss: 1813.3648681640625\n",
            "Epoch: 6601  Training Loss: 1677.68432617 Validation Loss: 1562.88232421875 Test Loss: 1813.2706298828125\n",
            "Epoch: 6606  Training Loss: 1677.58666992 Validation Loss: 1562.8665771484375 Test Loss: 1813.1768798828125\n",
            "Epoch: 6611  Training Loss: 1677.48901367 Validation Loss: 1562.850341796875 Test Loss: 1813.0828857421875\n",
            "Epoch: 6616  Training Loss: 1677.39135742 Validation Loss: 1562.8343505859375 Test Loss: 1812.98876953125\n",
            "Epoch: 6621  Training Loss: 1677.29394531 Validation Loss: 1562.8182373046875 Test Loss: 1812.8948974609375\n",
            "Epoch: 6626  Training Loss: 1677.19641113 Validation Loss: 1562.80224609375 Test Loss: 1812.8009033203125\n",
            "Epoch: 6631  Training Loss: 1677.09887695 Validation Loss: 1562.78564453125 Test Loss: 1812.7069091796875\n",
            "Epoch: 6636  Training Loss: 1677.00134277 Validation Loss: 1562.7696533203125 Test Loss: 1812.613037109375\n",
            "Epoch: 6641  Training Loss: 1676.90405273 Validation Loss: 1562.7529296875 Test Loss: 1812.5191650390625\n",
            "Epoch: 6646  Training Loss: 1676.80664062 Validation Loss: 1562.736572265625 Test Loss: 1812.4254150390625\n",
            "Epoch: 6651  Training Loss: 1676.70935059 Validation Loss: 1562.7200927734375 Test Loss: 1812.3314208984375\n",
            "Epoch: 6656  Training Loss: 1676.61181641 Validation Loss: 1562.703369140625 Test Loss: 1812.2376708984375\n",
            "Epoch: 6661  Training Loss: 1676.51440430 Validation Loss: 1562.686767578125 Test Loss: 1812.1436767578125\n",
            "Epoch: 6666  Training Loss: 1676.41735840 Validation Loss: 1562.6697998046875 Test Loss: 1812.0501708984375\n",
            "Epoch: 6671  Training Loss: 1676.32019043 Validation Loss: 1562.6529541015625 Test Loss: 1811.9560546875\n",
            "Epoch: 6676  Training Loss: 1676.22290039 Validation Loss: 1562.635986328125 Test Loss: 1811.8624267578125\n",
            "Epoch: 6681  Training Loss: 1676.12585449 Validation Loss: 1562.619140625 Test Loss: 1811.7686767578125\n",
            "Epoch: 6686  Training Loss: 1676.02856445 Validation Loss: 1562.60205078125 Test Loss: 1811.6749267578125\n",
            "Epoch: 6691  Training Loss: 1675.93139648 Validation Loss: 1562.584716796875 Test Loss: 1811.5810546875\n",
            "Epoch: 6696  Training Loss: 1675.83447266 Validation Loss: 1562.5675048828125 Test Loss: 1811.4876708984375\n",
            "Epoch: 6701  Training Loss: 1675.73718262 Validation Loss: 1562.5501708984375 Test Loss: 1811.3936767578125\n",
            "Epoch: 6706  Training Loss: 1675.64038086 Validation Loss: 1562.532958984375 Test Loss: 1811.3001708984375\n",
            "Epoch: 6711  Training Loss: 1675.54357910 Validation Loss: 1562.5152587890625 Test Loss: 1811.20654296875\n",
            "Epoch: 6716  Training Loss: 1675.44653320 Validation Loss: 1562.497802734375 Test Loss: 1811.1129150390625\n",
            "Epoch: 6721  Training Loss: 1675.34973145 Validation Loss: 1562.4801025390625 Test Loss: 1811.0191650390625\n",
            "Epoch: 6726  Training Loss: 1675.25280762 Validation Loss: 1562.4625244140625 Test Loss: 1810.9256591796875\n",
            "Epoch: 6731  Training Loss: 1675.15600586 Validation Loss: 1562.44482421875 Test Loss: 1810.8319091796875\n",
            "Epoch: 6736  Training Loss: 1675.05920410 Validation Loss: 1562.427001953125 Test Loss: 1810.738525390625\n",
            "Epoch: 6741  Training Loss: 1674.96240234 Validation Loss: 1562.4090576171875 Test Loss: 1810.644775390625\n",
            "Epoch: 6746  Training Loss: 1674.86560059 Validation Loss: 1562.3909912109375 Test Loss: 1810.55126953125\n",
            "Epoch: 6751  Training Loss: 1674.76904297 Validation Loss: 1562.3729248046875 Test Loss: 1810.4576416015625\n",
            "Epoch: 6756  Training Loss: 1674.67224121 Validation Loss: 1562.35498046875 Test Loss: 1810.3643798828125\n",
            "Epoch: 6761  Training Loss: 1674.57580566 Validation Loss: 1562.3369140625 Test Loss: 1810.2706298828125\n",
            "Epoch: 6766  Training Loss: 1674.47912598 Validation Loss: 1562.318603515625 Test Loss: 1810.177490234375\n",
            "Epoch: 6771  Training Loss: 1674.38256836 Validation Loss: 1562.3001708984375 Test Loss: 1810.0838623046875\n",
            "Epoch: 6776  Training Loss: 1674.28613281 Validation Loss: 1562.2816162109375 Test Loss: 1809.990234375\n",
            "Epoch: 6781  Training Loss: 1674.18957520 Validation Loss: 1562.26318359375 Test Loss: 1809.89697265625\n",
            "Epoch: 6786  Training Loss: 1674.09313965 Validation Loss: 1562.24462890625 Test Loss: 1809.8037109375\n",
            "Epoch: 6791  Training Loss: 1673.99658203 Validation Loss: 1562.22607421875 Test Loss: 1809.710205078125\n",
            "Epoch: 6796  Training Loss: 1673.90026855 Validation Loss: 1562.2073974609375 Test Loss: 1809.61669921875\n",
            "Epoch: 6801  Training Loss: 1673.80395508 Validation Loss: 1562.1885986328125 Test Loss: 1809.5235595703125\n",
            "Epoch: 6806  Training Loss: 1673.70764160 Validation Loss: 1562.169677734375 Test Loss: 1809.43017578125\n",
            "Epoch: 6811  Training Loss: 1673.61132812 Validation Loss: 1562.1507568359375 Test Loss: 1809.336669921875\n",
            "Epoch: 6816  Training Loss: 1673.51513672 Validation Loss: 1562.1318359375 Test Loss: 1809.2435302734375\n",
            "Epoch: 6821  Training Loss: 1673.41882324 Validation Loss: 1562.1126708984375 Test Loss: 1809.150390625\n",
            "Epoch: 6826  Training Loss: 1673.32287598 Validation Loss: 1562.093505859375 Test Loss: 1809.056884765625\n",
            "Epoch: 6831  Training Loss: 1673.22656250 Validation Loss: 1562.074462890625 Test Loss: 1808.9637451171875\n",
            "Epoch: 6836  Training Loss: 1673.13037109 Validation Loss: 1562.054931640625 Test Loss: 1808.8704833984375\n",
            "Epoch: 6841  Training Loss: 1673.03442383 Validation Loss: 1562.03564453125 Test Loss: 1808.777099609375\n",
            "Epoch: 6846  Training Loss: 1672.93835449 Validation Loss: 1562.0162353515625 Test Loss: 1808.683837890625\n",
            "Epoch: 6851  Training Loss: 1672.84240723 Validation Loss: 1561.9967041015625 Test Loss: 1808.5908203125\n",
            "Epoch: 6856  Training Loss: 1672.74621582 Validation Loss: 1561.9771728515625 Test Loss: 1808.497802734375\n",
            "Epoch: 6861  Training Loss: 1672.65026855 Validation Loss: 1561.95751953125 Test Loss: 1808.404541015625\n",
            "Epoch: 6866  Training Loss: 1672.55444336 Validation Loss: 1561.937744140625 Test Loss: 1808.311279296875\n",
            "Epoch: 6871  Training Loss: 1672.45861816 Validation Loss: 1561.91796875 Test Loss: 1808.2181396484375\n",
            "Epoch: 6876  Training Loss: 1672.36291504 Validation Loss: 1561.898193359375 Test Loss: 1808.125244140625\n",
            "Epoch: 6881  Training Loss: 1672.26684570 Validation Loss: 1561.878173828125 Test Loss: 1808.031982421875\n",
            "Epoch: 6886  Training Loss: 1672.17126465 Validation Loss: 1561.8580322265625 Test Loss: 1807.9388427734375\n",
            "Epoch: 6891  Training Loss: 1672.07543945 Validation Loss: 1561.8380126953125 Test Loss: 1807.845947265625\n",
            "Epoch: 6896  Training Loss: 1671.97961426 Validation Loss: 1561.81787109375 Test Loss: 1807.7529296875\n",
            "Epoch: 6901  Training Loss: 1671.88403320 Validation Loss: 1561.797607421875 Test Loss: 1807.6597900390625\n",
            "Epoch: 6906  Training Loss: 1671.78833008 Validation Loss: 1561.7772216796875 Test Loss: 1807.5667724609375\n",
            "Epoch: 6911  Training Loss: 1671.69274902 Validation Loss: 1561.7568359375 Test Loss: 1807.473876953125\n",
            "Epoch: 6916  Training Loss: 1671.59729004 Validation Loss: 1561.7364501953125 Test Loss: 1807.380859375\n",
            "Epoch: 6921  Training Loss: 1671.50183105 Validation Loss: 1561.7159423828125 Test Loss: 1807.2880859375\n",
            "Epoch: 6926  Training Loss: 1671.40637207 Validation Loss: 1561.6951904296875 Test Loss: 1807.195068359375\n",
            "Epoch: 6931  Training Loss: 1671.31066895 Validation Loss: 1561.6746826171875 Test Loss: 1807.10205078125\n",
            "Epoch: 6936  Training Loss: 1671.21533203 Validation Loss: 1561.65380859375 Test Loss: 1807.0091552734375\n",
            "Epoch: 6941  Training Loss: 1671.11987305 Validation Loss: 1561.633056640625 Test Loss: 1806.916259765625\n",
            "Epoch: 6946  Training Loss: 1671.02465820 Validation Loss: 1561.6121826171875 Test Loss: 1806.8233642578125\n",
            "Epoch: 6951  Training Loss: 1670.92932129 Validation Loss: 1561.5911865234375 Test Loss: 1806.730712890625\n",
            "Epoch: 6956  Training Loss: 1670.83398438 Validation Loss: 1561.5701904296875 Test Loss: 1806.6376953125\n",
            "Epoch: 6961  Training Loss: 1670.73864746 Validation Loss: 1561.549072265625 Test Loss: 1806.544921875\n",
            "Epoch: 6966  Training Loss: 1670.64355469 Validation Loss: 1561.5279541015625 Test Loss: 1806.4522705078125\n",
            "Epoch: 6971  Training Loss: 1670.54833984 Validation Loss: 1561.506591796875 Test Loss: 1806.359375\n",
            "Epoch: 6976  Training Loss: 1670.45324707 Validation Loss: 1561.4853515625 Test Loss: 1806.266845703125\n",
            "Epoch: 6981  Training Loss: 1670.35815430 Validation Loss: 1561.4639892578125 Test Loss: 1806.1739501953125\n",
            "Epoch: 6986  Training Loss: 1670.26293945 Validation Loss: 1561.4422607421875 Test Loss: 1806.0810546875\n",
            "Epoch: 6991  Training Loss: 1670.16796875 Validation Loss: 1561.4207763671875 Test Loss: 1805.988525390625\n",
            "Epoch: 6996  Training Loss: 1670.07287598 Validation Loss: 1561.3992919921875 Test Loss: 1805.8958740234375\n",
            "Epoch: 7001  Training Loss: 1669.97802734 Validation Loss: 1561.3775634765625 Test Loss: 1805.80322265625\n",
            "Epoch: 7006  Training Loss: 1669.88293457 Validation Loss: 1561.355712890625 Test Loss: 1805.71044921875\n",
            "Epoch: 7011  Training Loss: 1669.78808594 Validation Loss: 1561.3338623046875 Test Loss: 1805.617919921875\n",
            "Epoch: 7016  Training Loss: 1669.69323730 Validation Loss: 1561.3121337890625 Test Loss: 1805.525390625\n",
            "Epoch: 7021  Training Loss: 1669.59838867 Validation Loss: 1561.2900390625 Test Loss: 1805.4327392578125\n",
            "Epoch: 7026  Training Loss: 1669.50366211 Validation Loss: 1561.26806640625 Test Loss: 1805.3402099609375\n",
            "Epoch: 7031  Training Loss: 1669.40881348 Validation Loss: 1561.24609375 Test Loss: 1805.2476806640625\n",
            "Epoch: 7036  Training Loss: 1669.31408691 Validation Loss: 1561.223876953125 Test Loss: 1805.155029296875\n",
            "Epoch: 7041  Training Loss: 1669.21936035 Validation Loss: 1561.201416015625 Test Loss: 1805.0625\n",
            "Epoch: 7046  Training Loss: 1669.12463379 Validation Loss: 1561.1790771484375 Test Loss: 1804.969970703125\n",
            "Epoch: 7051  Training Loss: 1669.03002930 Validation Loss: 1561.156982421875 Test Loss: 1804.8775634765625\n",
            "Epoch: 7056  Training Loss: 1668.93542480 Validation Loss: 1561.1343994140625 Test Loss: 1804.7850341796875\n",
            "Epoch: 7061  Training Loss: 1668.84106445 Validation Loss: 1561.1119384765625 Test Loss: 1804.6925048828125\n",
            "Epoch: 7066  Training Loss: 1668.74633789 Validation Loss: 1561.08935546875 Test Loss: 1804.6002197265625\n",
            "Epoch: 7071  Training Loss: 1668.65209961 Validation Loss: 1561.0665283203125 Test Loss: 1804.5076904296875\n",
            "Epoch: 7076  Training Loss: 1668.55737305 Validation Loss: 1561.043701171875 Test Loss: 1804.41552734375\n",
            "Epoch: 7081  Training Loss: 1668.46313477 Validation Loss: 1561.02099609375 Test Loss: 1804.322998046875\n",
            "Epoch: 7086  Training Loss: 1668.36877441 Validation Loss: 1560.9981689453125 Test Loss: 1804.2305908203125\n",
            "Epoch: 7091  Training Loss: 1668.27441406 Validation Loss: 1560.9752197265625 Test Loss: 1804.1383056640625\n",
            "Epoch: 7096  Training Loss: 1668.18017578 Validation Loss: 1560.9520263671875 Test Loss: 1804.0460205078125\n",
            "Epoch: 7101  Training Loss: 1668.08593750 Validation Loss: 1560.9290771484375 Test Loss: 1803.9537353515625\n",
            "Epoch: 7106  Training Loss: 1667.99157715 Validation Loss: 1560.9058837890625 Test Loss: 1803.8614501953125\n",
            "Epoch: 7111  Training Loss: 1667.89733887 Validation Loss: 1560.8826904296875 Test Loss: 1803.7691650390625\n",
            "Epoch: 7116  Training Loss: 1667.80310059 Validation Loss: 1560.859130859375 Test Loss: 1803.6768798828125\n",
            "Epoch: 7121  Training Loss: 1667.70898438 Validation Loss: 1560.8358154296875 Test Loss: 1803.5845947265625\n",
            "Epoch: 7126  Training Loss: 1667.61499023 Validation Loss: 1560.8125 Test Loss: 1803.4923095703125\n",
            "Epoch: 7131  Training Loss: 1667.52099609 Validation Loss: 1560.7889404296875 Test Loss: 1803.400390625\n",
            "Epoch: 7136  Training Loss: 1667.42687988 Validation Loss: 1560.7652587890625 Test Loss: 1803.3082275390625\n",
            "Epoch: 7141  Training Loss: 1667.33288574 Validation Loss: 1560.741943359375 Test Loss: 1803.2159423828125\n",
            "Epoch: 7146  Training Loss: 1667.23889160 Validation Loss: 1560.718017578125 Test Loss: 1803.123779296875\n",
            "Epoch: 7151  Training Loss: 1667.14501953 Validation Loss: 1560.694091796875 Test Loss: 1803.0318603515625\n",
            "Epoch: 7156  Training Loss: 1667.05114746 Validation Loss: 1560.67041015625 Test Loss: 1802.9395751953125\n",
            "Epoch: 7161  Training Loss: 1666.95739746 Validation Loss: 1560.6463623046875 Test Loss: 1802.8475341796875\n",
            "Epoch: 7166  Training Loss: 1666.86352539 Validation Loss: 1560.62255859375 Test Loss: 1802.755615234375\n",
            "Epoch: 7171  Training Loss: 1666.76965332 Validation Loss: 1560.5985107421875 Test Loss: 1802.66357421875\n",
            "Epoch: 7176  Training Loss: 1666.67614746 Validation Loss: 1560.57421875 Test Loss: 1802.5716552734375\n",
            "Epoch: 7181  Training Loss: 1666.58239746 Validation Loss: 1560.550048828125 Test Loss: 1802.4796142578125\n",
            "Epoch: 7186  Training Loss: 1666.48852539 Validation Loss: 1560.5257568359375 Test Loss: 1802.3875732421875\n",
            "Epoch: 7191  Training Loss: 1666.39501953 Validation Loss: 1560.501708984375 Test Loss: 1802.2955322265625\n",
            "Epoch: 7196  Training Loss: 1666.30151367 Validation Loss: 1560.4771728515625 Test Loss: 1802.2037353515625\n",
            "Epoch: 7201  Training Loss: 1666.20788574 Validation Loss: 1560.4525146484375 Test Loss: 1802.1116943359375\n",
            "Epoch: 7206  Training Loss: 1666.11437988 Validation Loss: 1560.427978515625 Test Loss: 1802.019775390625\n",
            "Epoch: 7211  Training Loss: 1666.02087402 Validation Loss: 1560.4034423828125 Test Loss: 1801.927978515625\n",
            "Epoch: 7216  Training Loss: 1665.92736816 Validation Loss: 1560.378662109375 Test Loss: 1801.8360595703125\n",
            "Epoch: 7221  Training Loss: 1665.83398438 Validation Loss: 1560.35400390625 Test Loss: 1801.744140625\n",
            "Epoch: 7226  Training Loss: 1665.74060059 Validation Loss: 1560.3292236328125 Test Loss: 1801.6524658203125\n",
            "Epoch: 7231  Training Loss: 1665.64733887 Validation Loss: 1560.304443359375 Test Loss: 1801.560546875\n",
            "Epoch: 7236  Training Loss: 1665.55407715 Validation Loss: 1560.279541015625 Test Loss: 1801.46875\n",
            "Epoch: 7241  Training Loss: 1665.46057129 Validation Loss: 1560.25439453125 Test Loss: 1801.3770751953125\n",
            "Epoch: 7246  Training Loss: 1665.36743164 Validation Loss: 1560.2293701171875 Test Loss: 1801.2852783203125\n",
            "Epoch: 7251  Training Loss: 1665.27416992 Validation Loss: 1560.2042236328125 Test Loss: 1801.1937255859375\n",
            "Epoch: 7256  Training Loss: 1665.18090820 Validation Loss: 1560.1790771484375 Test Loss: 1801.101806640625\n",
            "Epoch: 7261  Training Loss: 1665.08789062 Validation Loss: 1560.1536865234375 Test Loss: 1801.0101318359375\n",
            "Epoch: 7266  Training Loss: 1664.99487305 Validation Loss: 1560.1285400390625 Test Loss: 1800.91845703125\n",
            "Epoch: 7271  Training Loss: 1664.90173340 Validation Loss: 1560.1029052734375 Test Loss: 1800.8267822265625\n",
            "Epoch: 7276  Training Loss: 1664.80871582 Validation Loss: 1560.0775146484375 Test Loss: 1800.7352294921875\n",
            "Epoch: 7281  Training Loss: 1664.71582031 Validation Loss: 1560.052001953125 Test Loss: 1800.6436767578125\n",
            "Epoch: 7286  Training Loss: 1664.62292480 Validation Loss: 1560.0264892578125 Test Loss: 1800.5521240234375\n",
            "Epoch: 7291  Training Loss: 1664.52990723 Validation Loss: 1560.000732421875 Test Loss: 1800.46044921875\n",
            "Epoch: 7296  Training Loss: 1664.43701172 Validation Loss: 1559.9749755859375 Test Loss: 1800.368896484375\n",
            "Epoch: 7301  Training Loss: 1664.34411621 Validation Loss: 1559.9490966796875 Test Loss: 1800.2774658203125\n",
            "Epoch: 7306  Training Loss: 1664.25134277 Validation Loss: 1559.92333984375 Test Loss: 1800.18603515625\n",
            "Epoch: 7311  Training Loss: 1664.15856934 Validation Loss: 1559.897216796875 Test Loss: 1800.0943603515625\n",
            "Epoch: 7316  Training Loss: 1664.06591797 Validation Loss: 1559.871337890625 Test Loss: 1800.0029296875\n",
            "Epoch: 7321  Training Loss: 1663.97314453 Validation Loss: 1559.84521484375 Test Loss: 1799.9114990234375\n",
            "Epoch: 7326  Training Loss: 1663.88037109 Validation Loss: 1559.8193359375 Test Loss: 1799.8201904296875\n",
            "Epoch: 7331  Training Loss: 1663.78784180 Validation Loss: 1559.7928466796875 Test Loss: 1799.7286376953125\n",
            "Epoch: 7336  Training Loss: 1663.69543457 Validation Loss: 1559.7667236328125 Test Loss: 1799.6373291015625\n",
            "Epoch: 7341  Training Loss: 1663.60266113 Validation Loss: 1559.740478515625 Test Loss: 1799.5458984375\n",
            "Epoch: 7346  Training Loss: 1663.51013184 Validation Loss: 1559.7139892578125 Test Loss: 1799.4544677734375\n",
            "Epoch: 7351  Training Loss: 1663.41784668 Validation Loss: 1559.6875 Test Loss: 1799.3631591796875\n",
            "Epoch: 7356  Training Loss: 1663.32531738 Validation Loss: 1559.6610107421875 Test Loss: 1799.2718505859375\n",
            "Epoch: 7361  Training Loss: 1663.23291016 Validation Loss: 1559.634521484375 Test Loss: 1799.1806640625\n",
            "Epoch: 7366  Training Loss: 1663.14038086 Validation Loss: 1559.60791015625 Test Loss: 1799.08935546875\n",
            "Epoch: 7371  Training Loss: 1663.04821777 Validation Loss: 1559.581298828125 Test Loss: 1798.998046875\n",
            "Epoch: 7376  Training Loss: 1662.95593262 Validation Loss: 1559.554443359375 Test Loss: 1798.906982421875\n",
            "Epoch: 7381  Training Loss: 1662.86364746 Validation Loss: 1559.527587890625 Test Loss: 1798.8157958984375\n",
            "Epoch: 7386  Training Loss: 1662.77148438 Validation Loss: 1559.5006103515625 Test Loss: 1798.724609375\n",
            "Epoch: 7391  Training Loss: 1662.67932129 Validation Loss: 1559.4737548828125 Test Loss: 1798.63330078125\n",
            "Epoch: 7396  Training Loss: 1662.58728027 Validation Loss: 1559.4466552734375 Test Loss: 1798.5421142578125\n",
            "Epoch: 7401  Training Loss: 1662.49511719 Validation Loss: 1559.419677734375 Test Loss: 1798.4510498046875\n",
            "Epoch: 7406  Training Loss: 1662.40283203 Validation Loss: 1559.3927001953125 Test Loss: 1798.3599853515625\n",
            "Epoch: 7411  Training Loss: 1662.31091309 Validation Loss: 1559.365234375 Test Loss: 1798.2689208984375\n",
            "Epoch: 7416  Training Loss: 1662.21887207 Validation Loss: 1559.337890625 Test Loss: 1798.177734375\n",
            "Epoch: 7421  Training Loss: 1662.12695312 Validation Loss: 1559.3106689453125 Test Loss: 1798.0867919921875\n",
            "Epoch: 7426  Training Loss: 1662.03491211 Validation Loss: 1559.2833251953125 Test Loss: 1797.9957275390625\n",
            "Epoch: 7431  Training Loss: 1661.94311523 Validation Loss: 1559.255859375 Test Loss: 1797.90478515625\n",
            "Epoch: 7436  Training Loss: 1661.85119629 Validation Loss: 1559.228515625 Test Loss: 1797.8138427734375\n",
            "Epoch: 7441  Training Loss: 1661.75952148 Validation Loss: 1559.20068359375 Test Loss: 1797.722900390625\n",
            "Epoch: 7446  Training Loss: 1661.66760254 Validation Loss: 1559.1732177734375 Test Loss: 1797.6319580078125\n",
            "Epoch: 7451  Training Loss: 1661.57580566 Validation Loss: 1559.1453857421875 Test Loss: 1797.541015625\n",
            "Epoch: 7456  Training Loss: 1661.48425293 Validation Loss: 1559.1177978515625 Test Loss: 1797.4501953125\n",
            "Epoch: 7461  Training Loss: 1661.39257812 Validation Loss: 1559.0897216796875 Test Loss: 1797.3592529296875\n",
            "Epoch: 7466  Training Loss: 1661.30090332 Validation Loss: 1559.0621337890625 Test Loss: 1797.2684326171875\n",
            "Epoch: 7471  Training Loss: 1661.20935059 Validation Loss: 1559.0340576171875 Test Loss: 1797.177490234375\n",
            "Epoch: 7476  Training Loss: 1661.11779785 Validation Loss: 1559.006103515625 Test Loss: 1797.086669921875\n",
            "Epoch: 7481  Training Loss: 1661.02624512 Validation Loss: 1558.9779052734375 Test Loss: 1796.9959716796875\n",
            "Epoch: 7486  Training Loss: 1660.93469238 Validation Loss: 1558.949951171875 Test Loss: 1796.9052734375\n",
            "Epoch: 7491  Training Loss: 1660.84338379 Validation Loss: 1558.921630859375 Test Loss: 1796.8145751953125\n",
            "Epoch: 7496  Training Loss: 1660.75183105 Validation Loss: 1558.8935546875 Test Loss: 1796.7237548828125\n",
            "Epoch: 7501  Training Loss: 1660.66040039 Validation Loss: 1558.865234375 Test Loss: 1796.63330078125\n",
            "Epoch: 7506  Training Loss: 1660.56909180 Validation Loss: 1558.8369140625 Test Loss: 1796.54248046875\n",
            "Epoch: 7511  Training Loss: 1660.47790527 Validation Loss: 1558.8082275390625 Test Loss: 1796.4517822265625\n",
            "Epoch: 7516  Training Loss: 1660.38647461 Validation Loss: 1558.7799072265625 Test Loss: 1796.361083984375\n",
            "Epoch: 7521  Training Loss: 1660.29541016 Validation Loss: 1558.7513427734375 Test Loss: 1796.2705078125\n",
            "Epoch: 7526  Training Loss: 1660.20410156 Validation Loss: 1558.7227783203125 Test Loss: 1796.1798095703125\n",
            "Epoch: 7531  Training Loss: 1660.11303711 Validation Loss: 1558.694091796875 Test Loss: 1796.08935546875\n",
            "Epoch: 7536  Training Loss: 1660.02185059 Validation Loss: 1558.6654052734375 Test Loss: 1795.998779296875\n",
            "Epoch: 7541  Training Loss: 1659.93078613 Validation Loss: 1558.63671875 Test Loss: 1795.9083251953125\n",
            "Epoch: 7546  Training Loss: 1659.83972168 Validation Loss: 1558.60791015625 Test Loss: 1795.8177490234375\n",
            "Epoch: 7551  Training Loss: 1659.74890137 Validation Loss: 1558.578857421875 Test Loss: 1795.7271728515625\n",
            "Epoch: 7556  Training Loss: 1659.65783691 Validation Loss: 1558.550048828125 Test Loss: 1795.6368408203125\n",
            "Epoch: 7561  Training Loss: 1659.56677246 Validation Loss: 1558.5211181640625 Test Loss: 1795.54638671875\n",
            "Epoch: 7566  Training Loss: 1659.47607422 Validation Loss: 1558.4920654296875 Test Loss: 1795.4559326171875\n",
            "Epoch: 7571  Training Loss: 1659.38513184 Validation Loss: 1558.462890625 Test Loss: 1795.3656005859375\n",
            "Epoch: 7576  Training Loss: 1659.29431152 Validation Loss: 1558.433837890625 Test Loss: 1795.275390625\n",
            "Epoch: 7581  Training Loss: 1659.20361328 Validation Loss: 1558.404541015625 Test Loss: 1795.184814453125\n",
            "Epoch: 7586  Training Loss: 1659.11267090 Validation Loss: 1558.3753662109375 Test Loss: 1795.094482421875\n",
            "Epoch: 7591  Training Loss: 1659.02197266 Validation Loss: 1558.345947265625 Test Loss: 1795.0042724609375\n",
            "Epoch: 7596  Training Loss: 1658.93139648 Validation Loss: 1558.3165283203125 Test Loss: 1794.9139404296875\n",
            "Epoch: 7601  Training Loss: 1658.84069824 Validation Loss: 1558.287109375 Test Loss: 1794.82373046875\n",
            "Epoch: 7606  Training Loss: 1658.75012207 Validation Loss: 1558.257568359375 Test Loss: 1794.7335205078125\n",
            "Epoch: 7611  Training Loss: 1658.65966797 Validation Loss: 1558.2281494140625 Test Loss: 1794.643310546875\n",
            "Epoch: 7616  Training Loss: 1658.56909180 Validation Loss: 1558.1983642578125 Test Loss: 1794.5531005859375\n",
            "Epoch: 7621  Training Loss: 1658.47863770 Validation Loss: 1558.168701171875 Test Loss: 1794.462890625\n",
            "Epoch: 7626  Training Loss: 1658.38818359 Validation Loss: 1558.138916015625 Test Loss: 1794.3726806640625\n",
            "Epoch: 7631  Training Loss: 1658.29772949 Validation Loss: 1558.109375 Test Loss: 1794.2825927734375\n",
            "Epoch: 7636  Training Loss: 1658.20739746 Validation Loss: 1558.0794677734375 Test Loss: 1794.1923828125\n",
            "Epoch: 7641  Training Loss: 1658.11694336 Validation Loss: 1558.0496826171875 Test Loss: 1794.102294921875\n",
            "Epoch: 7646  Training Loss: 1658.02673340 Validation Loss: 1558.01953125 Test Loss: 1794.0123291015625\n",
            "Epoch: 7651  Training Loss: 1657.93652344 Validation Loss: 1557.989501953125 Test Loss: 1793.9224853515625\n",
            "Epoch: 7656  Training Loss: 1657.84631348 Validation Loss: 1557.959716796875 Test Loss: 1793.832275390625\n",
            "Epoch: 7661  Training Loss: 1657.75610352 Validation Loss: 1557.929443359375 Test Loss: 1793.7423095703125\n",
            "Epoch: 7666  Training Loss: 1657.66601562 Validation Loss: 1557.8992919921875 Test Loss: 1793.6524658203125\n",
            "Epoch: 7671  Training Loss: 1657.57592773 Validation Loss: 1557.869140625 Test Loss: 1793.5623779296875\n",
            "Epoch: 7676  Training Loss: 1657.48571777 Validation Loss: 1557.8387451171875 Test Loss: 1793.4725341796875\n",
            "Epoch: 7681  Training Loss: 1657.39587402 Validation Loss: 1557.8084716796875 Test Loss: 1793.3826904296875\n",
            "Epoch: 7686  Training Loss: 1657.30578613 Validation Loss: 1557.7779541015625 Test Loss: 1793.292724609375\n",
            "Epoch: 7691  Training Loss: 1657.21582031 Validation Loss: 1557.74755859375 Test Loss: 1793.2030029296875\n",
            "Epoch: 7696  Training Loss: 1657.12597656 Validation Loss: 1557.71728515625 Test Loss: 1793.113037109375\n",
            "Epoch: 7701  Training Loss: 1657.03613281 Validation Loss: 1557.686767578125 Test Loss: 1793.0233154296875\n",
            "Epoch: 7706  Training Loss: 1656.94616699 Validation Loss: 1557.656005859375 Test Loss: 1792.933349609375\n",
            "Epoch: 7711  Training Loss: 1656.85632324 Validation Loss: 1557.6256103515625 Test Loss: 1792.8438720703125\n",
            "Epoch: 7716  Training Loss: 1656.76672363 Validation Loss: 1557.5948486328125 Test Loss: 1792.754150390625\n",
            "Epoch: 7721  Training Loss: 1656.67687988 Validation Loss: 1557.56396484375 Test Loss: 1792.664306640625\n",
            "Epoch: 7726  Training Loss: 1656.58728027 Validation Loss: 1557.5333251953125 Test Loss: 1792.5745849609375\n",
            "Epoch: 7731  Training Loss: 1656.49755859 Validation Loss: 1557.50244140625 Test Loss: 1792.4849853515625\n",
            "Epoch: 7736  Training Loss: 1656.40783691 Validation Loss: 1557.4716796875 Test Loss: 1792.3953857421875\n",
            "Epoch: 7741  Training Loss: 1656.31835938 Validation Loss: 1557.440673828125 Test Loss: 1792.3056640625\n",
            "Epoch: 7746  Training Loss: 1656.22888184 Validation Loss: 1557.4097900390625 Test Loss: 1792.2161865234375\n",
            "Epoch: 7751  Training Loss: 1656.13940430 Validation Loss: 1557.378662109375 Test Loss: 1792.12646484375\n",
            "Epoch: 7756  Training Loss: 1656.04992676 Validation Loss: 1557.347412109375 Test Loss: 1792.037109375\n",
            "Epoch: 7761  Training Loss: 1655.96044922 Validation Loss: 1557.3165283203125 Test Loss: 1791.947509765625\n",
            "Epoch: 7766  Training Loss: 1655.87109375 Validation Loss: 1557.28515625 Test Loss: 1791.85791015625\n",
            "Epoch: 7771  Training Loss: 1655.78161621 Validation Loss: 1557.2540283203125 Test Loss: 1791.7685546875\n",
            "Epoch: 7776  Training Loss: 1655.69238281 Validation Loss: 1557.22265625 Test Loss: 1791.6790771484375\n",
            "Epoch: 7781  Training Loss: 1655.60314941 Validation Loss: 1557.19140625 Test Loss: 1791.589599609375\n",
            "Epoch: 7786  Training Loss: 1655.51391602 Validation Loss: 1557.159912109375 Test Loss: 1791.500244140625\n",
            "Epoch: 7791  Training Loss: 1655.42468262 Validation Loss: 1557.1285400390625 Test Loss: 1791.4110107421875\n",
            "Epoch: 7796  Training Loss: 1655.33557129 Validation Loss: 1557.096923828125 Test Loss: 1791.3216552734375\n",
            "Epoch: 7801  Training Loss: 1655.24633789 Validation Loss: 1557.0654296875 Test Loss: 1791.2322998046875\n",
            "Epoch: 7806  Training Loss: 1655.15734863 Validation Loss: 1557.0338134765625 Test Loss: 1791.1429443359375\n",
            "Epoch: 7811  Training Loss: 1655.06835938 Validation Loss: 1557.002197265625 Test Loss: 1791.0537109375\n",
            "Epoch: 7816  Training Loss: 1654.97937012 Validation Loss: 1556.9705810546875 Test Loss: 1790.9644775390625\n",
            "Epoch: 7821  Training Loss: 1654.89038086 Validation Loss: 1556.9388427734375 Test Loss: 1790.875244140625\n",
            "Epoch: 7826  Training Loss: 1654.80151367 Validation Loss: 1556.9072265625 Test Loss: 1790.7860107421875\n",
            "Epoch: 7831  Training Loss: 1654.71264648 Validation Loss: 1556.8753662109375 Test Loss: 1790.6968994140625\n",
            "Epoch: 7836  Training Loss: 1654.62365723 Validation Loss: 1556.8433837890625 Test Loss: 1790.6077880859375\n",
            "Epoch: 7841  Training Loss: 1654.53491211 Validation Loss: 1556.8115234375 Test Loss: 1790.5184326171875\n",
            "Epoch: 7846  Training Loss: 1654.44616699 Validation Loss: 1556.7794189453125 Test Loss: 1790.4295654296875\n",
            "Epoch: 7851  Training Loss: 1654.35742188 Validation Loss: 1556.7474365234375 Test Loss: 1790.34033203125\n",
            "Epoch: 7856  Training Loss: 1654.26867676 Validation Loss: 1556.7154541015625 Test Loss: 1790.25146484375\n",
            "Epoch: 7861  Training Loss: 1654.18017578 Validation Loss: 1556.6832275390625 Test Loss: 1790.1624755859375\n",
            "Epoch: 7866  Training Loss: 1654.09155273 Validation Loss: 1556.651123046875 Test Loss: 1790.0731201171875\n",
            "Epoch: 7871  Training Loss: 1654.00292969 Validation Loss: 1556.618896484375 Test Loss: 1789.984375\n",
            "Epoch: 7876  Training Loss: 1653.91442871 Validation Loss: 1556.5867919921875 Test Loss: 1789.8955078125\n",
            "Epoch: 7881  Training Loss: 1653.82580566 Validation Loss: 1556.5545654296875 Test Loss: 1789.806640625\n",
            "Epoch: 7886  Training Loss: 1653.73754883 Validation Loss: 1556.5220947265625 Test Loss: 1789.717529296875\n",
            "Epoch: 7891  Training Loss: 1653.64892578 Validation Loss: 1556.48974609375 Test Loss: 1789.6287841796875\n",
            "Epoch: 7896  Training Loss: 1653.56066895 Validation Loss: 1556.4573974609375 Test Loss: 1789.5399169921875\n",
            "Epoch: 7901  Training Loss: 1653.47229004 Validation Loss: 1556.425048828125 Test Loss: 1789.4510498046875\n",
            "Epoch: 7906  Training Loss: 1653.38391113 Validation Loss: 1556.3924560546875 Test Loss: 1789.3623046875\n",
            "Epoch: 7911  Training Loss: 1653.29565430 Validation Loss: 1556.3597412109375 Test Loss: 1789.2735595703125\n",
            "Epoch: 7916  Training Loss: 1653.20739746 Validation Loss: 1556.3271484375 Test Loss: 1789.184814453125\n",
            "Epoch: 7921  Training Loss: 1653.11926270 Validation Loss: 1556.294677734375 Test Loss: 1789.0960693359375\n",
            "Epoch: 7926  Training Loss: 1653.03100586 Validation Loss: 1556.261962890625 Test Loss: 1789.00732421875\n",
            "Epoch: 7931  Training Loss: 1652.94311523 Validation Loss: 1556.22900390625 Test Loss: 1788.9188232421875\n",
            "Epoch: 7936  Training Loss: 1652.85498047 Validation Loss: 1556.196533203125 Test Loss: 1788.829833984375\n",
            "Epoch: 7941  Training Loss: 1652.76684570 Validation Loss: 1556.16357421875 Test Loss: 1788.741455078125\n",
            "Epoch: 7946  Training Loss: 1652.67895508 Validation Loss: 1556.1307373046875 Test Loss: 1788.6527099609375\n",
            "Epoch: 7951  Training Loss: 1652.59082031 Validation Loss: 1556.0977783203125 Test Loss: 1788.564208984375\n",
            "Epoch: 7956  Training Loss: 1652.50292969 Validation Loss: 1556.064697265625 Test Loss: 1788.475830078125\n",
            "Epoch: 7961  Training Loss: 1652.41503906 Validation Loss: 1556.031982421875 Test Loss: 1788.3870849609375\n",
            "Epoch: 7966  Training Loss: 1652.32727051 Validation Loss: 1555.9989013671875 Test Loss: 1788.298828125\n",
            "Epoch: 7971  Training Loss: 1652.23950195 Validation Loss: 1555.9658203125 Test Loss: 1788.210205078125\n",
            "Epoch: 7976  Training Loss: 1652.15173340 Validation Loss: 1555.9327392578125 Test Loss: 1788.1219482421875\n",
            "Epoch: 7981  Training Loss: 1652.06396484 Validation Loss: 1555.8994140625 Test Loss: 1788.033447265625\n",
            "Epoch: 7986  Training Loss: 1651.97631836 Validation Loss: 1555.8663330078125 Test Loss: 1787.9451904296875\n",
            "Epoch: 7991  Training Loss: 1651.88867188 Validation Loss: 1555.8331298828125 Test Loss: 1787.856689453125\n",
            "Epoch: 7996  Training Loss: 1651.80114746 Validation Loss: 1555.7998046875 Test Loss: 1787.7684326171875\n",
            "Epoch: 8001  Training Loss: 1651.71362305 Validation Loss: 1555.7666015625 Test Loss: 1787.679931640625\n",
            "Epoch: 8006  Training Loss: 1651.62597656 Validation Loss: 1555.7330322265625 Test Loss: 1787.591796875\n",
            "Epoch: 8011  Training Loss: 1651.53833008 Validation Loss: 1555.6995849609375 Test Loss: 1787.5035400390625\n",
            "Epoch: 8016  Training Loss: 1651.45117188 Validation Loss: 1555.6663818359375 Test Loss: 1787.4154052734375\n",
            "Epoch: 8021  Training Loss: 1651.36364746 Validation Loss: 1555.6326904296875 Test Loss: 1787.3272705078125\n",
            "Epoch: 8026  Training Loss: 1651.27624512 Validation Loss: 1555.5992431640625 Test Loss: 1787.2391357421875\n",
            "Epoch: 8031  Training Loss: 1651.18896484 Validation Loss: 1555.5657958984375 Test Loss: 1787.1507568359375\n",
            "Epoch: 8036  Training Loss: 1651.10156250 Validation Loss: 1555.5322265625 Test Loss: 1787.062744140625\n",
            "Epoch: 8041  Training Loss: 1651.01440430 Validation Loss: 1555.49853515625 Test Loss: 1786.974853515625\n",
            "Epoch: 8046  Training Loss: 1650.92712402 Validation Loss: 1555.4647216796875 Test Loss: 1786.8868408203125\n",
            "Epoch: 8051  Training Loss: 1650.83984375 Validation Loss: 1555.4310302734375 Test Loss: 1786.798583984375\n",
            "Epoch: 8056  Training Loss: 1650.75280762 Validation Loss: 1555.3974609375 Test Loss: 1786.71044921875\n",
            "Epoch: 8061  Training Loss: 1650.66564941 Validation Loss: 1555.3636474609375 Test Loss: 1786.6226806640625\n",
            "Epoch: 8066  Training Loss: 1650.57861328 Validation Loss: 1555.329833984375 Test Loss: 1786.5347900390625\n",
            "Epoch: 8071  Training Loss: 1650.49157715 Validation Loss: 1555.2958984375 Test Loss: 1786.44677734375\n",
            "Epoch: 8076  Training Loss: 1650.40466309 Validation Loss: 1555.261962890625 Test Loss: 1786.359130859375\n",
            "Epoch: 8081  Training Loss: 1650.31774902 Validation Loss: 1555.2281494140625 Test Loss: 1786.271240234375\n",
            "Epoch: 8086  Training Loss: 1650.23059082 Validation Loss: 1555.194091796875 Test Loss: 1786.1832275390625\n",
            "Epoch: 8091  Training Loss: 1650.14404297 Validation Loss: 1555.16015625 Test Loss: 1786.0953369140625\n",
            "Epoch: 8096  Training Loss: 1650.05712891 Validation Loss: 1555.1259765625 Test Loss: 1786.0076904296875\n",
            "Epoch: 8101  Training Loss: 1649.97033691 Validation Loss: 1555.0919189453125 Test Loss: 1785.9197998046875\n",
            "Epoch: 8106  Training Loss: 1649.88354492 Validation Loss: 1555.0577392578125 Test Loss: 1785.832275390625\n",
            "Epoch: 8111  Training Loss: 1649.79675293 Validation Loss: 1555.0235595703125 Test Loss: 1785.7445068359375\n",
            "Epoch: 8116  Training Loss: 1649.71032715 Validation Loss: 1554.9893798828125 Test Loss: 1785.6568603515625\n",
            "Epoch: 8121  Training Loss: 1649.62365723 Validation Loss: 1554.9552001953125 Test Loss: 1785.5693359375\n",
            "Epoch: 8126  Training Loss: 1649.53710938 Validation Loss: 1554.9207763671875 Test Loss: 1785.4815673828125\n",
            "Epoch: 8131  Training Loss: 1649.45043945 Validation Loss: 1554.8865966796875 Test Loss: 1785.3941650390625\n",
            "Epoch: 8136  Training Loss: 1649.36389160 Validation Loss: 1554.8521728515625 Test Loss: 1785.3062744140625\n",
            "Epoch: 8141  Training Loss: 1649.27758789 Validation Loss: 1554.81787109375 Test Loss: 1785.2188720703125\n",
            "Epoch: 8146  Training Loss: 1649.19116211 Validation Loss: 1554.7833251953125 Test Loss: 1785.1314697265625\n",
            "Epoch: 8151  Training Loss: 1649.10461426 Validation Loss: 1554.7490234375 Test Loss: 1785.0440673828125\n",
            "Epoch: 8156  Training Loss: 1649.01843262 Validation Loss: 1554.7144775390625 Test Loss: 1784.9566650390625\n",
            "Epoch: 8161  Training Loss: 1648.93212891 Validation Loss: 1554.679931640625 Test Loss: 1784.869140625\n",
            "Epoch: 8166  Training Loss: 1648.84582520 Validation Loss: 1554.6456298828125 Test Loss: 1784.78173828125\n",
            "Epoch: 8171  Training Loss: 1648.75952148 Validation Loss: 1554.61083984375 Test Loss: 1784.6944580078125\n",
            "Epoch: 8176  Training Loss: 1648.67321777 Validation Loss: 1554.5760498046875 Test Loss: 1784.607177734375\n",
            "Epoch: 8181  Training Loss: 1648.58728027 Validation Loss: 1554.54150390625 Test Loss: 1784.519775390625\n",
            "Epoch: 8186  Training Loss: 1648.50134277 Validation Loss: 1554.5068359375 Test Loss: 1784.4326171875\n",
            "Epoch: 8191  Training Loss: 1648.41503906 Validation Loss: 1554.4720458984375 Test Loss: 1784.345458984375\n",
            "Epoch: 8196  Training Loss: 1648.32910156 Validation Loss: 1554.437255859375 Test Loss: 1784.25830078125\n",
            "Epoch: 8201  Training Loss: 1648.24316406 Validation Loss: 1554.4027099609375 Test Loss: 1784.171142578125\n",
            "Epoch: 8206  Training Loss: 1648.15710449 Validation Loss: 1554.3677978515625 Test Loss: 1784.0838623046875\n",
            "Epoch: 8211  Training Loss: 1648.07116699 Validation Loss: 1554.3328857421875 Test Loss: 1783.9967041015625\n",
            "Epoch: 8216  Training Loss: 1647.98522949 Validation Loss: 1554.2979736328125 Test Loss: 1783.9097900390625\n",
            "Epoch: 8221  Training Loss: 1647.89929199 Validation Loss: 1554.2630615234375 Test Loss: 1783.8226318359375\n",
            "Epoch: 8226  Training Loss: 1647.81359863 Validation Loss: 1554.2281494140625 Test Loss: 1783.7357177734375\n",
            "Epoch: 8231  Training Loss: 1647.72790527 Validation Loss: 1554.1929931640625 Test Loss: 1783.648681640625\n",
            "Epoch: 8236  Training Loss: 1647.64208984 Validation Loss: 1554.1580810546875 Test Loss: 1783.5616455078125\n",
            "Epoch: 8241  Training Loss: 1647.55639648 Validation Loss: 1554.1229248046875 Test Loss: 1783.474609375\n",
            "Epoch: 8246  Training Loss: 1647.47082520 Validation Loss: 1554.0880126953125 Test Loss: 1783.387939453125\n",
            "Epoch: 8251  Training Loss: 1647.38537598 Validation Loss: 1554.052734375 Test Loss: 1783.301025390625\n",
            "Epoch: 8256  Training Loss: 1647.29968262 Validation Loss: 1554.017822265625 Test Loss: 1783.2139892578125\n",
            "Epoch: 8261  Training Loss: 1647.21423340 Validation Loss: 1553.9825439453125 Test Loss: 1783.127197265625\n",
            "Epoch: 8266  Training Loss: 1647.12866211 Validation Loss: 1553.9473876953125 Test Loss: 1783.0404052734375\n",
            "Epoch: 8271  Training Loss: 1647.04333496 Validation Loss: 1553.912109375 Test Loss: 1782.9537353515625\n",
            "Epoch: 8276  Training Loss: 1646.95788574 Validation Loss: 1553.8768310546875 Test Loss: 1782.8670654296875\n",
            "Epoch: 8281  Training Loss: 1646.87255859 Validation Loss: 1553.841552734375 Test Loss: 1782.7803955078125\n",
            "Epoch: 8286  Training Loss: 1646.78723145 Validation Loss: 1553.8062744140625 Test Loss: 1782.6937255859375\n",
            "Epoch: 8291  Training Loss: 1646.70190430 Validation Loss: 1553.77099609375 Test Loss: 1782.6070556640625\n",
            "Epoch: 8296  Training Loss: 1646.61682129 Validation Loss: 1553.7357177734375 Test Loss: 1782.5203857421875\n",
            "Epoch: 8301  Training Loss: 1646.53149414 Validation Loss: 1553.7001953125 Test Loss: 1782.4339599609375\n",
            "Epoch: 8306  Training Loss: 1646.44641113 Validation Loss: 1553.6646728515625 Test Loss: 1782.347412109375\n",
            "Epoch: 8311  Training Loss: 1646.36120605 Validation Loss: 1553.6292724609375 Test Loss: 1782.2608642578125\n",
            "Epoch: 8316  Training Loss: 1646.27624512 Validation Loss: 1553.59375 Test Loss: 1782.1744384765625\n",
            "Epoch: 8321  Training Loss: 1646.19128418 Validation Loss: 1553.55810546875 Test Loss: 1782.0880126953125\n",
            "Epoch: 8326  Training Loss: 1646.10607910 Validation Loss: 1553.5224609375 Test Loss: 1782.00146484375\n",
            "Epoch: 8331  Training Loss: 1646.02111816 Validation Loss: 1553.4869384765625 Test Loss: 1781.9150390625\n",
            "Epoch: 8336  Training Loss: 1645.93640137 Validation Loss: 1553.4512939453125 Test Loss: 1781.8287353515625\n",
            "Epoch: 8341  Training Loss: 1645.85131836 Validation Loss: 1553.4156494140625 Test Loss: 1781.74267578125\n",
            "Epoch: 8346  Training Loss: 1645.76660156 Validation Loss: 1553.3800048828125 Test Loss: 1781.65625\n",
            "Epoch: 8351  Training Loss: 1645.68188477 Validation Loss: 1553.3443603515625 Test Loss: 1781.570068359375\n",
            "Epoch: 8356  Training Loss: 1645.59716797 Validation Loss: 1553.3084716796875 Test Loss: 1781.483642578125\n",
            "Epoch: 8361  Training Loss: 1645.51232910 Validation Loss: 1553.2728271484375 Test Loss: 1781.3974609375\n",
            "Epoch: 8366  Training Loss: 1645.42761230 Validation Loss: 1553.2371826171875 Test Loss: 1781.311279296875\n",
            "Epoch: 8371  Training Loss: 1645.34301758 Validation Loss: 1553.2010498046875 Test Loss: 1781.22509765625\n",
            "Epoch: 8376  Training Loss: 1645.25854492 Validation Loss: 1553.1654052734375 Test Loss: 1781.13916015625\n",
            "Epoch: 8381  Training Loss: 1645.17382812 Validation Loss: 1553.12939453125 Test Loss: 1781.0531005859375\n",
            "Epoch: 8386  Training Loss: 1645.08935547 Validation Loss: 1553.093505859375 Test Loss: 1780.967041015625\n",
            "Epoch: 8391  Training Loss: 1645.00488281 Validation Loss: 1553.0577392578125 Test Loss: 1780.880859375\n",
            "Epoch: 8396  Training Loss: 1644.92041016 Validation Loss: 1553.0218505859375 Test Loss: 1780.795166015625\n",
            "Epoch: 8401  Training Loss: 1644.83618164 Validation Loss: 1552.98583984375 Test Loss: 1780.709228515625\n",
            "Epoch: 8406  Training Loss: 1644.75158691 Validation Loss: 1552.9495849609375 Test Loss: 1780.623291015625\n",
            "Epoch: 8411  Training Loss: 1644.66735840 Validation Loss: 1552.913818359375 Test Loss: 1780.5374755859375\n",
            "Epoch: 8416  Training Loss: 1644.58312988 Validation Loss: 1552.8775634765625 Test Loss: 1780.4515380859375\n",
            "Epoch: 8421  Training Loss: 1644.49890137 Validation Loss: 1552.841552734375 Test Loss: 1780.3656005859375\n",
            "Epoch: 8426  Training Loss: 1644.41467285 Validation Loss: 1552.8055419921875 Test Loss: 1780.2799072265625\n",
            "Epoch: 8431  Training Loss: 1644.33056641 Validation Loss: 1552.769287109375 Test Loss: 1780.194091796875\n",
            "Epoch: 8436  Training Loss: 1644.24633789 Validation Loss: 1552.7332763671875 Test Loss: 1780.1085205078125\n",
            "Epoch: 8441  Training Loss: 1644.16235352 Validation Loss: 1552.6968994140625 Test Loss: 1780.022705078125\n",
            "Epoch: 8446  Training Loss: 1644.07836914 Validation Loss: 1552.660888671875 Test Loss: 1779.937255859375\n",
            "Epoch: 8451  Training Loss: 1643.99426270 Validation Loss: 1552.6246337890625 Test Loss: 1779.8514404296875\n",
            "Epoch: 8456  Training Loss: 1643.91040039 Validation Loss: 1552.5882568359375 Test Loss: 1779.765869140625\n",
            "Epoch: 8461  Training Loss: 1643.82641602 Validation Loss: 1552.552001953125 Test Loss: 1779.6802978515625\n",
            "Epoch: 8466  Training Loss: 1643.74267578 Validation Loss: 1552.515869140625 Test Loss: 1779.594970703125\n",
            "Epoch: 8471  Training Loss: 1643.65881348 Validation Loss: 1552.4793701171875 Test Loss: 1779.5093994140625\n",
            "Epoch: 8476  Training Loss: 1643.57495117 Validation Loss: 1552.4432373046875 Test Loss: 1779.423828125\n",
            "Epoch: 8481  Training Loss: 1643.49108887 Validation Loss: 1552.4066162109375 Test Loss: 1779.3385009765625\n",
            "Epoch: 8486  Training Loss: 1643.40747070 Validation Loss: 1552.3702392578125 Test Loss: 1779.2529296875\n",
            "Epoch: 8491  Training Loss: 1643.32385254 Validation Loss: 1552.3338623046875 Test Loss: 1779.167724609375\n",
            "Epoch: 8496  Training Loss: 1643.24011230 Validation Loss: 1552.2974853515625 Test Loss: 1779.0823974609375\n",
            "Epoch: 8501  Training Loss: 1643.15661621 Validation Loss: 1552.260986328125 Test Loss: 1778.9970703125\n",
            "Epoch: 8506  Training Loss: 1643.07312012 Validation Loss: 1552.2244873046875 Test Loss: 1778.9117431640625\n",
            "Epoch: 8511  Training Loss: 1642.98950195 Validation Loss: 1552.1881103515625 Test Loss: 1778.82666015625\n",
            "Epoch: 8516  Training Loss: 1642.90612793 Validation Loss: 1552.1514892578125 Test Loss: 1778.7415771484375\n",
            "Epoch: 8521  Training Loss: 1642.82263184 Validation Loss: 1552.1148681640625 Test Loss: 1778.6563720703125\n",
            "Epoch: 8526  Training Loss: 1642.73937988 Validation Loss: 1552.078369140625 Test Loss: 1778.571044921875\n",
            "Epoch: 8531  Training Loss: 1642.65588379 Validation Loss: 1552.041748046875 Test Loss: 1778.486083984375\n",
            "Epoch: 8536  Training Loss: 1642.57263184 Validation Loss: 1552.005126953125 Test Loss: 1778.401123046875\n",
            "Epoch: 8541  Training Loss: 1642.48950195 Validation Loss: 1551.9683837890625 Test Loss: 1778.3160400390625\n",
            "Epoch: 8546  Training Loss: 1642.40612793 Validation Loss: 1551.9317626953125 Test Loss: 1778.2310791015625\n",
            "Epoch: 8551  Training Loss: 1642.32299805 Validation Loss: 1551.895263671875 Test Loss: 1778.1461181640625\n",
            "Epoch: 8556  Training Loss: 1642.23986816 Validation Loss: 1551.8582763671875 Test Loss: 1778.061279296875\n",
            "Epoch: 8561  Training Loss: 1642.15686035 Validation Loss: 1551.8216552734375 Test Loss: 1777.9764404296875\n",
            "Epoch: 8566  Training Loss: 1642.07360840 Validation Loss: 1551.784912109375 Test Loss: 1777.8914794921875\n",
            "Epoch: 8571  Training Loss: 1641.99060059 Validation Loss: 1551.7479248046875 Test Loss: 1777.806640625\n",
            "Epoch: 8576  Training Loss: 1641.90759277 Validation Loss: 1551.71142578125 Test Loss: 1777.7220458984375\n",
            "Epoch: 8581  Training Loss: 1641.82482910 Validation Loss: 1551.6744384765625 Test Loss: 1777.63720703125\n",
            "Epoch: 8586  Training Loss: 1641.74182129 Validation Loss: 1551.6376953125 Test Loss: 1777.552490234375\n",
            "Epoch: 8591  Training Loss: 1641.65881348 Validation Loss: 1551.6007080078125 Test Loss: 1777.4677734375\n",
            "Epoch: 8596  Training Loss: 1641.57617188 Validation Loss: 1551.56396484375 Test Loss: 1777.383056640625\n",
            "Epoch: 8601  Training Loss: 1641.49340820 Validation Loss: 1551.52685546875 Test Loss: 1777.298583984375\n",
            "Epoch: 8606  Training Loss: 1641.41064453 Validation Loss: 1551.4901123046875 Test Loss: 1777.2139892578125\n",
            "Epoch: 8611  Training Loss: 1641.32788086 Validation Loss: 1551.452880859375 Test Loss: 1777.1292724609375\n",
            "Epoch: 8616  Training Loss: 1641.24523926 Validation Loss: 1551.4161376953125 Test Loss: 1777.0447998046875\n",
            "Epoch: 8621  Training Loss: 1641.16271973 Validation Loss: 1551.3792724609375 Test Loss: 1776.9603271484375\n",
            "Epoch: 8626  Training Loss: 1641.08007812 Validation Loss: 1551.342041015625 Test Loss: 1776.8759765625\n",
            "Epoch: 8631  Training Loss: 1640.99755859 Validation Loss: 1551.30517578125 Test Loss: 1776.7916259765625\n",
            "Epoch: 8636  Training Loss: 1640.91503906 Validation Loss: 1551.26806640625 Test Loss: 1776.707275390625\n",
            "Epoch: 8641  Training Loss: 1640.83251953 Validation Loss: 1551.2308349609375 Test Loss: 1776.6229248046875\n",
            "Epoch: 8646  Training Loss: 1640.75012207 Validation Loss: 1551.1939697265625 Test Loss: 1776.53857421875\n",
            "Epoch: 8651  Training Loss: 1640.66760254 Validation Loss: 1551.156982421875 Test Loss: 1776.454345703125\n",
            "Epoch: 8656  Training Loss: 1640.58544922 Validation Loss: 1551.119873046875 Test Loss: 1776.3701171875\n",
            "Epoch: 8661  Training Loss: 1640.50317383 Validation Loss: 1551.082763671875 Test Loss: 1776.2860107421875\n",
            "Epoch: 8666  Training Loss: 1640.42077637 Validation Loss: 1551.04541015625 Test Loss: 1776.2017822265625\n",
            "Epoch: 8671  Training Loss: 1640.33862305 Validation Loss: 1551.00830078125 Test Loss: 1776.11767578125\n",
            "Epoch: 8676  Training Loss: 1640.25634766 Validation Loss: 1550.97119140625 Test Loss: 1776.033447265625\n",
            "Epoch: 8681  Training Loss: 1640.17419434 Validation Loss: 1550.9339599609375 Test Loss: 1775.949462890625\n",
            "Epoch: 8686  Training Loss: 1640.09216309 Validation Loss: 1550.896728515625 Test Loss: 1775.8656005859375\n",
            "Epoch: 8691  Training Loss: 1640.01013184 Validation Loss: 1550.859619140625 Test Loss: 1775.781494140625\n",
            "Epoch: 8696  Training Loss: 1639.92810059 Validation Loss: 1550.822265625 Test Loss: 1775.697509765625\n",
            "Epoch: 8701  Training Loss: 1639.84606934 Validation Loss: 1550.784912109375 Test Loss: 1775.6136474609375\n",
            "Epoch: 8706  Training Loss: 1639.76428223 Validation Loss: 1550.747802734375 Test Loss: 1775.5299072265625\n",
            "Epoch: 8711  Training Loss: 1639.68237305 Validation Loss: 1550.7103271484375 Test Loss: 1775.446044921875\n",
            "Epoch: 8716  Training Loss: 1639.60058594 Validation Loss: 1550.6732177734375 Test Loss: 1775.3621826171875\n",
            "Epoch: 8721  Training Loss: 1639.51867676 Validation Loss: 1550.6358642578125 Test Loss: 1775.2783203125\n",
            "Epoch: 8726  Training Loss: 1639.43701172 Validation Loss: 1550.598388671875 Test Loss: 1775.1947021484375\n",
            "Epoch: 8731  Training Loss: 1639.35510254 Validation Loss: 1550.560791015625 Test Loss: 1775.1109619140625\n",
            "Epoch: 8736  Training Loss: 1639.27343750 Validation Loss: 1550.523681640625 Test Loss: 1775.0274658203125\n",
            "Epoch: 8741  Training Loss: 1639.19177246 Validation Loss: 1550.4862060546875 Test Loss: 1774.9437255859375\n",
            "Epoch: 8746  Training Loss: 1639.11022949 Validation Loss: 1550.44873046875 Test Loss: 1774.860107421875\n",
            "Epoch: 8751  Training Loss: 1639.02856445 Validation Loss: 1550.411376953125 Test Loss: 1774.7764892578125\n",
            "Epoch: 8756  Training Loss: 1638.94702148 Validation Loss: 1550.3739013671875 Test Loss: 1774.6929931640625\n",
            "Epoch: 8761  Training Loss: 1638.86560059 Validation Loss: 1550.3365478515625 Test Loss: 1774.609619140625\n",
            "Epoch: 8766  Training Loss: 1638.78417969 Validation Loss: 1550.2989501953125 Test Loss: 1774.526123046875\n",
            "Epoch: 8771  Training Loss: 1638.70275879 Validation Loss: 1550.2613525390625 Test Loss: 1774.442626953125\n",
            "Epoch: 8776  Training Loss: 1638.62121582 Validation Loss: 1550.2237548828125 Test Loss: 1774.3592529296875\n",
            "Epoch: 8781  Training Loss: 1638.54003906 Validation Loss: 1550.1864013671875 Test Loss: 1774.2760009765625\n",
            "Epoch: 8786  Training Loss: 1638.45886230 Validation Loss: 1550.14892578125 Test Loss: 1774.1927490234375\n",
            "Epoch: 8791  Training Loss: 1638.37744141 Validation Loss: 1550.111083984375 Test Loss: 1774.109375\n",
            "Epoch: 8796  Training Loss: 1638.29626465 Validation Loss: 1550.073486328125 Test Loss: 1774.0262451171875\n",
            "Epoch: 8801  Training Loss: 1638.21520996 Validation Loss: 1550.0360107421875 Test Loss: 1773.943115234375\n",
            "Epoch: 8806  Training Loss: 1638.13403320 Validation Loss: 1549.998291015625 Test Loss: 1773.85986328125\n",
            "Epoch: 8811  Training Loss: 1638.05285645 Validation Loss: 1549.9608154296875 Test Loss: 1773.77685546875\n",
            "Epoch: 8816  Training Loss: 1637.97180176 Validation Loss: 1549.9232177734375 Test Loss: 1773.693603515625\n",
            "Epoch: 8821  Training Loss: 1637.89074707 Validation Loss: 1549.885498046875 Test Loss: 1773.610595703125\n",
            "Epoch: 8826  Training Loss: 1637.80981445 Validation Loss: 1549.8477783203125 Test Loss: 1773.5277099609375\n",
            "Epoch: 8831  Training Loss: 1637.72900391 Validation Loss: 1549.81005859375 Test Loss: 1773.444580078125\n",
            "Epoch: 8836  Training Loss: 1637.64794922 Validation Loss: 1549.7724609375 Test Loss: 1773.3616943359375\n",
            "Epoch: 8841  Training Loss: 1637.56713867 Validation Loss: 1549.7347412109375 Test Loss: 1773.2786865234375\n",
            "Epoch: 8846  Training Loss: 1637.48632812 Validation Loss: 1549.697021484375 Test Loss: 1773.196044921875\n",
            "Epoch: 8851  Training Loss: 1637.40551758 Validation Loss: 1549.6591796875 Test Loss: 1773.1131591796875\n",
            "Epoch: 8856  Training Loss: 1637.32495117 Validation Loss: 1549.6214599609375 Test Loss: 1773.0303955078125\n",
            "Epoch: 8861  Training Loss: 1637.24414062 Validation Loss: 1549.5838623046875 Test Loss: 1772.947509765625\n",
            "Epoch: 8866  Training Loss: 1637.16357422 Validation Loss: 1549.5458984375 Test Loss: 1772.8648681640625\n",
            "Epoch: 8871  Training Loss: 1637.08300781 Validation Loss: 1549.508056640625 Test Loss: 1772.7821044921875\n",
            "Epoch: 8876  Training Loss: 1637.00231934 Validation Loss: 1549.47021484375 Test Loss: 1772.6995849609375\n",
            "Epoch: 8881  Training Loss: 1636.92175293 Validation Loss: 1549.4324951171875 Test Loss: 1772.616943359375\n",
            "Epoch: 8886  Training Loss: 1636.84143066 Validation Loss: 1549.3946533203125 Test Loss: 1772.5345458984375\n",
            "Epoch: 8891  Training Loss: 1636.76086426 Validation Loss: 1549.3568115234375 Test Loss: 1772.451904296875\n",
            "Epoch: 8896  Training Loss: 1636.68041992 Validation Loss: 1549.3189697265625 Test Loss: 1772.369384765625\n",
            "Epoch: 8901  Training Loss: 1636.60009766 Validation Loss: 1549.281005859375 Test Loss: 1772.2869873046875\n",
            "Epoch: 8906  Training Loss: 1636.51965332 Validation Loss: 1549.2431640625 Test Loss: 1772.20458984375\n",
            "Epoch: 8911  Training Loss: 1636.43945312 Validation Loss: 1549.205322265625 Test Loss: 1772.1221923828125\n",
            "Epoch: 8916  Training Loss: 1636.35925293 Validation Loss: 1549.16748046875 Test Loss: 1772.039794921875\n",
            "Epoch: 8921  Training Loss: 1636.27905273 Validation Loss: 1549.129638671875 Test Loss: 1771.95751953125\n",
            "Epoch: 8926  Training Loss: 1636.19885254 Validation Loss: 1549.09130859375 Test Loss: 1771.8751220703125\n",
            "Epoch: 8931  Training Loss: 1636.11865234 Validation Loss: 1549.0537109375 Test Loss: 1771.7930908203125\n",
            "Epoch: 8936  Training Loss: 1636.03869629 Validation Loss: 1549.015625 Test Loss: 1771.7108154296875\n",
            "Epoch: 8941  Training Loss: 1635.95849609 Validation Loss: 1548.9775390625 Test Loss: 1771.6285400390625\n",
            "Epoch: 8946  Training Loss: 1635.87866211 Validation Loss: 1548.939697265625 Test Loss: 1771.54638671875\n",
            "Epoch: 8951  Training Loss: 1635.79858398 Validation Loss: 1548.901611328125 Test Loss: 1771.46435546875\n",
            "Epoch: 8956  Training Loss: 1635.71887207 Validation Loss: 1548.8636474609375 Test Loss: 1771.38232421875\n",
            "Epoch: 8961  Training Loss: 1635.63879395 Validation Loss: 1548.82568359375 Test Loss: 1771.30029296875\n",
            "Epoch: 8966  Training Loss: 1635.55908203 Validation Loss: 1548.7874755859375 Test Loss: 1771.21826171875\n",
            "Epoch: 8971  Training Loss: 1635.47900391 Validation Loss: 1548.7493896484375 Test Loss: 1771.136474609375\n",
            "Epoch: 8976  Training Loss: 1635.39929199 Validation Loss: 1548.71142578125 Test Loss: 1771.0545654296875\n",
            "Epoch: 8981  Training Loss: 1635.31970215 Validation Loss: 1548.67333984375 Test Loss: 1770.9725341796875\n",
            "Epoch: 8986  Training Loss: 1635.23999023 Validation Loss: 1548.63525390625 Test Loss: 1770.890869140625\n",
            "Epoch: 8991  Training Loss: 1635.16040039 Validation Loss: 1548.5972900390625 Test Loss: 1770.80908203125\n",
            "Epoch: 8996  Training Loss: 1635.08068848 Validation Loss: 1548.5589599609375 Test Loss: 1770.727294921875\n",
            "Epoch: 9001  Training Loss: 1635.00109863 Validation Loss: 1548.52099609375 Test Loss: 1770.645751953125\n",
            "Epoch: 9006  Training Loss: 1634.92163086 Validation Loss: 1548.48291015625 Test Loss: 1770.56396484375\n",
            "Epoch: 9011  Training Loss: 1634.84204102 Validation Loss: 1548.4447021484375 Test Loss: 1770.4822998046875\n",
            "Epoch: 9016  Training Loss: 1634.76257324 Validation Loss: 1548.4066162109375 Test Loss: 1770.400634765625\n",
            "Epoch: 9021  Training Loss: 1634.68322754 Validation Loss: 1548.368408203125 Test Loss: 1770.3193359375\n",
            "Epoch: 9026  Training Loss: 1634.60388184 Validation Loss: 1548.330322265625 Test Loss: 1770.2376708984375\n",
            "Epoch: 9031  Training Loss: 1634.52465820 Validation Loss: 1548.292236328125 Test Loss: 1770.15625\n",
            "Epoch: 9036  Training Loss: 1634.44519043 Validation Loss: 1548.25390625 Test Loss: 1770.0745849609375\n",
            "Epoch: 9041  Training Loss: 1634.36596680 Validation Loss: 1548.2158203125 Test Loss: 1769.993408203125\n",
            "Epoch: 9046  Training Loss: 1634.28674316 Validation Loss: 1548.1776123046875 Test Loss: 1769.911865234375\n",
            "Epoch: 9051  Training Loss: 1634.20764160 Validation Loss: 1548.1392822265625 Test Loss: 1769.8306884765625\n",
            "Epoch: 9056  Training Loss: 1634.12841797 Validation Loss: 1548.10107421875 Test Loss: 1769.7493896484375\n",
            "Epoch: 9061  Training Loss: 1634.04919434 Validation Loss: 1548.0628662109375 Test Loss: 1769.668212890625\n",
            "Epoch: 9066  Training Loss: 1633.97009277 Validation Loss: 1548.024658203125 Test Loss: 1769.5869140625\n",
            "Epoch: 9071  Training Loss: 1633.89135742 Validation Loss: 1547.9862060546875 Test Loss: 1769.505615234375\n",
            "Epoch: 9076  Training Loss: 1633.81237793 Validation Loss: 1547.947998046875 Test Loss: 1769.424560546875\n",
            "Epoch: 9081  Training Loss: 1633.73339844 Validation Loss: 1547.909912109375 Test Loss: 1769.343505859375\n",
            "Epoch: 9086  Training Loss: 1633.65454102 Validation Loss: 1547.8717041015625 Test Loss: 1769.2623291015625\n",
            "Epoch: 9091  Training Loss: 1633.57568359 Validation Loss: 1547.833251953125 Test Loss: 1769.181396484375\n",
            "Epoch: 9096  Training Loss: 1633.49682617 Validation Loss: 1547.7950439453125 Test Loss: 1769.100341796875\n",
            "Epoch: 9101  Training Loss: 1633.41796875 Validation Loss: 1547.7568359375 Test Loss: 1769.0194091796875\n",
            "Epoch: 9106  Training Loss: 1633.33935547 Validation Loss: 1547.7183837890625 Test Loss: 1768.9384765625\n",
            "Epoch: 9111  Training Loss: 1633.26062012 Validation Loss: 1547.68017578125 Test Loss: 1768.8577880859375\n",
            "Epoch: 9116  Training Loss: 1633.18212891 Validation Loss: 1547.6416015625 Test Loss: 1768.77685546875\n",
            "Epoch: 9121  Training Loss: 1633.10339355 Validation Loss: 1547.603515625 Test Loss: 1768.696044921875\n",
            "Epoch: 9126  Training Loss: 1633.02478027 Validation Loss: 1547.5650634765625 Test Loss: 1768.615234375\n",
            "Epoch: 9131  Training Loss: 1632.94616699 Validation Loss: 1547.526611328125 Test Loss: 1768.5347900390625\n",
            "Epoch: 9136  Training Loss: 1632.86779785 Validation Loss: 1547.48828125 Test Loss: 1768.453857421875\n",
            "Epoch: 9141  Training Loss: 1632.78918457 Validation Loss: 1547.4498291015625 Test Loss: 1768.373291015625\n",
            "Epoch: 9146  Training Loss: 1632.71081543 Validation Loss: 1547.41162109375 Test Loss: 1768.292724609375\n",
            "Epoch: 9151  Training Loss: 1632.63256836 Validation Loss: 1547.3729248046875 Test Loss: 1768.212158203125\n",
            "Epoch: 9156  Training Loss: 1632.55407715 Validation Loss: 1547.334716796875 Test Loss: 1768.1314697265625\n",
            "Epoch: 9161  Training Loss: 1632.47570801 Validation Loss: 1547.2962646484375 Test Loss: 1768.0511474609375\n",
            "Epoch: 9166  Training Loss: 1632.39758301 Validation Loss: 1547.2579345703125 Test Loss: 1767.970458984375\n",
            "Epoch: 9171  Training Loss: 1632.31921387 Validation Loss: 1547.219482421875 Test Loss: 1767.89013671875\n",
            "Epoch: 9176  Training Loss: 1632.24108887 Validation Loss: 1547.1810302734375 Test Loss: 1767.809814453125\n",
            "Epoch: 9181  Training Loss: 1632.16284180 Validation Loss: 1547.1427001953125 Test Loss: 1767.7296142578125\n",
            "Epoch: 9186  Training Loss: 1632.08483887 Validation Loss: 1547.10400390625 Test Loss: 1767.649169921875\n",
            "Epoch: 9191  Training Loss: 1632.00671387 Validation Loss: 1547.0657958984375 Test Loss: 1767.56884765625\n",
            "Epoch: 9196  Training Loss: 1631.92883301 Validation Loss: 1547.0272216796875 Test Loss: 1767.4886474609375\n",
            "Epoch: 9201  Training Loss: 1631.85070801 Validation Loss: 1546.9886474609375 Test Loss: 1767.4085693359375\n",
            "Epoch: 9206  Training Loss: 1631.77282715 Validation Loss: 1546.9503173828125 Test Loss: 1767.328369140625\n",
            "Epoch: 9211  Training Loss: 1631.69482422 Validation Loss: 1546.9117431640625 Test Loss: 1767.2484130859375\n",
            "Epoch: 9216  Training Loss: 1631.61694336 Validation Loss: 1546.87353515625 Test Loss: 1767.1683349609375\n",
            "Epoch: 9221  Training Loss: 1631.53918457 Validation Loss: 1546.834716796875 Test Loss: 1767.0882568359375\n",
            "Epoch: 9226  Training Loss: 1631.46142578 Validation Loss: 1546.7962646484375 Test Loss: 1767.0084228515625\n",
            "Epoch: 9231  Training Loss: 1631.38366699 Validation Loss: 1546.7576904296875 Test Loss: 1766.9283447265625\n",
            "Epoch: 9236  Training Loss: 1631.30590820 Validation Loss: 1546.719482421875 Test Loss: 1766.8485107421875\n",
            "Epoch: 9241  Training Loss: 1631.22814941 Validation Loss: 1546.680908203125 Test Loss: 1766.7685546875\n",
            "Epoch: 9246  Training Loss: 1631.15063477 Validation Loss: 1546.642333984375 Test Loss: 1766.688720703125\n",
            "Epoch: 9251  Training Loss: 1631.07312012 Validation Loss: 1546.6036376953125 Test Loss: 1766.60888671875\n",
            "Epoch: 9256  Training Loss: 1630.99536133 Validation Loss: 1546.5650634765625 Test Loss: 1766.5291748046875\n",
            "Epoch: 9261  Training Loss: 1630.91796875 Validation Loss: 1546.5264892578125 Test Loss: 1766.449462890625\n",
            "Epoch: 9266  Training Loss: 1630.84033203 Validation Loss: 1546.488037109375 Test Loss: 1766.369873046875\n",
            "Epoch: 9271  Training Loss: 1630.76318359 Validation Loss: 1546.449462890625 Test Loss: 1766.2900390625\n",
            "Epoch: 9276  Training Loss: 1630.68566895 Validation Loss: 1546.410888671875 Test Loss: 1766.210693359375\n",
            "Epoch: 9281  Training Loss: 1630.60827637 Validation Loss: 1546.3721923828125 Test Loss: 1766.131103515625\n",
            "Epoch: 9286  Training Loss: 1630.53100586 Validation Loss: 1546.3336181640625 Test Loss: 1766.0516357421875\n",
            "Epoch: 9291  Training Loss: 1630.45373535 Validation Loss: 1546.295166015625 Test Loss: 1765.97216796875\n",
            "Epoch: 9296  Training Loss: 1630.37634277 Validation Loss: 1546.256591796875 Test Loss: 1765.8927001953125\n",
            "Epoch: 9301  Training Loss: 1630.29919434 Validation Loss: 1546.218017578125 Test Loss: 1765.8131103515625\n",
            "Epoch: 9306  Training Loss: 1630.22216797 Validation Loss: 1546.179443359375 Test Loss: 1765.7337646484375\n",
            "Epoch: 9311  Training Loss: 1630.14489746 Validation Loss: 1546.140625 Test Loss: 1765.654541015625\n",
            "Epoch: 9316  Training Loss: 1630.06787109 Validation Loss: 1546.10205078125 Test Loss: 1765.5753173828125\n",
            "Epoch: 9321  Training Loss: 1629.99084473 Validation Loss: 1546.0634765625 Test Loss: 1765.4962158203125\n",
            "Epoch: 9326  Training Loss: 1629.91369629 Validation Loss: 1546.0247802734375 Test Loss: 1765.416748046875\n",
            "Epoch: 9331  Training Loss: 1629.83679199 Validation Loss: 1545.9862060546875 Test Loss: 1765.337646484375\n",
            "Epoch: 9336  Training Loss: 1629.75988770 Validation Loss: 1545.9476318359375 Test Loss: 1765.258544921875\n",
            "Epoch: 9341  Training Loss: 1629.68286133 Validation Loss: 1545.9090576171875 Test Loss: 1765.179443359375\n",
            "Epoch: 9346  Training Loss: 1629.60595703 Validation Loss: 1545.8702392578125 Test Loss: 1765.1002197265625\n",
            "Epoch: 9351  Training Loss: 1629.52941895 Validation Loss: 1545.8316650390625 Test Loss: 1765.0213623046875\n",
            "Epoch: 9356  Training Loss: 1629.45239258 Validation Loss: 1545.7928466796875 Test Loss: 1764.9425048828125\n",
            "Epoch: 9361  Training Loss: 1629.37561035 Validation Loss: 1545.75439453125 Test Loss: 1764.863525390625\n",
            "Epoch: 9366  Training Loss: 1629.29882812 Validation Loss: 1545.715576171875 Test Loss: 1764.7845458984375\n",
            "Epoch: 9371  Training Loss: 1629.22229004 Validation Loss: 1545.677001953125 Test Loss: 1764.705810546875\n",
            "Epoch: 9376  Training Loss: 1629.14562988 Validation Loss: 1545.63818359375 Test Loss: 1764.626953125\n",
            "Epoch: 9381  Training Loss: 1629.06909180 Validation Loss: 1545.599609375 Test Loss: 1764.5482177734375\n",
            "Epoch: 9386  Training Loss: 1628.99243164 Validation Loss: 1545.56103515625 Test Loss: 1764.4693603515625\n",
            "Epoch: 9391  Training Loss: 1628.91601562 Validation Loss: 1545.522216796875 Test Loss: 1764.3907470703125\n",
            "Epoch: 9396  Training Loss: 1628.83959961 Validation Loss: 1545.483642578125 Test Loss: 1764.312255859375\n",
            "Epoch: 9401  Training Loss: 1628.76318359 Validation Loss: 1545.44482421875 Test Loss: 1764.2335205078125\n",
            "Epoch: 9406  Training Loss: 1628.68688965 Validation Loss: 1545.40625 Test Loss: 1764.1549072265625\n",
            "Epoch: 9411  Training Loss: 1628.61035156 Validation Loss: 1545.367431640625 Test Loss: 1764.076416015625\n",
            "Epoch: 9416  Training Loss: 1628.53417969 Validation Loss: 1545.3287353515625 Test Loss: 1763.997802734375\n",
            "Epoch: 9421  Training Loss: 1628.45764160 Validation Loss: 1545.2899169921875 Test Loss: 1763.9193115234375\n",
            "Epoch: 9426  Training Loss: 1628.38159180 Validation Loss: 1545.25146484375 Test Loss: 1763.8408203125\n",
            "Epoch: 9431  Training Loss: 1628.30517578 Validation Loss: 1545.212646484375 Test Loss: 1763.7625732421875\n",
            "Epoch: 9436  Training Loss: 1628.22912598 Validation Loss: 1545.1739501953125 Test Loss: 1763.68408203125\n",
            "Epoch: 9441  Training Loss: 1628.15283203 Validation Loss: 1545.1351318359375 Test Loss: 1763.60595703125\n",
            "Epoch: 9446  Training Loss: 1628.07690430 Validation Loss: 1545.0963134765625 Test Loss: 1763.527587890625\n",
            "Epoch: 9451  Training Loss: 1628.00085449 Validation Loss: 1545.0577392578125 Test Loss: 1763.4495849609375\n",
            "Epoch: 9456  Training Loss: 1627.92492676 Validation Loss: 1545.0189208984375 Test Loss: 1763.3712158203125\n",
            "Epoch: 9461  Training Loss: 1627.84875488 Validation Loss: 1544.97998046875 Test Loss: 1763.2928466796875\n",
            "Epoch: 9466  Training Loss: 1627.77294922 Validation Loss: 1544.94140625 Test Loss: 1763.2149658203125\n",
            "Epoch: 9471  Training Loss: 1627.69689941 Validation Loss: 1544.902587890625 Test Loss: 1763.1368408203125\n",
            "Epoch: 9476  Training Loss: 1627.62109375 Validation Loss: 1544.86376953125 Test Loss: 1763.0587158203125\n",
            "Epoch: 9481  Training Loss: 1627.54528809 Validation Loss: 1544.824951171875 Test Loss: 1762.9808349609375\n",
            "Epoch: 9486  Training Loss: 1627.46948242 Validation Loss: 1544.7862548828125 Test Loss: 1762.90283203125\n",
            "Epoch: 9491  Training Loss: 1627.39367676 Validation Loss: 1544.7474365234375 Test Loss: 1762.8248291015625\n",
            "Epoch: 9496  Training Loss: 1627.31811523 Validation Loss: 1544.7086181640625 Test Loss: 1762.7470703125\n",
            "Epoch: 9501  Training Loss: 1627.24230957 Validation Loss: 1544.6700439453125 Test Loss: 1762.669189453125\n",
            "Epoch: 9506  Training Loss: 1627.16662598 Validation Loss: 1544.6312255859375 Test Loss: 1762.5914306640625\n",
            "Epoch: 9511  Training Loss: 1627.09118652 Validation Loss: 1544.5924072265625 Test Loss: 1762.513671875\n",
            "Epoch: 9516  Training Loss: 1627.01562500 Validation Loss: 1544.5537109375 Test Loss: 1762.43603515625\n",
            "Epoch: 9521  Training Loss: 1626.93994141 Validation Loss: 1544.5146484375 Test Loss: 1762.358154296875\n",
            "Epoch: 9526  Training Loss: 1626.86450195 Validation Loss: 1544.4759521484375 Test Loss: 1762.2806396484375\n",
            "Epoch: 9531  Training Loss: 1626.78906250 Validation Loss: 1544.4371337890625 Test Loss: 1762.203125\n",
            "Epoch: 9536  Training Loss: 1626.71362305 Validation Loss: 1544.3983154296875 Test Loss: 1762.12548828125\n",
            "Epoch: 9541  Training Loss: 1626.63830566 Validation Loss: 1544.359619140625 Test Loss: 1762.0477294921875\n",
            "Epoch: 9546  Training Loss: 1626.56298828 Validation Loss: 1544.32080078125 Test Loss: 1761.970458984375\n",
            "Epoch: 9551  Training Loss: 1626.48767090 Validation Loss: 1544.281982421875 Test Loss: 1761.8929443359375\n",
            "Epoch: 9556  Training Loss: 1626.41235352 Validation Loss: 1544.2427978515625 Test Loss: 1761.8157958984375\n",
            "Epoch: 9561  Training Loss: 1626.33715820 Validation Loss: 1544.2042236328125 Test Loss: 1761.7381591796875\n",
            "Epoch: 9566  Training Loss: 1626.26196289 Validation Loss: 1544.1654052734375 Test Loss: 1761.6607666015625\n",
            "Epoch: 9571  Training Loss: 1626.18688965 Validation Loss: 1544.126708984375 Test Loss: 1761.5833740234375\n",
            "Epoch: 9576  Training Loss: 1626.11169434 Validation Loss: 1544.087646484375 Test Loss: 1761.50634765625\n",
            "Epoch: 9581  Training Loss: 1626.03662109 Validation Loss: 1544.0489501953125 Test Loss: 1761.428955078125\n",
            "Epoch: 9586  Training Loss: 1625.96166992 Validation Loss: 1544.0101318359375 Test Loss: 1761.351806640625\n",
            "Epoch: 9591  Training Loss: 1625.88647461 Validation Loss: 1543.97119140625 Test Loss: 1761.2747802734375\n",
            "Epoch: 9596  Training Loss: 1625.81152344 Validation Loss: 1543.932373046875 Test Loss: 1761.1978759765625\n",
            "Epoch: 9601  Training Loss: 1625.73669434 Validation Loss: 1543.8935546875 Test Loss: 1761.1207275390625\n",
            "Epoch: 9606  Training Loss: 1625.66174316 Validation Loss: 1543.854736328125 Test Loss: 1761.0435791015625\n",
            "Epoch: 9611  Training Loss: 1625.58691406 Validation Loss: 1543.8157958984375 Test Loss: 1760.9666748046875\n",
            "Epoch: 9616  Training Loss: 1625.51208496 Validation Loss: 1543.7769775390625 Test Loss: 1760.8897705078125\n",
            "Epoch: 9621  Training Loss: 1625.43737793 Validation Loss: 1543.73828125 Test Loss: 1760.812744140625\n",
            "Epoch: 9626  Training Loss: 1625.36254883 Validation Loss: 1543.69921875 Test Loss: 1760.7359619140625\n",
            "Epoch: 9631  Training Loss: 1625.28784180 Validation Loss: 1543.6605224609375 Test Loss: 1760.6591796875\n",
            "Epoch: 9636  Training Loss: 1625.21325684 Validation Loss: 1543.6217041015625 Test Loss: 1760.582275390625\n",
            "Epoch: 9641  Training Loss: 1625.13867188 Validation Loss: 1543.582763671875 Test Loss: 1760.505615234375\n",
            "Epoch: 9646  Training Loss: 1625.06396484 Validation Loss: 1543.5439453125 Test Loss: 1760.428955078125\n",
            "Epoch: 9651  Training Loss: 1624.98950195 Validation Loss: 1543.5050048828125 Test Loss: 1760.35205078125\n",
            "Epoch: 9656  Training Loss: 1624.91491699 Validation Loss: 1543.46630859375 Test Loss: 1760.2755126953125\n",
            "Epoch: 9661  Training Loss: 1624.84033203 Validation Loss: 1543.42724609375 Test Loss: 1760.198974609375\n",
            "Epoch: 9666  Training Loss: 1624.76574707 Validation Loss: 1543.388427734375 Test Loss: 1760.122314453125\n",
            "Epoch: 9671  Training Loss: 1624.69165039 Validation Loss: 1543.349609375 Test Loss: 1760.045654296875\n",
            "Epoch: 9676  Training Loss: 1624.61706543 Validation Loss: 1543.3106689453125 Test Loss: 1759.9693603515625\n",
            "Epoch: 9681  Training Loss: 1624.54260254 Validation Loss: 1543.2718505859375 Test Loss: 1759.8929443359375\n",
            "Epoch: 9686  Training Loss: 1624.46850586 Validation Loss: 1543.23291015625 Test Loss: 1759.8165283203125\n",
            "Epoch: 9691  Training Loss: 1624.39416504 Validation Loss: 1543.194091796875 Test Loss: 1759.7401123046875\n",
            "Epoch: 9696  Training Loss: 1624.32006836 Validation Loss: 1543.1551513671875 Test Loss: 1759.6639404296875\n",
            "Epoch: 9701  Training Loss: 1624.24572754 Validation Loss: 1543.1163330078125 Test Loss: 1759.5875244140625\n",
            "Epoch: 9706  Training Loss: 1624.17163086 Validation Loss: 1543.077392578125 Test Loss: 1759.51123046875\n",
            "Epoch: 9711  Training Loss: 1624.09741211 Validation Loss: 1543.03857421875 Test Loss: 1759.4351806640625\n",
            "Epoch: 9716  Training Loss: 1624.02331543 Validation Loss: 1542.999755859375 Test Loss: 1759.359130859375\n",
            "Epoch: 9721  Training Loss: 1623.94934082 Validation Loss: 1542.9608154296875 Test Loss: 1759.2828369140625\n",
            "Epoch: 9726  Training Loss: 1623.87536621 Validation Loss: 1542.921875 Test Loss: 1759.206787109375\n",
            "Epoch: 9731  Training Loss: 1623.80151367 Validation Loss: 1542.883056640625 Test Loss: 1759.1307373046875\n",
            "Epoch: 9736  Training Loss: 1623.72753906 Validation Loss: 1542.8443603515625 Test Loss: 1759.0545654296875\n",
            "Epoch: 9741  Training Loss: 1623.65368652 Validation Loss: 1542.80517578125 Test Loss: 1758.9786376953125\n",
            "Epoch: 9746  Training Loss: 1623.57971191 Validation Loss: 1542.766357421875 Test Loss: 1758.9027099609375\n",
            "Epoch: 9751  Training Loss: 1623.50585938 Validation Loss: 1542.7274169921875 Test Loss: 1758.826904296875\n",
            "Epoch: 9756  Training Loss: 1623.43212891 Validation Loss: 1542.6885986328125 Test Loss: 1758.7508544921875\n",
            "Epoch: 9761  Training Loss: 1623.35827637 Validation Loss: 1542.649658203125 Test Loss: 1758.6751708984375\n",
            "Epoch: 9766  Training Loss: 1623.28442383 Validation Loss: 1542.61083984375 Test Loss: 1758.5994873046875\n",
            "Epoch: 9771  Training Loss: 1623.21093750 Validation Loss: 1542.5718994140625 Test Loss: 1758.5235595703125\n",
            "Epoch: 9776  Training Loss: 1623.13732910 Validation Loss: 1542.532958984375 Test Loss: 1758.447998046875\n",
            "Epoch: 9781  Training Loss: 1623.06359863 Validation Loss: 1542.493896484375 Test Loss: 1758.3721923828125\n",
            "Epoch: 9786  Training Loss: 1622.98999023 Validation Loss: 1542.4549560546875 Test Loss: 1758.296630859375\n",
            "Epoch: 9791  Training Loss: 1622.91650391 Validation Loss: 1542.4161376953125 Test Loss: 1758.2210693359375\n",
            "Epoch: 9796  Training Loss: 1622.84301758 Validation Loss: 1542.377197265625 Test Loss: 1758.1453857421875\n",
            "Epoch: 9801  Training Loss: 1622.76953125 Validation Loss: 1542.33837890625 Test Loss: 1758.070068359375\n",
            "Epoch: 9806  Training Loss: 1622.69604492 Validation Loss: 1542.2994384765625 Test Loss: 1757.9935302734375\n",
            "Epoch: 9811  Training Loss: 1622.62268066 Validation Loss: 1542.260498046875 Test Loss: 1757.914794921875\n",
            "Epoch: 9816  Training Loss: 1622.54931641 Validation Loss: 1542.2215576171875 Test Loss: 1757.83642578125\n",
            "Epoch: 9821  Training Loss: 1622.47607422 Validation Loss: 1542.1824951171875 Test Loss: 1757.7576904296875\n",
            "Epoch: 9826  Training Loss: 1622.40270996 Validation Loss: 1542.143798828125 Test Loss: 1757.67919921875\n",
            "Epoch: 9831  Training Loss: 1622.32934570 Validation Loss: 1542.104736328125 Test Loss: 1757.6007080078125\n",
            "Epoch: 9836  Training Loss: 1622.25622559 Validation Loss: 1542.0660400390625 Test Loss: 1757.522216796875\n",
            "Epoch: 9841  Training Loss: 1622.18310547 Validation Loss: 1542.0269775390625 Test Loss: 1757.4439697265625\n",
            "Epoch: 9846  Training Loss: 1622.11010742 Validation Loss: 1541.9879150390625 Test Loss: 1757.3656005859375\n",
            "Epoch: 9851  Training Loss: 1622.03674316 Validation Loss: 1541.9490966796875 Test Loss: 1757.287109375\n",
            "Epoch: 9856  Training Loss: 1621.96374512 Validation Loss: 1541.91015625 Test Loss: 1757.2088623046875\n",
            "Epoch: 9861  Training Loss: 1621.89074707 Validation Loss: 1541.8712158203125 Test Loss: 1757.130615234375\n",
            "Epoch: 9866  Training Loss: 1621.81762695 Validation Loss: 1541.83203125 Test Loss: 1757.052490234375\n",
            "Epoch: 9871  Training Loss: 1621.74475098 Validation Loss: 1541.7933349609375 Test Loss: 1756.9739990234375\n",
            "Epoch: 9876  Training Loss: 1621.67175293 Validation Loss: 1541.75439453125 Test Loss: 1756.8958740234375\n",
            "Epoch: 9881  Training Loss: 1621.59875488 Validation Loss: 1541.715576171875 Test Loss: 1756.8177490234375\n",
            "Epoch: 9886  Training Loss: 1621.52587891 Validation Loss: 1541.676513671875 Test Loss: 1756.7398681640625\n",
            "Epoch: 9891  Training Loss: 1621.45324707 Validation Loss: 1541.637451171875 Test Loss: 1756.6617431640625\n",
            "Epoch: 9896  Training Loss: 1621.38037109 Validation Loss: 1541.598388671875 Test Loss: 1756.5838623046875\n",
            "Epoch: 9901  Training Loss: 1621.30761719 Validation Loss: 1541.5596923828125 Test Loss: 1756.505859375\n",
            "Epoch: 9906  Training Loss: 1621.23486328 Validation Loss: 1541.5206298828125 Test Loss: 1756.427978515625\n",
            "Epoch: 9911  Training Loss: 1621.16210938 Validation Loss: 1541.481689453125 Test Loss: 1756.3499755859375\n",
            "Epoch: 9916  Training Loss: 1621.08959961 Validation Loss: 1541.4427490234375 Test Loss: 1756.2720947265625\n",
            "Epoch: 9921  Training Loss: 1621.01708984 Validation Loss: 1541.40380859375 Test Loss: 1756.1943359375\n",
            "Epoch: 9926  Training Loss: 1620.94421387 Validation Loss: 1541.3648681640625 Test Loss: 1756.1165771484375\n",
            "Epoch: 9931  Training Loss: 1620.87182617 Validation Loss: 1541.3260498046875 Test Loss: 1756.0389404296875\n",
            "Epoch: 9936  Training Loss: 1620.79919434 Validation Loss: 1541.286865234375 Test Loss: 1755.9610595703125\n",
            "Epoch: 9941  Training Loss: 1620.72668457 Validation Loss: 1541.2481689453125 Test Loss: 1755.88330078125\n",
            "Epoch: 9946  Training Loss: 1620.65441895 Validation Loss: 1541.2091064453125 Test Loss: 1755.8056640625\n",
            "Epoch: 9951  Training Loss: 1620.58203125 Validation Loss: 1541.1700439453125 Test Loss: 1755.7281494140625\n",
            "Epoch: 9956  Training Loss: 1620.50939941 Validation Loss: 1541.1309814453125 Test Loss: 1755.6505126953125\n",
            "Epoch: 9961  Training Loss: 1620.43713379 Validation Loss: 1541.092041015625 Test Loss: 1755.5731201171875\n",
            "Epoch: 9966  Training Loss: 1620.36486816 Validation Loss: 1541.0531005859375 Test Loss: 1755.49560546875\n",
            "Epoch: 9971  Training Loss: 1620.29248047 Validation Loss: 1541.0142822265625 Test Loss: 1755.4180908203125\n",
            "Epoch: 9976  Training Loss: 1620.22033691 Validation Loss: 1540.9752197265625 Test Loss: 1755.3408203125\n",
            "Epoch: 9981  Training Loss: 1620.14807129 Validation Loss: 1540.9361572265625 Test Loss: 1755.2633056640625\n",
            "Epoch: 9986  Training Loss: 1620.07592773 Validation Loss: 1540.8970947265625 Test Loss: 1755.18603515625\n",
            "Epoch: 9991  Training Loss: 1620.00366211 Validation Loss: 1540.8582763671875 Test Loss: 1755.1085205078125\n",
            "Epoch: 9996  Training Loss: 1619.93164062 Validation Loss: 1540.819091796875 Test Loss: 1755.03125\n",
            "Epoch: 10000  Training Loss: 1619.87402344 Validation Loss: 1540.7880859375 Test Loss: 1754.9696044921875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_diagrams(train_loss_1,valid_loss_1,test_loss_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Z5dLEN1kXk_W",
        "outputId": "87cbc8fe-e506-4d57-d97b-2eaf21e9e7d0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nuQ2lUGRaEM"
      },
      "source": [
        "### Loss function : MAE & optimizer : SGD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flZBVC6JRaEM",
        "outputId": "f26705d8-b3fd-4099-c0ef-e860aa8a1dd5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=68, out_features=2, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "torch.manual_seed(50)\n",
        "input_features = 68\n",
        "output_features = 2\n",
        "hidden_layers = []\n",
        "tr = Model(input_features, output_features, hidden_layers, p=0.5)\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.SGD(tr.parameters(), lr=0.001, momentum=0.9)\n",
        "tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqqdqRU0RaEM"
      },
      "source": [
        "#### Training for 50 epochs\n",
        "The results from Linear Regression is bettet than this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj8dNU7PRaEN",
        "outputId": "b7144637-376d-45ce-ed80-cca65e369919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:   1  Training Loss: 39.23548126 Validation Loss: 40.11814880371094 Test Loss: 41.45738983154297\n",
            "Epoch:   6  Training Loss: 39.22882462 Validation Loss: 40.10969543457031 Test Loss: 41.44982147216797\n",
            "Epoch:  11  Training Loss: 39.21451569 Validation Loss: 40.091529846191406 Test Loss: 41.43356704711914\n",
            "Epoch:  16  Training Loss: 39.19569778 Validation Loss: 40.067630767822266 Test Loss: 41.412174224853516\n",
            "Epoch:  21  Training Loss: 39.17420959 Validation Loss: 40.04034423828125 Test Loss: 41.387760162353516\n",
            "Epoch:  26  Training Loss: 39.15114594 Validation Loss: 40.01105880737305 Test Loss: 41.361549377441406\n",
            "Epoch:  31  Training Loss: 39.12715912 Validation Loss: 39.9805908203125 Test Loss: 41.33428192138672\n",
            "Epoch:  36  Training Loss: 39.10261536 Validation Loss: 39.94942855834961 Test Loss: 41.306396484375\n",
            "Epoch:  41  Training Loss: 39.07775497 Validation Loss: 39.91785430908203 Test Loss: 41.278141021728516\n",
            "Epoch:  46  Training Loss: 39.05270004 Validation Loss: 39.88603591918945 Test Loss: 41.24966812133789\n",
            "Epoch:  51  Training Loss: 39.02752686 Validation Loss: 39.85407638549805 Test Loss: 41.22106170654297\n",
            "Epoch:  56  Training Loss: 39.00229263 Validation Loss: 39.82202911376953 Test Loss: 41.1923828125\n",
            "Epoch:  61  Training Loss: 38.97701645 Validation Loss: 39.78993606567383 Test Loss: 41.163658142089844\n",
            "Epoch:  66  Training Loss: 38.95172119 Validation Loss: 39.75780487060547 Test Loss: 41.13491439819336\n",
            "Epoch:  71  Training Loss: 38.92640686 Validation Loss: 39.72566604614258 Test Loss: 41.10614776611328\n",
            "Epoch:  76  Training Loss: 38.90108490 Validation Loss: 39.693511962890625 Test Loss: 41.077369689941406\n",
            "Epoch:  81  Training Loss: 38.87576294 Validation Loss: 39.661354064941406 Test Loss: 41.04859161376953\n",
            "Epoch:  86  Training Loss: 38.85043335 Validation Loss: 39.62919235229492 Test Loss: 41.019805908203125\n",
            "Epoch:  91  Training Loss: 38.82510376 Validation Loss: 39.597023010253906 Test Loss: 40.99102020263672\n",
            "Epoch:  96  Training Loss: 38.79977417 Validation Loss: 39.56485366821289 Test Loss: 40.96223831176758\n",
            "Epoch: 100  Training Loss: 38.77951050 Validation Loss: 39.53912353515625 Test Loss: 40.939208984375\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "train_loss_1,valid_loss_1,test_loss_1 = train(tr,criterion,optimizer,epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Y3GvHj8hRaEN",
        "outputId": "a16edc9f-0859-462e-f1d8-af4e33800166"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "show_diagrams(train_loss_1,valid_loss_1,test_loss_1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFpKdnlfRaEN"
      },
      "source": [
        "#### Training for 10 epochs\n",
        "The results from Linear Regression is bettet than this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LxRmhZ7RaEN",
        "outputId": "4b62d108-2404-45d0-ffec-edb3474f257b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:   1  Training Loss: 38.77444458 Validation Loss: 39.53268814086914 Test Loss: 40.93344497680664\n",
            "Epoch:   6  Training Loss: 38.74911499 Validation Loss: 39.50052261352539 Test Loss: 40.904659271240234\n",
            "Epoch:  11  Training Loss: 38.72378540 Validation Loss: 39.46835708618164 Test Loss: 40.87587356567383\n",
            "Epoch:  16  Training Loss: 38.69844818 Validation Loss: 39.43618392944336 Test Loss: 40.847084045410156\n",
            "Epoch:  21  Training Loss: 38.67311859 Validation Loss: 39.404022216796875 Test Loss: 40.818294525146484\n",
            "Epoch:  26  Training Loss: 38.64786148 Validation Loss: 39.371856689453125 Test Loss: 40.789512634277344\n",
            "Epoch:  31  Training Loss: 38.62305832 Validation Loss: 39.339813232421875 Test Loss: 40.76083755493164\n",
            "Epoch:  36  Training Loss: 38.59922028 Validation Loss: 39.30811309814453 Test Loss: 40.73246765136719\n",
            "Epoch:  41  Training Loss: 38.57670975 Validation Loss: 39.27708053588867 Test Loss: 40.70469284057617\n",
            "Epoch:  46  Training Loss: 38.55527115 Validation Loss: 39.24681854248047 Test Loss: 40.6776008605957\n",
            "Epoch:  51  Training Loss: 38.53477097 Validation Loss: 39.21723175048828 Test Loss: 40.65122985839844\n",
            "Epoch:  56  Training Loss: 38.51525497 Validation Loss: 39.1883430480957 Test Loss: 40.62579345703125\n",
            "Epoch:  61  Training Loss: 38.49650574 Validation Loss: 39.16011428833008 Test Loss: 40.60093307495117\n",
            "Epoch:  66  Training Loss: 38.47835541 Validation Loss: 39.132450103759766 Test Loss: 40.57656478881836\n",
            "Epoch:  71  Training Loss: 38.46057892 Validation Loss: 39.105228424072266 Test Loss: 40.55289840698242\n",
            "Epoch:  76  Training Loss: 38.44298172 Validation Loss: 39.07827377319336 Test Loss: 40.529541015625\n",
            "Epoch:  81  Training Loss: 38.42548370 Validation Loss: 39.051483154296875 Test Loss: 40.50638961791992\n",
            "Epoch:  86  Training Loss: 38.40805054 Validation Loss: 39.0247917175293 Test Loss: 40.48359298706055\n",
            "Epoch:  91  Training Loss: 38.39072418 Validation Loss: 38.99817657470703 Test Loss: 40.46086502075195\n",
            "Epoch:  96  Training Loss: 38.37348938 Validation Loss: 38.971641540527344 Test Loss: 40.438209533691406\n",
            "Epoch: 101  Training Loss: 38.35628891 Validation Loss: 38.94516372680664 Test Loss: 40.41559600830078\n",
            "Epoch: 106  Training Loss: 38.33910370 Validation Loss: 38.9187126159668 Test Loss: 40.39300537109375\n",
            "Epoch: 111  Training Loss: 38.32193375 Validation Loss: 38.89228057861328 Test Loss: 40.37044143676758\n",
            "Epoch: 116  Training Loss: 38.30476761 Validation Loss: 38.86586380004883 Test Loss: 40.347877502441406\n",
            "Epoch: 121  Training Loss: 38.28760910 Validation Loss: 38.839454650878906 Test Loss: 40.325321197509766\n",
            "Epoch: 126  Training Loss: 38.27045059 Validation Loss: 38.81304168701172 Test Loss: 40.302764892578125\n",
            "Epoch: 131  Training Loss: 38.25329590 Validation Loss: 38.78663635253906 Test Loss: 40.28021240234375\n",
            "Epoch: 136  Training Loss: 38.23613739 Validation Loss: 38.76022720336914 Test Loss: 40.257659912109375\n",
            "Epoch: 141  Training Loss: 38.21898651 Validation Loss: 38.733821868896484 Test Loss: 40.235111236572266\n",
            "Epoch: 146  Training Loss: 38.20182800 Validation Loss: 38.707420349121094 Test Loss: 40.212562561035156\n",
            "Epoch: 151  Training Loss: 38.18467712 Validation Loss: 38.68101501464844 Test Loss: 40.19001007080078\n",
            "Epoch: 156  Training Loss: 38.16752625 Validation Loss: 38.65461349487305 Test Loss: 40.16746139526367\n",
            "Epoch: 161  Training Loss: 38.15036774 Validation Loss: 38.62820816040039 Test Loss: 40.14491271972656\n",
            "Epoch: 166  Training Loss: 38.13321304 Validation Loss: 38.601802825927734 Test Loss: 40.12236404418945\n",
            "Epoch: 171  Training Loss: 38.11605835 Validation Loss: 38.575401306152344 Test Loss: 40.099815368652344\n",
            "Epoch: 176  Training Loss: 38.09890366 Validation Loss: 38.54899597167969 Test Loss: 40.077266693115234\n",
            "Epoch: 181  Training Loss: 38.08175278 Validation Loss: 38.5225944519043 Test Loss: 40.054718017578125\n",
            "Epoch: 186  Training Loss: 38.06459427 Validation Loss: 38.496192932128906 Test Loss: 40.03217315673828\n",
            "Epoch: 191  Training Loss: 38.04744339 Validation Loss: 38.469783782958984 Test Loss: 40.00962448120117\n",
            "Epoch: 196  Training Loss: 38.03029251 Validation Loss: 38.443382263183594 Test Loss: 39.98706817626953\n",
            "Epoch: 201  Training Loss: 38.01313400 Validation Loss: 38.4169807434082 Test Loss: 39.96451950073242\n",
            "Epoch: 206  Training Loss: 37.99597931 Validation Loss: 38.39057159423828 Test Loss: 39.941978454589844\n",
            "Epoch: 211  Training Loss: 37.97882843 Validation Loss: 38.36417007446289 Test Loss: 39.91942596435547\n",
            "Epoch: 216  Training Loss: 37.96167755 Validation Loss: 38.3377685546875 Test Loss: 39.896881103515625\n",
            "Epoch: 221  Training Loss: 37.94451904 Validation Loss: 38.311363220214844 Test Loss: 39.87432861328125\n",
            "Epoch: 226  Training Loss: 37.92736435 Validation Loss: 38.28496170043945 Test Loss: 39.85177993774414\n",
            "Epoch: 231  Training Loss: 37.91020966 Validation Loss: 38.2585563659668 Test Loss: 39.82923126220703\n",
            "Epoch: 236  Training Loss: 37.89305496 Validation Loss: 38.23215103149414 Test Loss: 39.806678771972656\n",
            "Epoch: 241  Training Loss: 37.87590408 Validation Loss: 38.20574951171875 Test Loss: 39.78413391113281\n",
            "Epoch: 246  Training Loss: 37.85874557 Validation Loss: 38.179344177246094 Test Loss: 39.76158142089844\n",
            "Epoch: 251  Training Loss: 37.84159088 Validation Loss: 38.1529426574707 Test Loss: 39.73903274536133\n",
            "Epoch: 256  Training Loss: 37.82444000 Validation Loss: 38.12653732299805 Test Loss: 39.71648406982422\n",
            "Epoch: 261  Training Loss: 37.80728531 Validation Loss: 38.100135803222656 Test Loss: 39.69393539428711\n",
            "Epoch: 266  Training Loss: 37.79013443 Validation Loss: 38.073726654052734 Test Loss: 39.67138671875\n",
            "Epoch: 271  Training Loss: 37.77297974 Validation Loss: 38.04732894897461 Test Loss: 39.64883804321289\n",
            "Epoch: 276  Training Loss: 37.75582123 Validation Loss: 38.02091979980469 Test Loss: 39.62628936767578\n",
            "Epoch: 281  Training Loss: 37.73866653 Validation Loss: 37.9945182800293 Test Loss: 39.60374069213867\n",
            "Epoch: 286  Training Loss: 37.72151184 Validation Loss: 37.968116760253906 Test Loss: 39.58119201660156\n",
            "Epoch: 291  Training Loss: 37.70435715 Validation Loss: 37.941707611083984 Test Loss: 39.55863952636719\n",
            "Epoch: 296  Training Loss: 37.68720627 Validation Loss: 37.915306091308594 Test Loss: 39.53609085083008\n",
            "Epoch: 301  Training Loss: 37.67005157 Validation Loss: 37.88889694213867 Test Loss: 39.51354217529297\n",
            "Epoch: 306  Training Loss: 37.65289688 Validation Loss: 37.86249542236328 Test Loss: 39.490989685058594\n",
            "Epoch: 311  Training Loss: 37.63574600 Validation Loss: 37.83609390258789 Test Loss: 39.46844482421875\n",
            "Epoch: 316  Training Loss: 37.61859131 Validation Loss: 37.8096923828125 Test Loss: 39.445892333984375\n",
            "Epoch: 321  Training Loss: 37.60143280 Validation Loss: 37.78329086303711 Test Loss: 39.423343658447266\n",
            "Epoch: 326  Training Loss: 37.58428192 Validation Loss: 37.75688171386719 Test Loss: 39.400794982910156\n",
            "Epoch: 331  Training Loss: 37.56712341 Validation Loss: 37.7304801940918 Test Loss: 39.37824630737305\n",
            "Epoch: 336  Training Loss: 37.54997253 Validation Loss: 37.70407485961914 Test Loss: 39.35569763183594\n",
            "Epoch: 341  Training Loss: 37.53281784 Validation Loss: 37.677669525146484 Test Loss: 39.33314514160156\n",
            "Epoch: 346  Training Loss: 37.51566696 Validation Loss: 37.65126419067383 Test Loss: 39.31060028076172\n",
            "Epoch: 351  Training Loss: 37.49850845 Validation Loss: 37.62486267089844 Test Loss: 39.288047790527344\n",
            "Epoch: 356  Training Loss: 37.48135376 Validation Loss: 37.598453521728516 Test Loss: 39.265499114990234\n",
            "Epoch: 361  Training Loss: 37.46419907 Validation Loss: 37.572052001953125 Test Loss: 39.242950439453125\n",
            "Epoch: 366  Training Loss: 37.44705200 Validation Loss: 37.54564666748047 Test Loss: 39.22040939331055\n",
            "Epoch: 371  Training Loss: 37.42989349 Validation Loss: 37.51924514770508 Test Loss: 39.19785690307617\n",
            "Epoch: 376  Training Loss: 37.41273880 Validation Loss: 37.49284362792969 Test Loss: 39.17530822753906\n",
            "Epoch: 381  Training Loss: 37.39558029 Validation Loss: 37.46643829345703 Test Loss: 39.15275955200195\n",
            "Epoch: 386  Training Loss: 37.37842941 Validation Loss: 37.440032958984375 Test Loss: 39.130210876464844\n",
            "Epoch: 391  Training Loss: 37.36127853 Validation Loss: 37.413631439208984 Test Loss: 39.107662200927734\n",
            "Epoch: 396  Training Loss: 37.34412384 Validation Loss: 37.38722610473633 Test Loss: 39.08510971069336\n",
            "Epoch: 401  Training Loss: 37.32696915 Validation Loss: 37.36082458496094 Test Loss: 39.06256103515625\n",
            "Epoch: 406  Training Loss: 37.30981445 Validation Loss: 37.33441925048828 Test Loss: 39.04001235961914\n",
            "Epoch: 411  Training Loss: 37.29265976 Validation Loss: 37.30801773071289 Test Loss: 39.0174674987793\n",
            "Epoch: 416  Training Loss: 37.27550507 Validation Loss: 37.281612396240234 Test Loss: 38.99491500854492\n",
            "Epoch: 421  Training Loss: 37.25835037 Validation Loss: 37.255210876464844 Test Loss: 38.97236633300781\n",
            "Epoch: 426  Training Loss: 37.24119949 Validation Loss: 37.22880554199219 Test Loss: 38.9498176574707\n",
            "Epoch: 431  Training Loss: 37.22404480 Validation Loss: 37.20240020751953 Test Loss: 38.927268981933594\n",
            "Epoch: 436  Training Loss: 37.20688629 Validation Loss: 37.17599868774414 Test Loss: 38.904720306396484\n",
            "Epoch: 441  Training Loss: 37.18973541 Validation Loss: 37.149593353271484 Test Loss: 38.88216781616211\n",
            "Epoch: 446  Training Loss: 37.17258072 Validation Loss: 37.12318801879883 Test Loss: 38.859619140625\n",
            "Epoch: 451  Training Loss: 37.15542603 Validation Loss: 37.09678649902344 Test Loss: 38.83707046508789\n",
            "Epoch: 456  Training Loss: 37.13827133 Validation Loss: 37.07038116455078 Test Loss: 38.81452178955078\n",
            "Epoch: 461  Training Loss: 37.12111664 Validation Loss: 37.04397964477539 Test Loss: 38.79197311401367\n",
            "Epoch: 466  Training Loss: 37.10396576 Validation Loss: 37.017574310302734 Test Loss: 38.76942443847656\n",
            "Epoch: 471  Training Loss: 37.08681107 Validation Loss: 36.99116897583008 Test Loss: 38.74687576293945\n",
            "Epoch: 476  Training Loss: 37.06965637 Validation Loss: 36.96476745605469 Test Loss: 38.724327087402344\n",
            "Epoch: 481  Training Loss: 37.05250549 Validation Loss: 36.9383659362793 Test Loss: 38.701778411865234\n",
            "Epoch: 486  Training Loss: 37.03534698 Validation Loss: 36.91196060180664 Test Loss: 38.679229736328125\n",
            "Epoch: 491  Training Loss: 37.01819229 Validation Loss: 36.88638687133789 Test Loss: 38.65667724609375\n",
            "Epoch: 496  Training Loss: 37.00104141 Validation Loss: 36.86113739013672 Test Loss: 38.63413619995117\n",
            "Epoch: 501  Training Loss: 36.98395157 Validation Loss: 36.83589553833008 Test Loss: 38.611595153808594\n",
            "Epoch: 506  Training Loss: 36.96697617 Validation Loss: 36.81071853637695 Test Loss: 38.589111328125\n",
            "Epoch: 511  Training Loss: 36.95002747 Validation Loss: 36.785587310791016 Test Loss: 38.56666564941406\n",
            "Epoch: 516  Training Loss: 36.93310165 Validation Loss: 36.76047897338867 Test Loss: 38.544246673583984\n",
            "Epoch: 521  Training Loss: 36.91619110 Validation Loss: 36.73538589477539 Test Loss: 38.52184295654297\n",
            "Epoch: 526  Training Loss: 36.89943314 Validation Loss: 36.710350036621094 Test Loss: 38.499488830566406\n",
            "Epoch: 531  Training Loss: 36.88300323 Validation Loss: 36.68545913696289 Test Loss: 38.47726058959961\n",
            "Epoch: 536  Training Loss: 36.86676788 Validation Loss: 36.66072463989258 Test Loss: 38.455169677734375\n",
            "Epoch: 541  Training Loss: 36.85087204 Validation Loss: 36.63619613647461 Test Loss: 38.433265686035156\n",
            "Epoch: 546  Training Loss: 36.83531570 Validation Loss: 36.61191940307617 Test Loss: 38.41158676147461\n",
            "Epoch: 551  Training Loss: 36.81988144 Validation Loss: 36.58782958984375 Test Loss: 38.3902702331543\n",
            "Epoch: 556  Training Loss: 36.80451965 Validation Loss: 36.5638542175293 Test Loss: 38.369258880615234\n",
            "Epoch: 561  Training Loss: 36.78926086 Validation Loss: 36.53995895385742 Test Loss: 38.34831237792969\n",
            "Epoch: 566  Training Loss: 36.77428055 Validation Loss: 36.51618957519531 Test Loss: 38.32773208618164\n",
            "Epoch: 571  Training Loss: 36.75949478 Validation Loss: 36.49259948730469 Test Loss: 38.30783462524414\n",
            "Epoch: 576  Training Loss: 36.74490356 Validation Loss: 36.46915817260742 Test Loss: 38.28805923461914\n",
            "Epoch: 581  Training Loss: 36.73044968 Validation Loss: 36.44585418701172 Test Loss: 38.26840591430664\n",
            "Epoch: 586  Training Loss: 36.71612930 Validation Loss: 36.42268371582031 Test Loss: 38.24900817871094\n",
            "Epoch: 591  Training Loss: 36.70186234 Validation Loss: 36.399593353271484 Test Loss: 38.22990798950195\n",
            "Epoch: 596  Training Loss: 36.68762970 Validation Loss: 36.376556396484375 Test Loss: 38.210845947265625\n",
            "Epoch: 601  Training Loss: 36.67341232 Validation Loss: 36.35354995727539 Test Loss: 38.19215393066406\n",
            "Epoch: 606  Training Loss: 36.65921021 Validation Loss: 36.33055877685547 Test Loss: 38.173492431640625\n",
            "Epoch: 611  Training Loss: 36.64500809 Validation Loss: 36.308292388916016 Test Loss: 38.15483856201172\n",
            "Epoch: 616  Training Loss: 36.63081360 Validation Loss: 36.286441802978516 Test Loss: 38.13633728027344\n",
            "Epoch: 621  Training Loss: 36.61680222 Validation Loss: 36.26530456542969 Test Loss: 38.118324279785156\n",
            "Epoch: 626  Training Loss: 36.60290527 Validation Loss: 36.244266510009766 Test Loss: 38.10051345825195\n",
            "Epoch: 631  Training Loss: 36.58923721 Validation Loss: 36.22333526611328 Test Loss: 38.08279037475586\n",
            "Epoch: 636  Training Loss: 36.57569122 Validation Loss: 36.202545166015625 Test Loss: 38.065269470214844\n",
            "Epoch: 641  Training Loss: 36.56219864 Validation Loss: 36.18183517456055 Test Loss: 38.04808044433594\n",
            "Epoch: 646  Training Loss: 36.54873657 Validation Loss: 36.16117858886719 Test Loss: 38.03092956542969\n",
            "Epoch: 651  Training Loss: 36.53539658 Validation Loss: 36.14057540893555 Test Loss: 38.013832092285156\n",
            "Epoch: 656  Training Loss: 36.52214813 Validation Loss: 36.12004470825195 Test Loss: 37.99694061279297\n",
            "Epoch: 661  Training Loss: 36.50927353 Validation Loss: 36.099666595458984 Test Loss: 37.98035430908203\n",
            "Epoch: 666  Training Loss: 36.49657440 Validation Loss: 36.07949447631836 Test Loss: 37.96400833129883\n",
            "Epoch: 671  Training Loss: 36.48405075 Validation Loss: 36.059471130371094 Test Loss: 37.94802474975586\n",
            "Epoch: 676  Training Loss: 36.47161102 Validation Loss: 36.039573669433594 Test Loss: 37.932132720947266\n",
            "Epoch: 681  Training Loss: 36.45923615 Validation Loss: 36.01974868774414 Test Loss: 37.91630172729492\n",
            "Epoch: 686  Training Loss: 36.44697571 Validation Loss: 36.0000114440918 Test Loss: 37.90053939819336\n",
            "Epoch: 691  Training Loss: 36.43481827 Validation Loss: 35.9803466796875 Test Loss: 37.88483810424805\n",
            "Epoch: 696  Training Loss: 36.42276764 Validation Loss: 35.96076965332031 Test Loss: 37.869205474853516\n",
            "Epoch: 701  Training Loss: 36.41083908 Validation Loss: 35.94129180908203 Test Loss: 37.8536491394043\n",
            "Epoch: 706  Training Loss: 36.39894867 Validation Loss: 35.921875 Test Loss: 37.83815002441406\n",
            "Epoch: 711  Training Loss: 36.38708878 Validation Loss: 35.902618408203125 Test Loss: 37.822879791259766\n",
            "Epoch: 716  Training Loss: 36.37531281 Validation Loss: 35.88401412963867 Test Loss: 37.807735443115234\n",
            "Epoch: 721  Training Loss: 36.36359024 Validation Loss: 35.86547088623047 Test Loss: 37.79269790649414\n",
            "Epoch: 726  Training Loss: 36.35191727 Validation Loss: 35.846961975097656 Test Loss: 37.77805709838867\n",
            "Epoch: 731  Training Loss: 36.34042740 Validation Loss: 35.82853698730469 Test Loss: 37.76362991333008\n",
            "Epoch: 736  Training Loss: 36.32899475 Validation Loss: 35.810203552246094 Test Loss: 37.74927520751953\n",
            "Epoch: 741  Training Loss: 36.31759644 Validation Loss: 35.79192352294922 Test Loss: 37.73495864868164\n",
            "Epoch: 746  Training Loss: 36.30630112 Validation Loss: 35.77369689941406 Test Loss: 37.720924377441406\n",
            "Epoch: 751  Training Loss: 36.29513168 Validation Loss: 35.75555419921875 Test Loss: 37.70698928833008\n",
            "Epoch: 756  Training Loss: 36.28401566 Validation Loss: 35.73748779296875 Test Loss: 37.693119049072266\n",
            "Epoch: 761  Training Loss: 36.27293015 Validation Loss: 35.71946334838867 Test Loss: 37.67927932739258\n",
            "Epoch: 766  Training Loss: 36.26186371 Validation Loss: 35.70147705078125 Test Loss: 37.665462493896484\n",
            "Epoch: 771  Training Loss: 36.25080109 Validation Loss: 35.683494567871094 Test Loss: 37.65165710449219\n",
            "Epoch: 776  Training Loss: 36.23974991 Validation Loss: 35.66552734375 Test Loss: 37.637855529785156\n",
            "Epoch: 781  Training Loss: 36.22869873 Validation Loss: 35.64756774902344 Test Loss: 37.624061584472656\n",
            "Epoch: 786  Training Loss: 36.21765137 Validation Loss: 35.629608154296875 Test Loss: 37.610267639160156\n",
            "Epoch: 791  Training Loss: 36.20661163 Validation Loss: 35.61165237426758 Test Loss: 37.59647750854492\n",
            "Epoch: 796  Training Loss: 36.19565964 Validation Loss: 35.59372329711914 Test Loss: 37.58271408081055\n",
            "Epoch: 801  Training Loss: 36.18472672 Validation Loss: 35.57583999633789 Test Loss: 37.568973541259766\n",
            "Epoch: 806  Training Loss: 36.17380905 Validation Loss: 35.5579719543457 Test Loss: 37.55525588989258\n",
            "Epoch: 811  Training Loss: 36.16289520 Validation Loss: 35.54011917114258 Test Loss: 37.541542053222656\n",
            "Epoch: 816  Training Loss: 36.15198898 Validation Loss: 35.522274017333984 Test Loss: 37.527835845947266\n",
            "Epoch: 821  Training Loss: 36.14108276 Validation Loss: 35.50443649291992 Test Loss: 37.51413345336914\n",
            "Epoch: 826  Training Loss: 36.13025665 Validation Loss: 35.48661422729492 Test Loss: 37.500450134277344\n",
            "Epoch: 831  Training Loss: 36.11946106 Validation Loss: 35.468841552734375 Test Loss: 37.48679733276367\n",
            "Epoch: 836  Training Loss: 36.10868073 Validation Loss: 35.45108413696289 Test Loss: 37.47315979003906\n",
            "Epoch: 841  Training Loss: 36.09791183 Validation Loss: 35.433345794677734 Test Loss: 37.459537506103516\n",
            "Epoch: 846  Training Loss: 36.08713913 Validation Loss: 35.415618896484375 Test Loss: 37.44591522216797\n",
            "Epoch: 851  Training Loss: 36.07637787 Validation Loss: 35.39789581298828 Test Loss: 37.43230438232422\n",
            "Epoch: 856  Training Loss: 36.06561661 Validation Loss: 35.38016891479492 Test Loss: 37.41869354248047\n",
            "Epoch: 861  Training Loss: 36.05485535 Validation Loss: 35.362449645996094 Test Loss: 37.40507888793945\n",
            "Epoch: 866  Training Loss: 36.04409409 Validation Loss: 35.34473419189453 Test Loss: 37.39147186279297\n",
            "Epoch: 871  Training Loss: 36.03333282 Validation Loss: 35.32701110839844 Test Loss: 37.37786102294922\n",
            "Epoch: 876  Training Loss: 36.02257156 Validation Loss: 35.309295654296875 Test Loss: 37.364253997802734\n",
            "Epoch: 881  Training Loss: 36.01181412 Validation Loss: 35.29157638549805 Test Loss: 37.350643157958984\n",
            "Epoch: 886  Training Loss: 36.00105286 Validation Loss: 35.27385711669922 Test Loss: 37.3370361328125\n",
            "Epoch: 891  Training Loss: 35.99029160 Validation Loss: 35.256141662597656 Test Loss: 37.32342529296875\n",
            "Epoch: 896  Training Loss: 35.97953415 Validation Loss: 35.23841857910156 Test Loss: 37.309818267822266\n",
            "Epoch: 901  Training Loss: 35.96877670 Validation Loss: 35.220699310302734 Test Loss: 37.29621124267578\n",
            "Epoch: 906  Training Loss: 35.95801544 Validation Loss: 35.20298385620117 Test Loss: 37.28260040283203\n",
            "Epoch: 911  Training Loss: 35.94725418 Validation Loss: 35.18526840209961 Test Loss: 37.26899337768555\n",
            "Epoch: 916  Training Loss: 35.93654251 Validation Loss: 35.16755676269531 Test Loss: 37.25539016723633\n",
            "Epoch: 921  Training Loss: 35.92592621 Validation Loss: 35.14990997314453 Test Loss: 37.24183654785156\n",
            "Epoch: 926  Training Loss: 35.91533661 Validation Loss: 35.13230514526367 Test Loss: 37.228309631347656\n",
            "Epoch: 931  Training Loss: 35.90478897 Validation Loss: 35.114952087402344 Test Loss: 37.21481704711914\n",
            "Epoch: 936  Training Loss: 35.89435577 Validation Loss: 35.09834289550781 Test Loss: 37.20172882080078\n",
            "Epoch: 941  Training Loss: 35.88395691 Validation Loss: 35.081787109375 Test Loss: 37.18867874145508\n",
            "Epoch: 946  Training Loss: 35.87357330 Validation Loss: 35.06526184082031 Test Loss: 37.175655364990234\n",
            "Epoch: 951  Training Loss: 35.86320114 Validation Loss: 35.04875183105469 Test Loss: 37.16264343261719\n",
            "Epoch: 956  Training Loss: 35.85283661 Validation Loss: 35.032257080078125 Test Loss: 37.14964294433594\n",
            "Epoch: 961  Training Loss: 35.84247971 Validation Loss: 35.01664352416992 Test Loss: 37.13664627075195\n",
            "Epoch: 966  Training Loss: 35.83211899 Validation Loss: 35.00102996826172 Test Loss: 37.12364959716797\n",
            "Epoch: 971  Training Loss: 35.82176208 Validation Loss: 34.98542785644531 Test Loss: 37.11065673828125\n",
            "Epoch: 976  Training Loss: 35.81140900 Validation Loss: 34.969825744628906 Test Loss: 37.09782791137695\n",
            "Epoch: 981  Training Loss: 35.80105209 Validation Loss: 34.95421600341797 Test Loss: 37.085182189941406\n",
            "Epoch: 986  Training Loss: 35.79069519 Validation Loss: 34.93861389160156 Test Loss: 37.072540283203125\n",
            "Epoch: 991  Training Loss: 35.78033829 Validation Loss: 34.92301559448242 Test Loss: 37.059898376464844\n",
            "Epoch: 996  Training Loss: 35.76998520 Validation Loss: 34.90740966796875 Test Loss: 37.0472526550293\n",
            "Epoch: 1000  Training Loss: 35.76169968 Validation Loss: 34.894927978515625 Test Loss: 37.03713607788086\n"
          ]
        }
      ],
      "source": [
        "epochs = 1000\n",
        "model = Model(input_features, output_features, hidden_layers, p=0.5)\n",
        "train_loss_1,valid_loss_1,test_loss_1 = train(tr,criterion,optimizer,epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "b_pgAKSCRaEO",
        "outputId": "5f176426-d8d1-4b12-d810-4df2a34e6ab5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "show_diagrams(train_loss_1,valid_loss_1,test_loss_1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5000\n",
        "tr = Model(input_features, output_features, hidden_layers, p=0.5)\n",
        "train_loss_1,valid_loss_1,test_loss_1 = train(tr,criterion,optimizer,epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SMmGfvYYMGf",
        "outputId": "c850cace-b17c-466e-81bf-baa98e25709c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:   1  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:   6  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  11  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  16  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  21  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  26  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  31  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  36  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  41  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  46  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  51  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  56  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  61  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  66  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  71  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  76  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  81  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  86  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  91  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch:  96  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 101  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 106  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 111  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 116  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 121  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 126  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 131  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 136  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 141  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 146  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 151  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 156  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 161  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 166  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 171  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 176  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 181  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 186  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 191  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 196  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 201  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 206  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 211  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 216  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 221  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 226  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 231  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 236  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 241  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 246  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 251  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 256  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 261  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 266  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 271  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 276  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 281  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 286  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 291  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 296  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 301  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 306  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 311  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 316  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 321  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 326  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 331  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 336  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 341  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 346  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 351  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 356  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 361  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 366  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 371  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 376  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 381  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 386  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 391  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 396  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 401  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 406  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 411  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 416  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 421  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 426  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 431  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 436  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 441  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 446  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 451  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 456  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 461  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 466  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 471  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 476  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 481  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 486  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 491  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 496  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 501  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 506  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 511  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 516  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 521  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 526  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 531  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 536  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 541  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 546  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 551  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 556  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 561  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 566  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 571  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 576  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 581  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 586  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 591  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 596  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 601  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 606  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 611  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 616  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 621  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 626  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 631  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 636  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 641  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 646  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 651  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 656  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 661  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 666  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 671  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 676  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 681  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 686  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 691  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 696  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 701  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 706  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 711  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 716  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 721  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 726  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 731  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 736  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 741  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 746  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 751  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 756  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 761  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 766  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 771  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 776  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 781  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 786  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 791  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 796  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 801  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 806  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 811  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 816  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 821  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 826  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 831  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 836  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 841  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 846  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 851  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 856  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 861  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 866  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 871  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 876  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 881  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 886  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 891  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 896  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 901  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 906  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 911  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 916  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 921  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 926  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 931  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 936  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 941  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 946  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 951  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 956  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 961  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 966  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 971  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 976  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 981  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 986  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 991  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 996  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1001  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1006  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1011  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1016  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1021  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1026  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1031  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1036  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1041  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1046  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1051  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1056  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1061  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1066  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1071  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1076  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1081  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1086  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1091  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1096  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1101  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1106  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1111  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1116  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1121  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1126  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1131  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1136  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1141  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1146  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1151  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1156  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1161  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1166  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1171  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1176  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1181  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1186  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1191  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1196  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1201  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1206  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1211  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1216  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1221  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1226  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1231  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1236  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1241  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1246  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1251  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1256  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1261  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1266  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1271  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1276  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1281  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1286  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1291  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1296  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1301  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1306  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1311  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1316  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1321  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1326  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1331  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1336  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1341  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1346  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1351  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1356  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1361  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1366  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1371  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1376  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1381  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1386  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1391  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1396  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1401  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1406  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1411  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1416  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1421  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1426  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1431  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1436  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1441  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1446  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1451  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1456  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1461  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1466  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1471  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1476  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1481  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1486  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1491  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1496  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1501  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1506  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1511  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1516  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1521  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1526  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1531  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1536  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1541  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1546  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1551  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1556  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1561  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1566  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1571  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1576  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1581  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1586  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1591  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1596  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1601  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1606  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1611  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1616  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1621  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1626  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1631  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1636  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1641  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1646  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1651  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1656  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1661  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1666  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1671  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1676  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1681  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1686  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1691  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1696  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1701  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1706  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1711  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1716  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1721  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1726  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1731  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1736  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1741  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1746  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1751  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1756  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1761  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1766  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1771  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1776  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1781  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1786  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1791  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1796  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1801  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1806  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1811  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1816  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1821  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1826  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1831  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1836  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1841  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1846  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1851  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1856  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1861  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1866  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1871  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1876  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1881  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1886  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1891  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1896  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1901  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1906  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1911  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1916  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1921  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1926  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1931  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1936  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1941  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1946  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1951  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1956  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1961  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1966  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1971  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1976  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1981  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1986  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1991  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 1996  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2001  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2006  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2011  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2016  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2021  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2026  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2031  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2036  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2041  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2046  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2051  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2056  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2061  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2066  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2071  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2076  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2081  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2086  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2091  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2096  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2101  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2106  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2111  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2116  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2121  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2126  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2131  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2136  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2141  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2146  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2151  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2156  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2161  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2166  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2171  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2176  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2181  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2186  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2191  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2196  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2201  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2206  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2211  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2216  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2221  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2226  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2231  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2236  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2241  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2246  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2251  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2256  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2261  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2266  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2271  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2276  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2281  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2286  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2291  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2296  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2301  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2306  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2311  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2316  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2321  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2326  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2331  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2336  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2341  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2346  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2351  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2356  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2361  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2366  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2371  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2376  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2381  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2386  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2391  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2396  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2401  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2406  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2411  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2416  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2421  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2426  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2431  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2436  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2441  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2446  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2451  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2456  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2461  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2466  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2471  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2476  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2481  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2486  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2491  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2496  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2501  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2506  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2511  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2516  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2521  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2526  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2531  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2536  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2541  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2546  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2551  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2556  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2561  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2566  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2571  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2576  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2581  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2586  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2591  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2596  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2601  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2606  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2611  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2616  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2621  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2626  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2631  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2636  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2641  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2646  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2651  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2656  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2661  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2666  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2671  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2676  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2681  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2686  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2691  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2696  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2701  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2706  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2711  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2716  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2721  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2726  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2731  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2736  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2741  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2746  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2751  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2756  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2761  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2766  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2771  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2776  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2781  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2786  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2791  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2796  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2801  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2806  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2811  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2816  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2821  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2826  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2831  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2836  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2841  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2846  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2851  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2856  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2861  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2866  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2871  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2876  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2881  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2886  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2891  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2896  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2901  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2906  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2911  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2916  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2921  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2926  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2931  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2936  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2941  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2946  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2951  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2956  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2961  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2966  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2971  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2976  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2981  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2986  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2991  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 2996  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3001  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3006  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3011  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3016  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3021  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3026  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3031  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3036  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3041  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3046  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3051  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3056  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3061  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3066  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3071  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3076  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3081  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3086  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3091  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3096  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3101  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3106  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3111  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3116  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3121  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3126  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3131  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3136  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3141  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3146  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3151  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3156  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3161  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3166  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3171  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3176  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3181  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3186  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3191  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3196  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3201  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3206  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3211  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3216  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3221  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3226  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3231  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3236  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3241  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3246  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3251  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3256  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3261  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3266  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3271  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3276  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3281  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3286  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3291  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3296  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3301  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3306  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3311  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3316  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3321  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3326  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3331  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3336  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3341  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3346  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3351  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3356  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3361  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3366  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3371  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3376  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3381  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3386  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3391  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3396  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3401  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3406  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3411  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3416  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3421  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3426  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3431  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3436  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3441  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3446  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3451  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3456  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3461  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3466  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3471  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3476  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3481  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3486  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3491  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3496  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3501  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3506  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3511  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3516  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3521  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3526  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3531  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3536  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3541  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3546  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3551  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3556  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3561  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3566  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3571  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3576  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3581  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3586  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3591  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3596  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3601  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3606  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3611  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3616  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3621  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3626  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3631  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3636  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3641  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3646  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3651  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3656  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3661  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3666  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3671  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3676  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3681  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3686  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3691  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3696  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3701  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3706  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3711  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3716  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3721  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3726  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3731  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3736  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3741  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3746  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3751  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3756  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3761  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3766  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3771  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3776  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3781  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3786  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3791  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3796  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3801  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3806  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3811  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3816  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3821  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3826  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3831  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3836  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3841  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3846  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3851  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3856  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3861  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3866  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3871  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3876  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3881  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3886  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3891  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3896  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3901  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3906  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3911  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3916  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3921  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3926  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3931  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3936  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3941  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3946  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3951  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3956  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3961  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3966  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3971  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3976  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3981  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3986  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3991  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 3996  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4001  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4006  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4011  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4016  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4021  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4026  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4031  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4036  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4041  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4046  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4051  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4056  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4061  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4066  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4071  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4076  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4081  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4086  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4091  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4096  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4101  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4106  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4111  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4116  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4121  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4126  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4131  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4136  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4141  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4146  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4151  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4156  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4161  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4166  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4171  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4176  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4181  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4186  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4191  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4196  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4201  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4206  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4211  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4216  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4221  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4226  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4231  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4236  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4241  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4246  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4251  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4256  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4261  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4266  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4271  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4276  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4281  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4286  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4291  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4296  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4301  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4306  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4311  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4316  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4321  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4326  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4331  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4336  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4341  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4346  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4351  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4356  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4361  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4366  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4371  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4376  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4381  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4386  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4391  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4396  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4401  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4406  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4411  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4416  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4421  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4426  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4431  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4436  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4441  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4446  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4451  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4456  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4461  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4466  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4471  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4476  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4481  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4486  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4491  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4496  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4501  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4506  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4511  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4516  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4521  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4526  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4531  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4536  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4541  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4546  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4551  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4556  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4561  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4566  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4571  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4576  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4581  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4586  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4591  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4596  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4601  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4606  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4611  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4616  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4621  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4626  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4631  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4636  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4641  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4646  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4651  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4656  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4661  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4666  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4671  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4676  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4681  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4686  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4691  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4696  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4701  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4706  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4711  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4716  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4721  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4726  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4731  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4736  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4741  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4746  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4751  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4756  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4761  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4766  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4771  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4776  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4781  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4786  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4791  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4796  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4801  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4806  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4811  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4816  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4821  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4826  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4831  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4836  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4841  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4846  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4851  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4856  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4861  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4866  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4871  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4876  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4881  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4886  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4891  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4896  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4901  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4906  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4911  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4916  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4921  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4926  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4931  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4936  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4941  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4946  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4951  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4956  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4961  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4966  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4971  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4976  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4981  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4986  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4991  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 4996  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n",
            "Epoch: 5000  Training Loss: 39.32359695 Validation Loss: 40.230525970458984 Test Loss: 41.556602478027344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_diagrams(train_loss_1,valid_loss_1,test_loss_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hecO-HD5YNU3",
        "outputId": "e5ed4e63-712e-456e-acf0-4faa7de742d4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjE_HQwQRaEO"
      },
      "source": [
        "### Loss function : MAE & optimizer : Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKCTeQUzRaEO",
        "outputId": "ecea3c45-2bd8-4d77-8359-9ec7a5c5dc39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=68, out_features=2, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "torch.manual_seed(50)\n",
        "input_features = 68\n",
        "output_features = 2\n",
        "hidden_layers = []\n",
        "tr = Model(input_features, output_features, hidden_layers, p=0.5)\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(tr.parameters(), lr=0.001)\n",
        "tr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPod6aGvRaEO"
      },
      "source": [
        "#### Training for 50 epochs\n",
        "The results from Linear Regression is bettet than this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfH1mYjJRaEO",
        "outputId": "75708434-2197-43e9-b639-03a28c6df329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:   1  Training Loss: 38.71606827 Validation Loss: 39.4741325378418 Test Loss: 40.868560791015625\n",
            "Epoch:   6  Training Loss: 38.69009781 Validation Loss: 39.441932678222656 Test Loss: 40.83911895751953\n",
            "Epoch:  11  Training Loss: 38.66436386 Validation Loss: 39.409767150878906 Test Loss: 40.80970764160156\n",
            "Epoch:  16  Training Loss: 38.63926315 Validation Loss: 39.37778854370117 Test Loss: 40.78047180175781\n",
            "Epoch:  21  Training Loss: 38.61502075 Validation Loss: 39.34619140625 Test Loss: 40.7515754699707\n",
            "Epoch:  26  Training Loss: 38.59166336 Validation Loss: 39.315040588378906 Test Loss: 40.72308349609375\n",
            "Epoch:  31  Training Loss: 38.56940460 Validation Loss: 39.28446578979492 Test Loss: 40.69511795043945\n",
            "Epoch:  36  Training Loss: 38.54779434 Validation Loss: 39.25440979003906 Test Loss: 40.66761779785156\n",
            "Epoch:  41  Training Loss: 38.52669907 Validation Loss: 39.2247428894043 Test Loss: 40.64048385620117\n",
            "Epoch:  46  Training Loss: 38.50645065 Validation Loss: 39.195518493652344 Test Loss: 40.61415481567383\n",
            "Epoch:  51  Training Loss: 38.48656464 Validation Loss: 39.166683197021484 Test Loss: 40.58818435668945\n",
            "Epoch:  56  Training Loss: 38.46717453 Validation Loss: 39.13813400268555 Test Loss: 40.56247329711914\n",
            "Epoch:  61  Training Loss: 38.44802856 Validation Loss: 39.10984420776367 Test Loss: 40.53717803955078\n",
            "Epoch:  66  Training Loss: 38.42895508 Validation Loss: 39.081668853759766 Test Loss: 40.51218795776367\n",
            "Epoch:  71  Training Loss: 38.40999603 Validation Loss: 39.05354309082031 Test Loss: 40.48724365234375\n",
            "Epoch:  76  Training Loss: 38.39117432 Validation Loss: 39.02547073364258 Test Loss: 40.46234130859375\n",
            "Epoch:  81  Training Loss: 38.37237930 Validation Loss: 38.99741744995117 Test Loss: 40.43745422363281\n",
            "Epoch:  86  Training Loss: 38.35368347 Validation Loss: 38.969364166259766 Test Loss: 40.412574768066406\n",
            "Epoch:  91  Training Loss: 38.33498001 Validation Loss: 38.94131088256836 Test Loss: 40.38787078857422\n",
            "Epoch:  96  Training Loss: 38.31626129 Validation Loss: 38.913211822509766 Test Loss: 40.36328125\n",
            "Epoch: 100  Training Loss: 38.30124664 Validation Loss: 38.89069366455078 Test Loss: 40.34357452392578\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "train_loss_1,valid_loss_1,test_loss_1 = train(tr,criterion,optimizer,epochs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_diagrams(train_loss_1,valid_loss_1,test_loss_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "k7Hn6xg2Yy1k",
        "outputId": "e3925257-d7ae-4384-bcb5-b33b9fbd679b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqzkKGIPRaEO"
      },
      "source": [
        "#### Training for 10 epochs\n",
        "The results from Linear Regression is bettet than this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okqrO2XORaEO",
        "outputId": "4c9e6097-90a3-4298-878d-dc71aa63a373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:   1  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:   6  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  11  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  16  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  21  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  26  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  31  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  36  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  41  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  46  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  51  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  56  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  61  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  66  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  71  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  76  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  81  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  86  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  91  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch:  96  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 101  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 106  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 111  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 116  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 121  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 126  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 131  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 136  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 141  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 146  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 151  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 156  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 161  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 166  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 171  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 176  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 181  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 186  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 191  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 196  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 201  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 206  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 211  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 216  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 221  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 226  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 231  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 236  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 241  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 246  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 251  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 256  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 261  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 266  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 271  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 276  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 281  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 286  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 291  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 296  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 301  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 306  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 311  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 316  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 321  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 326  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 331  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 336  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 341  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 346  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 351  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 356  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 361  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 366  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 371  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 376  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 381  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 386  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 391  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 396  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 401  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 406  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 411  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 416  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 421  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 426  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 431  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 436  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 441  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 446  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 451  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 456  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 461  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 466  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 471  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 476  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 481  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 486  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 491  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 496  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 501  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 506  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 511  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 516  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 521  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 526  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 531  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 536  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 541  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 546  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 551  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 556  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 561  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 566  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 571  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 576  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 581  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 586  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 591  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 596  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 601  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 606  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 611  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 616  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 621  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 626  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 631  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 636  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 641  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 646  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 651  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 656  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 661  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 666  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 671  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 676  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 681  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 686  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 691  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 696  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 701  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 706  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 711  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 716  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 721  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 726  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 731  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 736  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 741  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 746  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 751  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 756  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 761  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 766  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 771  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 776  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 781  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 786  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 791  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 796  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 801  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 806  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 811  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 816  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 821  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 826  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 831  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 836  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 841  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 846  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 851  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 856  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 861  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 866  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 871  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 876  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 881  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 886  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 891  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 896  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 901  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 906  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 911  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 916  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 921  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 926  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 931  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 936  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 941  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 946  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 951  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 956  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 961  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 966  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 971  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 976  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 981  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 986  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 991  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 996  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n",
            "Epoch: 1000  Training Loss: 39.28547287 Validation Loss: 40.1985969543457 Test Loss: 41.52099609375\n"
          ]
        }
      ],
      "source": [
        "epochs = 1000\n",
        "tr = Model(input_features, output_features, hidden_layers, p=0.5)\n",
        "train_loss_1,valid_loss_1,test_loss_1 = train(tr,criterion,optimizer,epochs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_diagrams(train_loss_1,valid_loss_1,test_loss_1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "i7VNP_szZCvD",
        "outputId": "f855f188-5adc-4e13-e7ce-9b13b446740a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjRN9C6vRaEP"
      },
      "source": [
        "# Part 3 \n",
        "ReLU was chosen as the activation function because the range of other activation functions is not between (-180,180).\n",
        "As you can see, the results almost stayed the same with or without the activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-rtf_vURaEP",
        "outputId": "1d51333e-3ce8-4294-d750-46d252192118"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=68, out_features=2, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "input_features = 68\n",
        "output_features = 2\n",
        "hidden_layers = []\n",
        "tr = Model(input_features, output_features, hidden_layers, p=0.5, activation_function = nn.ReLU(inplace = True))\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(tr.parameters(), lr=0.001, momentum=0.9)\n",
        "tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpdhSX3rRaEP"
      },
      "source": [
        "### 50 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM-vqq3aRaEP",
        "outputId": "f6a13b98-33be-4b02-9ce2-0ab8126c300b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:   1  Training Loss: 2470.60473633 Validation Loss: 2439.492919921875 Test Loss: 2734.841064453125\n",
            "Epoch:   6  Training Loss: 2256.10961914 Validation Loss: 2197.303466796875 Test Loss: 2509.598876953125\n",
            "Epoch:  11  Training Loss: 1900.90063477 Validation Loss: 1785.1776123046875 Test Loss: 2134.05078125\n",
            "Epoch:  16  Training Loss: 1624.82824707 Validation Loss: 1443.302001953125 Test Loss: 1835.9298095703125\n",
            "Epoch:  21  Training Loss: 1486.96081543 Validation Loss: 1245.5555419921875 Test Loss: 1679.089599609375\n",
            "Epoch:  26  Training Loss: 1448.44274902 Validation Loss: 1161.8509521484375 Test Loss: 1626.804443359375\n",
            "Epoch:  31  Training Loss: 1451.10156250 Validation Loss: 1136.9619140625 Test Loss: 1621.189453125\n",
            "Epoch:  36  Training Loss: 1457.70617676 Validation Loss: 1132.161376953125 Test Loss: 1624.2314453125\n",
            "Epoch:  41  Training Loss: 1456.21765137 Validation Loss: 1131.4188232421875 Test Loss: 1622.6300048828125\n",
            "Epoch:  46  Training Loss: 1448.60290527 Validation Loss: 1132.0091552734375 Test Loss: 1617.016357421875\n",
            "Epoch:  51  Training Loss: 1439.92089844 Validation Loss: 1134.8231201171875 Test Loss: 1611.288330078125\n",
            "Epoch:  56  Training Loss: 1433.15783691 Validation Loss: 1139.7957763671875 Test Loss: 1607.569580078125\n",
            "Epoch:  61  Training Loss: 1428.71606445 Validation Loss: 1145.501220703125 Test Loss: 1605.7293701171875\n",
            "Epoch:  66  Training Loss: 1425.72998047 Validation Loss: 1150.2869873046875 Test Loss: 1604.6591796875\n",
            "Epoch:  71  Training Loss: 1423.26672363 Validation Loss: 1153.1788330078125 Test Loss: 1603.4033203125\n",
            "Epoch:  76  Training Loss: 1420.81762695 Validation Loss: 1154.068359375 Test Loss: 1601.5654296875\n",
            "Epoch:  81  Training Loss: 1418.26416016 Validation Loss: 1153.4248046875 Test Loss: 1599.200439453125\n",
            "Epoch:  86  Training Loss: 1415.66809082 Validation Loss: 1151.906005859375 Test Loss: 1596.5482177734375\n",
            "Epoch:  91  Training Loss: 1413.10986328 Validation Loss: 1150.087646484375 Test Loss: 1593.8387451171875\n",
            "Epoch:  96  Training Loss: 1410.62634277 Validation Loss: 1148.35498046875 Test Loss: 1591.2142333984375\n",
            "Epoch: 100  Training Loss: 1408.69262695 Validation Loss: 1147.1678466796875 Test Loss: 1589.2164306640625\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "train_loss_1,valid_loss_1,test_loss_1 = train(tr,criterion,optimizer,epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "CAIznPwARaEQ",
        "outputId": "c6b9f660-3f2c-46e8-9f53-7ad85d2f06ab"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "show_diagrams(train_loss_1,valid_loss_1,test_loss_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4888CemRaEQ"
      },
      "source": [
        "### 10 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpHiK_4qRaEQ",
        "outputId": "aa7c91b9-b9e7-4258-a3e3-b88b8009584a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:   1  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:   6  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  11  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  16  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  21  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  26  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  31  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  36  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  41  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  46  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  51  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  56  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  61  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  66  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  71  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  76  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  81  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  86  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  91  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch:  96  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 101  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 106  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 111  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 116  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 121  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 126  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 131  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 136  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 141  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 146  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 151  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 156  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 161  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 166  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 171  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 176  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 181  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 186  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 191  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 196  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 201  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 206  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 211  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 216  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 221  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 226  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 231  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 236  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 241  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 246  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 251  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 256  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 261  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 266  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 271  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 276  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 281  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 286  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 291  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 296  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 301  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 306  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 311  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 316  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 321  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 326  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 331  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 336  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 341  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 346  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 351  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 356  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 361  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 366  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 371  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 376  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 381  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 386  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 391  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 396  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 401  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 406  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 411  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 416  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 421  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 426  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 431  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 436  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 441  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 446  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 451  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 456  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 461  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 466  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 471  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 476  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 481  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 486  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 491  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 496  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 501  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 506  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 511  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 516  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 521  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 526  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 531  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 536  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 541  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 546  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 551  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 556  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 561  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 566  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 571  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 576  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 581  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 586  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 591  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 596  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 601  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 606  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 611  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 616  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 621  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 626  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 631  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 636  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 641  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 646  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 651  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 656  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 661  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 666  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 671  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 676  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 681  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 686  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 691  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 696  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 701  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 706  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 711  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 716  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 721  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 726  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 731  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 736  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 741  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 746  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 751  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 756  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 761  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 766  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 771  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 776  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 781  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 786  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 791  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 796  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 801  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 806  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 811  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 816  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 821  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 826  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 831  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 836  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 841  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 846  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 851  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 856  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 861  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 866  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 871  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 876  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 881  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 886  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 891  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 896  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 901  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 906  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 911  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 916  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 921  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 926  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 931  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 936  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 941  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 946  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 951  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 956  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 961  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 966  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 971  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 976  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 981  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 986  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 991  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 996  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n",
            "Epoch: 1000  Training Loss: 2469.76049805 Validation Loss: 2437.472412109375 Test Loss: 2734.015380859375\n"
          ]
        }
      ],
      "source": [
        "tr = Model(input_features, output_features, hidden_layers, p=0.5, activation_function = nn.ReLU(inplace = True))\n",
        "epochs = 1000\n",
        "train_loss_1,valid_loss_1,test_loss_1 = train(tr,criterion,optimizer,epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "0oIup2tQRaEQ",
        "outputId": "ee44f1f9-feb2-494b-dc4a-64ba6ca8f701"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "show_diagrams(train_loss_1,valid_loss_1,test_loss_1)"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "c4743f3265159d618cbfb7986907cf5d87d97b4cb2417375c3879348497beb2d"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}